"""
å›¾åƒæ•°æ®é›†åŠ è½½å™¨ï¼ˆå‚è€ƒ fastai/train.py çš„ç®€åŒ–å®ç°ï¼‰

æ”¯æŒæ ‡å‡†çš„train/valç›®å½•ç»“æ„ï¼Œä½¿ç”¨ pickle ç¼“å­˜
"""

import os
from datasets import Dataset
from pathlib import Path
import pandas as pd
from typing import Optional, Dict, Any
from hugging.utils import print_main


class ImageDataset:
    """
    å›¾åƒæ•°æ®é›†åŠ è½½å™¨ï¼ˆå‚è€ƒ fastai/train.pyï¼‰
    
    æ”¯æŒçš„ç›®å½•ç»“æ„:
    data_path/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ class1/
    â”‚   â”œâ”€â”€ class2/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ val/
        â”œâ”€â”€ class1/
        â”œâ”€â”€ class2/
        â””â”€â”€ ...
    """
    
    @staticmethod
    def from_directory(
        data_path: str,
        train_size: Optional[int] = None,
        val_size: Optional[int] = None,
        img_size: int = 224,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        ä»ç›®å½•ç»“æ„åŠ è½½æ•°æ®é›†ï¼ˆå‚è€ƒ fastai/train.py çš„å®ç°ï¼‰
        
        æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ï¼Œä½¿ç”¨ç®€å•çš„ pickle ç¼“å­˜
        
        Args:
            data_path: æ•°æ®é›†æ ¹ç›®å½•
            train_size: è®­ç»ƒé›†å¤§å°é™åˆ¶ï¼ˆå¯é€‰ï¼‰
            val_size: éªŒè¯é›†å¤§å°é™åˆ¶ï¼ˆå¯é€‰ï¼‰
            img_size: å›¾åƒå¤§å°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            åŒ…å«trainã€valæ•°æ®é›†å’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        path = Path(data_path).absolute()
        cache_dir = path / '.dataset_cache'
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        def build_df(subset: str, size_limit: Optional[int] = None) -> pd.DataFrame:
            """æ„å»ºæ•°æ®é›†DataFrameï¼Œä½¿ç”¨ pickle ç¼“å­˜"""
            # ç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_key = f"{subset}_{img_size}"
            if size_limit:
                cache_key += f"_{size_limit}"
            cache_file = cache_dir / f"{cache_key}.pkl"
            
            # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
            if use_cache and cache_file.exists():
                print_main(f"âœ“ ä½¿ç”¨ç¼“å­˜çš„ {subset} æ•°æ®é›†")
                return pd.read_pickle(cache_file)
            
            # æ„å»ºæ•°æ®é›†
            print_main(f"âš™ï¸  æ„å»º {subset} æ•°æ®é›†...")
            
            records = []
            subset_path = path / subset
            
            if not subset_path.exists():
                raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {subset_path}")
            
            for class_dir in subset_path.iterdir():
                if not class_dir.is_dir():
                    continue
                
                class_name = class_dir.name
                images = (
                    list(class_dir.glob('*.jpg')) + 
                    list(class_dir.glob('*.png')) +
                    list(class_dir.glob('*.jpeg'))
                )
                
                for img_path in images:
                    records.append({
                        'image_path': str(img_path),
                        'label': class_name
                    })
            
            df = pd.DataFrame(records)
            
            # é™åˆ¶å¤§å°
            if size_limit and len(df) > size_limit:
                df = df.sample(n=size_limit, random_state=42)
            
            # ä¿å­˜ç¼“å­˜
            if use_cache:
                df.to_pickle(cache_file)
                print_main(f"âœ“ {subset} æ•°æ®é›†æ„å»ºå®Œæˆ: {len(df)} å¼ å›¾ç‰‡ï¼ˆå·²ç¼“å­˜ï¼‰")
            else:
                print_main(f"âœ“ {subset} æ•°æ®é›†æ„å»ºå®Œæˆ: {len(df)} å¼ å›¾ç‰‡")
            
            return df
        
        # åŠ è½½æ•°æ®
        train_df = build_df('train', train_size)
        val_df = build_df('val', val_size)
        
        # ğŸ”§ å…³é”®ä¿®å¤: æ‰“ä¹±éªŒè¯é›†
        print_main("ğŸ”€ æ‰“ä¹±éªŒè¯é›†ä»¥ç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒæ—¶çš„å‡†ç¡®æ€§...")
        val_df = val_df.sample(frac=1.0, random_state=42).reset_index(drop=True)
        
        print_main(f"  è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
        print_main(f"  éªŒè¯é›†: {len(val_df)} æ ·æœ¬")
        
        # åˆ›å»ºæ ‡ç­¾æ˜ å°„
        all_labels = sorted(set(train_df['label'].unique()) | set(val_df['label'].unique()))
        label2id = {label: i for i, label in enumerate(all_labels)}
        id2label = {i: label for label, i in label2id.items()}
        
        print_main(f"  ç±»åˆ«æ•°: {len(all_labels)}")
        
        # è½¬æ¢ä¸ºHuggingFace Dataset
        train_dataset = Dataset.from_pandas(train_df)
        val_dataset = Dataset.from_pandas(val_df)
        
        # æ·»åŠ label_idåˆ—
        def add_label_id(example):
            example['label_id'] = label2id[example['label']]
            return example
        
        train_dataset = train_dataset.map(add_label_id)
        val_dataset = val_dataset.map(add_label_id)
        
        return {
            'train': train_dataset,
            'val': val_dataset,
            'num_classes': len(all_labels),
            'label2id': label2id,
            'id2label': id2label,
            'labels': all_labels
        }
