"""
ä¿å­˜æ¨¡å‹å›è°ƒ

ä¿å­˜å®Œæ•´çš„checkpointä¿¡æ¯ï¼ŒåŒ…æ‹¬epochã€lossã€ä¼˜åŒ–å™¨çŠ¶æ€ç­‰
æ”¯æŒearly stopping
"""

import torch
from pathlib import Path
from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
import os
from hugging.utils import print_main

class SaveModelCallback(TrainerCallback):
    """
    ä¿å­˜å®Œæ•´çš„checkpointä¿¡æ¯
    
    ä¿å­˜å†…å®¹:
    - model: æ¨¡å‹æƒé‡
    - opt: ä¼˜åŒ–å™¨çŠ¶æ€
    - epoch: å½“å‰epoch
    - loss: å½“å‰loss
    - img_size: å›¾åƒå°ºå¯¸
    - arch: æ¨¡å‹æ¶æ„
    
    æ”¯æŒearly stoppingåŠŸèƒ½
    """
    
    def __init__(
        self,
        img_size: int,
        arch: str,
        resume_from_epoch: int = 0,
        patience: int = 100,
        monitor: str = 'eval_loss',
        mode: str = 'min'
    ):
        """
        Args:
            img_size: å›¾åƒå°ºå¯¸
            arch: æ¨¡å‹æ¶æ„åç§°
            resume_from_epoch: æ¢å¤è®­ç»ƒçš„èµ·å§‹epoch
            patience: æ—©åœpatienceï¼ˆéªŒè¯lossä¸æ”¹å–„çš„epochæ•°ï¼‰
            monitor: ç›‘æ§çš„æŒ‡æ ‡åç§°
            mode: 'min' æˆ– 'max'ï¼ŒæŒ‡æ ‡æ˜¯è¶Šå°è¶Šå¥½è¿˜æ˜¯è¶Šå¤§è¶Šå¥½
        """
        self.img_size = img_size
        self.arch = arch
        self.resume_from_epoch = resume_from_epoch
        self.patience = patience
        self.monitor = monitor
        self.mode = mode
        
        # Early stopping state
        self.best_metric = None
        self.wait = 0
        self.stopped_epoch = 0
        
        # Store trainer reference
        self.trainer = None
    
    def on_train_begin(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """Store trainer reference at training begin"""
        # Get trainer from kwargs (Trainer passes itself to callbacks)
        if 'model' in kwargs:
            # Find the trainer instance - it should be accessible through the call stack
            # We'll store it when we get it in on_evaluate
            pass
    
    def _is_better(self, current, best):
        """åˆ¤æ–­å½“å‰æŒ‡æ ‡æ˜¯å¦æ›´å¥½"""
        if best is None:
            return True
        if self.mode == 'min':
            return current < best
        else:
            return current > best
    
    def on_evaluate(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        metrics=None,
        model=None,
        **kwargs
    ):
        """
        è¯„ä¼°åæ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æœ€ä¼˜æ¨¡å‹å’Œearly stopping
        """
        # åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ
        if not state.is_local_process_zero:
            return control
        
        if metrics is None or self.monitor not in metrics:
            return control
        
        current_metric = metrics[self.monitor]
        
        # Model is passed by Trainer automatically
        # Try to get optimizer and lr_scheduler from kwargs - Trainer might pass it
        optimizer = None
        lr_scheduler = None
        
        # Check if we can get the trainer instance
        # The trainer instance should have the optimizer and lr_scheduler
        if self.trainer is not None:
            optimizer = getattr(self.trainer, 'optimizer', None)
            lr_scheduler = getattr(self.trainer, 'lr_scheduler', None)
        
        if model is None:
            return control
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä¼˜æ¨¡å‹
        if self._is_better(current_metric, self.best_metric):
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            self.best_metric = current_metric
            self.wait = 0
            
            # ä¿å­˜best.pth
            output_dir = Path(args.output_dir)
            best_path = output_dir / 'best.pth'
            
            # è·å–å®é™…çš„epoch
            actual_epoch = int(state.epoch) + self.resume_from_epoch - 1 if state.epoch else 0
            
            # è·å–æ¨¡å‹çŠ¶æ€ï¼ˆå¤„ç†DDPåŒ…è£…ï¼‰
            if hasattr(model, 'module'):
                model_state = model.module.state_dict()
            else:
                model_state = model.state_dict()
            
            # æ„å»ºå®Œæ•´çš„checkpoint
            checkpoint = {
                'model': model_state,
                'epoch': actual_epoch,
                'img_size': self.img_size,
                'arch': self.arch,
                'loss': current_metric,
            }
            
            # æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€
            if optimizer is not None:
                checkpoint['opt'] = optimizer.state_dict()
            
            # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
            if lr_scheduler is not None:
                checkpoint['scheduler'] = lr_scheduler.state_dict()
            
            # ä¿å­˜
            torch.save(checkpoint, best_path)
            
            print_main(f"\nğŸ’¾ ä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ°: {best_path}")
            print_main(f"   - Epoch: {actual_epoch}")
            print_main(f"   - {self.monitor}: {current_metric:.6f} (improved)")
            print_main(f"   - lr: {optimizer.param_groups[0]['lr'] if optimizer else 'N/A'}")
            print_main(f"   - åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€: {'âœ…' if optimizer else 'âŒ'}")
            print_main(f"   - åŒ…å«è°ƒåº¦å™¨çŠ¶æ€: {'âœ…' if lr_scheduler else 'âŒ'}")
        else:
            # æ²¡æœ‰æ”¹å–„
            self.wait += 1
            print_main(f"\nâ³ {self.monitor}: {current_metric:.6f} (no improvement, patience: {self.wait}/{self.patience})")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦early stopping
            if self.wait >= self.patience:
                self.stopped_epoch = state.epoch
                control.should_training_stop = True
                print_main(f"\nğŸ›‘ Early stopping triggered at epoch {int(state.epoch)}")
                print_main(f"   - Best {self.monitor}: {self.best_metric:.6f}")
                print_main(f"   - lr: {optimizer.param_groups[0]['lr'] if optimizer else 'N/A'}")
                print_main(f"   - No improvement for {self.patience} evaluations")
        
        return control
    
    def on_train_end(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """è®­ç»ƒç»“æŸæ—¶çš„æ€»ç»“"""
        if not state.is_local_process_zero:
            return
        
        if self.stopped_epoch > 0:
            print_main(f"\nâœ… è®­ç»ƒå› early stoppingç»“æŸäºepoch {int(self.stopped_epoch)}")
        
        if self.best_metric is not None:
            print_main(f"\nğŸ† æœ€ä¼˜ {self.monitor}: {self.best_metric:.6f}")
