#!/usr/bin/env python3
"""
HuggingFaceå›¾åƒåˆ†ç±»è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–¹å¼ï¼ˆå‘½ä»¤è¡Œå‚æ•°å®Œå…¨å…¼å®¹åŸæœ‰è„šæœ¬ï¼‰:
    python hugging/train.py \\
        --data_path /path/to/data \\
        --arch resnet18 \\
        --batch_size 256 \\
        --epochs 100 \\
        --lr0 0.01 \\
        --distributed

åˆ†å¸ƒå¼è®­ç»ƒ:
    accelerate launch hugging/train.py \\
        --data_path /path/to/data \\
        --arch resnet18 \\
        --distributed
"""

import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
from transformers import Trainer
from accelerate import Accelerator

from hugging.config import TrainingConfig
from hugging.models import create_classification_model
from hugging.data import ImageDataset, ImageCollator
from hugging.callbacks import MLflowCallback, SaveModelCallback
from hugging.optimizers import create_optimizer
from hugging.metrics import compute_metrics
from hugging.utils import print_main, generate_evaluation_reports


class CustomTrainer(Trainer):
    """è‡ªå®šä¹‰Trainerï¼Œæ”¯æŒè®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨ä¸åŒçš„data collator"""
    
    def __init__(self, *args, val_collator=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.val_collator = val_collator
        self.train_collator = self.data_collator
    
    def evaluate(self, *args, **kwargs):
        """é‡å†™evaluateæ–¹æ³•ï¼Œä½¿ç”¨éªŒè¯ä¸“ç”¨çš„collator"""
        # ä¸´æ—¶åˆ‡æ¢åˆ°éªŒè¯collator
        if self.val_collator is not None:
            original_collator = self.data_collator
            self.data_collator = self.val_collator
            try:
                metrics = super().evaluate(*args, **kwargs)
            finally:
                # æ¢å¤è®­ç»ƒcollator
                self.data_collator = original_collator
        else:
            metrics = super().evaluate(*args, **kwargs)
        return metrics
    
    def predict(self, *args, **kwargs):
        """é‡å†™predictæ–¹æ³•ï¼Œä½¿ç”¨éªŒè¯ä¸“ç”¨çš„collator"""
        # ä¸´æ—¶åˆ‡æ¢åˆ°éªŒè¯collator
        if self.val_collator is not None:
            original_collator = self.data_collator
            self.data_collator = self.val_collator
            try:
                result = super().predict(*args, **kwargs)
            finally:
                # æ¢å¤è®­ç»ƒcollator
                self.data_collator = original_collator
        else:
            result = super().predict(*args, **kwargs)
        return result


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    ä¿æŒä¸åŸæœ‰è„šæœ¬å®Œå…¨ä¸€è‡´çš„å‚æ•°åç§°
    """
    parser = argparse.ArgumentParser(
        description='HuggingFaceå›¾åƒåˆ†ç±»è®­ç»ƒè„šæœ¬'
    )
    
    # === æ•°æ®å‚æ•° ===
    parser.add_argument(
        '--data_path',
        type=str,
        required=True,
        help='æ•°æ®é›†è·¯å¾„ï¼ˆåŒ…å«train/valå­ç›®å½•ï¼‰'
    )
    parser.add_argument(
        '--train_size',
        type=int,
        default=None,
        help='è®­ç»ƒé›†å¤§å°é™åˆ¶'
    )
    parser.add_argument(
        '--val_size',
        type=int,
        default=None,
        help='éªŒè¯é›†å¤§å°é™åˆ¶'
    )
    
    # === æ¨¡å‹å‚æ•° ===
    parser.add_argument(
        '--arch',
        type=str,
        default='resnet18',
        help='æ¨¡å‹æ¶æ„ (resnet18, resnet50, vit_base, etc.)'
    )
    parser.add_argument(
        '--pretrained',
        action='store_true',
        help='ä½¿ç”¨é¢„è®­ç»ƒæƒé‡'
    )
    parser.add_argument(
        '--img_size',
        type=int,
        default=224,
        help='å›¾åƒå¤§å°'
    )
    
    # === è®­ç»ƒå‚æ•° ===
    parser.add_argument(
        '--batch_size',
        type=int,
        default=256,
        help='æ¯ä¸ªè®¾å¤‡çš„batch size'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=100,
        help='è®­ç»ƒè½®æ•°'
    )
    parser.add_argument(
        '--lr0',
        type=float,
        default=0.01,
        help='åˆå§‹å­¦ä¹ ç‡'
    )
    parser.add_argument(
        '--lrf',
        type=float,
        default=0.1,
        help='æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹ (final_lr = lr0 * lrf)'
    )
    parser.add_argument(
        '--wd',
        type=float,
        default=0.01,
        help='æƒé‡è¡°å‡ (weight decay)'
    )
    parser.add_argument(
        '--optimizer',
        type=str,
        default='Adam',
        choices=['SGD', 'Adam', 'AdamW', 'RMSprop'],
        help='ä¼˜åŒ–å™¨ç±»å‹'
    )
    parser.add_argument(
        '--grad_acc',
        type=int,
        default=-1,
        help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (-1è¡¨ç¤ºä¸ä½¿ç”¨)'
    )
    
    # === åˆ†å¸ƒå¼å‚æ•° ===
    parser.add_argument(
        '--distributed',
        action='store_true',
        help='å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆéœ€è¦ä½¿ç”¨accelerate launchï¼‰'
    )
    parser.add_argument(
        '--device',
        type=str,
        default=None,
        help='æŒ‡å®šGPUè®¾å¤‡ï¼Œä¾‹å¦‚ "0", "1" æˆ– "cuda:0", None=ä½¿ç”¨é»˜è®¤'
    )
    
    # === å›è°ƒå‚æ•° ===
    parser.add_argument(
        '--early_stopping',
        type=int,
        default=100,
        help='Early stoppingçš„patienceï¼ˆé»˜è®¤100ï¼‰'
    )
    parser.add_argument(
        '--scheduler_type',
        type=str,
        default='cosine',
        choices=['cosine', 'cosine_restarts', 'linear', 'constant', 'polynomial'],
        help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹'
    )
    parser.add_argument(
        '--warmup_epochs',
        type=int,
        default=3,
        help='çƒ­èº«é˜¶æ®µçš„epochæ•°ï¼ˆé»˜è®¤3ï¼‰'
    )
    parser.add_argument(
        '--min_lr',
        type=float,
        default=None,
        help='æœ€å°å­¦ä¹ ç‡ï¼ˆé»˜è®¤ä¸ºlr0çš„1%ï¼‰'
    )
    
    # === ä¿å­˜/æ¢å¤å‚æ•° ===
    parser.add_argument(
        '--model_path',
        type=str,
        default='last',
        help='ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼Œé»˜è®¤: lastï¼‰'
    )
    parser.add_argument(
        '--models_base_dir',
        type=str,
        default='runs',
        help='ç»Ÿä¸€çš„æ¨¡å‹ä¿å­˜åŸºç¡€ç›®å½•ï¼Œå®é™…ä¿å­˜è·¯å¾„ä¸º: models_base_dir/project_name/task_name/'
    )
    parser.add_argument(
        '--load_model',
        type=str,
        default=None,
        help='åŠ è½½å·²æœ‰çš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆå®Œæ•´è·¯å¾„ï¼‰'
    )
    
    parser.add_argument(
        '--project_name',
        type=str,
        default='ai-classifier',
        help='MLflowå®éªŒåç§°ï¼ˆé»˜è®¤: ai-classifierï¼‰'
    )
    parser.add_argument(
        '--task_name',
        type=str,
        default='Image Classification',
        help='MLflowè¿è¡Œåç§°ï¼ˆé»˜è®¤: Image Classificationï¼‰'
    )
    parser.add_argument(
        '--mlflow_tracking_uri',
        type=str,
        default=None,
        help='MLflow Tracking URIï¼ˆé»˜è®¤: ä»ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰'
    )
    parser.add_argument(
        '--skip_mlflow_model_upload',
        action='store_true',
        help='è·³è¿‡æ¨¡å‹ä¸Šä¼ åˆ°MLflow'
    )
    parser.add_argument(
        '--disable_mlflow',
        action='store_true',
        help='ç¦ç”¨MLflowï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    
    # === å…¶ä»–å‚æ•° ===
    parser.add_argument(
        '--only_val',
        action='store_true',
        help='ä»…æ‰§è¡ŒéªŒè¯ï¼ˆä¸è®­ç»ƒï¼‰'
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    

    # æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼ˆç”¨äºè¾“å‡ºæ§åˆ¶ï¼‰
    is_main = not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0

    print_main("=" * 80)
    print_main("ğŸš€ HuggingFaceå›¾åƒåˆ†ç±»è®­ç»ƒ")
    print_main("=" * 80)
    
    if is_main:
        # æ˜¾ç¤ºæ¨¡å‹ä¿å­˜è·¯å¾„ä¿¡æ¯
        print_main(f"\n{'='*80}")
        print_main(f"æ¨¡å‹ä¿å­˜é…ç½®:")
        print_main(f"  åŸºç¡€ç›®å½•: {args.models_base_dir}")
        print_main(f"  é¡¹ç›®åç§°: {args.project_name}")
        print_main(f"  ä»»åŠ¡åç§°: {args.task_name}")
        print_main(f"  å®é™…è·¯å¾„: {Path(args.models_base_dir).absolute() / args.project_name / args.task_name}")
        print_main(f"{'='*80}\n")
    
    # 0. å¤„ç†GPUè®¾å¤‡é€‰æ‹©
    if args.device is not None:
        import os
        device_id = args.device.split(':')[-1] if ':' in args.device else args.device
        os.environ['CUDA_VISIBLE_DEVICES'] = device_id

    print_main(f"\nğŸ® ä½¿ç”¨æŒ‡å®šGPU: {args.device}")
    
    # 1. åŠ è½½æ•°æ®é›†
    print_main(f"\nğŸ“ åŠ è½½æ•°æ®é›†...")

    dataset_info = ImageDataset.from_directory(
        data_path=args.data_path,
        train_size=args.train_size,
        val_size=args.val_size,
        img_size=args.img_size
    )
    
    train_dataset = dataset_info['train']
    val_dataset = dataset_info['val']
    num_classes = dataset_info['num_classes']
    
    # 2. åˆ›å»ºæ¨¡å‹
    print_main(f"\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹: {args.arch}")
    print_main(f"   ç±»åˆ«æ•°: {num_classes}")
    print_main(f"   é¢„è®­ç»ƒ: {'æ˜¯' if args.pretrained else 'å¦'}")
    
    model = create_classification_model(
        arch=args.arch,
        num_classes=num_classes,
        pretrained=args.pretrained
    )
    
    # 2.5 å¤„ç†æ¨¡å‹åŠ è½½å’Œæ¢å¤
    resume_from_epoch = 0
    resume_best_metric = None
    optimizer_state_to_load = None
    scheduler_state_to_load = None
    
    # æ„å»ºç»Ÿä¸€çš„æ¨¡å‹ä¿å­˜ç›®å½•: models_base_dir/project_name/task_name/
    model_save_dir = Path(args.models_base_dir) / args.project_name / args.task_name
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    print_main(f"\nğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•: {model_save_dir.absolute()}")
    
    # è‡ªåŠ¨æ¢å¤é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®šload_modelï¼Œä¸”å¯ç”¨äº†auto_resumeï¼Œå°è¯•è‡ªåŠ¨åŠ è½½
    load_model_path = args.load_model
    if load_model_path is None:
        auto_load_path = model_save_dir / 'best.pth'
        if auto_load_path.exists():
            print_main(f"\nğŸ” å‘ç°å·²å­˜åœ¨çš„æ¨¡å‹: {auto_load_path}")
            print_main(f"   è‡ªåŠ¨åŠ è½½ä»¥ç»§ç»­è®­ç»ƒ...")
            load_model_path = str(auto_load_path)
    
    # åŠ è½½checkpoint
    if load_model_path is not None:
        print_main(f"\n{'='*80}")
        print_main(f"ğŸ“¦ ä» checkpoint åŠ è½½æ¨¡å‹")
        print_main(f"{'='*80}")
        print_main(f"æ¨¡å‹è·¯å¾„: {load_model_path}")
        
        # åŠ è½½çŠ¶æ€å­—å…¸
        checkpoint = torch.load(load_model_path, map_location='cpu')
        
        print_main(f"\nğŸ“‹ Checkpoint ä¿¡æ¯:")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«epochä¿¡æ¯
        if isinstance(checkpoint, dict) and 'epoch' in checkpoint:
            resume_from_epoch = checkpoint['epoch'] + 1  # +1å› ä¸ºä¿å­˜çš„æ˜¯å®Œæˆçš„epoch
            print_main(f"  - å·²å®Œæˆçš„ epoch: {checkpoint['epoch']}")
            print_main(f"  - ä¸‹æ¬¡è®­ç»ƒèµ·å§‹ epoch: {resume_from_epoch}")
        else:
            print_main(f"  - Epoch ä¿¡æ¯: âš ï¸  æœªæ‰¾åˆ°")
        
        # æ£€æŸ¥lossä¿¡æ¯
        if isinstance(checkpoint, dict) and 'loss' in checkpoint:
            resume_best_metric = checkpoint['loss']
            print_main(f"  - å½“å‰ valid_loss: {resume_best_metric:.6f}")
        else:
            print_main(f"  - æœ€ä½³æŒ‡æ ‡: âš ï¸  æœªæ‰¾åˆ°")
        
        # æ£€æŸ¥å…¶ä»–ä¿¡æ¯
        if isinstance(checkpoint, dict):
            if 'img_size' in checkpoint:
                print_main(f"  - å›¾åƒå°ºå¯¸: {checkpoint['img_size']}")
            if 'arch' in checkpoint:
                print_main(f"  - æ¨¡å‹æ¶æ„: {checkpoint['arch']}")
            if 'opt' in checkpoint:
                print_main(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: âœ… å·²ä¿å­˜")
                optimizer_state_to_load = checkpoint['opt']
            else:
                print_main(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: âš ï¸  æœªæ‰¾åˆ°")
            if 'scheduler' in checkpoint:
                print_main(f"  - è°ƒåº¦å™¨çŠ¶æ€: âœ… å·²ä¿å­˜")
                scheduler_state_to_load = checkpoint['scheduler']
            else:
                print_main(f"  - è°ƒåº¦å™¨çŠ¶æ€: âš ï¸  æœªæ‰¾åˆ°")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        if isinstance(checkpoint, dict) and 'model' in checkpoint:
            model_state = checkpoint['model']
        else:
            model_state = checkpoint  # çº¯æ¨¡å‹æƒé‡
        
        # å¤„ç†DDPåŒ…è£…çš„æƒé‡åç§°
        from collections import OrderedDict
        new_state_dict = OrderedDict()
        for k, v in model_state.items():
            name = k.replace('module.', '') if k.startswith('module.') else k
            new_state_dict[name] = v
        
        model.load_state_dict(new_state_dict)
        
        print_main(f"\nâœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        if resume_from_epoch > 0:
            print_main(f"ğŸ“Š å°†ä» epoch {resume_from_epoch} ç»§ç»­è®­ç»ƒ")
    
    # 3. åˆ›å»ºè®­ç»ƒé…ç½®
    print_main(f"\nâš™ï¸  é…ç½®è®­ç»ƒå‚æ•°...")
    print_main(f"   ä¼ å…¥batch_size: {args.batch_size}")

    config = TrainingConfig(**vars(args))
    training_args = config.to_training_arguments()

    print_main(f"   å®é™…per_device_train_batch_size: {training_args.per_device_train_batch_size}")
    print_main(f"   å®é™…per_device_eval_batch_size: {training_args.per_device_eval_batch_size}")
    
    # æ˜¾ç¤ºå­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    print_main(f"\nğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®:")
    print_main(f"   ç±»å‹: {args.scheduler_type}")
    print_main(f"   åˆå§‹å­¦ä¹ ç‡ (lr0): {args.lr0}")
    print_main(f"   æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹ (lrf): {args.lrf}")
    print_main(f"   æœ€ç»ˆå­¦ä¹ ç‡: {args.lr0 * args.lrf}")
    print_main(f"   çƒ­èº«epochs: {args.warmup_epochs}")
    print_main(f"   çƒ­èº«æ¯”ä¾‹: {args.warmup_epochs / args.epochs if args.epochs > 0 else 0:.2%}")
    if args.min_lr:
        print_main(f"   æœ€å°å­¦ä¹ ç‡: {args.min_lr}")
    else:
        print_main(f"   æœ€å°å­¦ä¹ ç‡: {args.lr0 * 0.01} (é»˜è®¤ä¸ºlr0çš„1%)")
    
    # 4. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(
        optimizer_name=args.optimizer,
        model=model,
        lr=args.lr0,
        weight_decay=args.wd
    )
    
    # 4.5 åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
    if optimizer_state_to_load is not None:
        try:
            optimizer.load_state_dict(optimizer_state_to_load)
            print_main(f"\nğŸ’¾ ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
            # æ˜¾ç¤ºæ¢å¤çš„å­¦ä¹ ç‡
            if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
                restored_lr = optimizer.param_groups[0].get('lr', 'N/A')
                print_main(f"   æ¢å¤çš„å­¦ä¹ ç‡: {restored_lr}")
        except Exception as e:
            print_main(f"\nâš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
            print_main(f"   å°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€")

    ## è¾“å‡ºä¼˜åŒ–å™¨ä¸­çš„ lr å‚æ•°
    if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
        initial_lr = optimizer.param_groups[0].get('lr', 'N/A')
        print_main(f"\nğŸ”§ ä¼˜åŒ–å™¨å½“å‰å­¦ä¹ ç‡: {initial_lr}")
    
    # 4.6 åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    from transformers import get_scheduler
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    steps_per_epoch = len(train_dataset) // (args.batch_size * training_args.gradient_accumulation_steps)
    if args.distributed:
        import torch.distributed as dist
        if dist.is_initialized():
            steps_per_epoch = steps_per_epoch // dist.get_world_size()
    
    num_training_steps = steps_per_epoch * args.epochs
    num_warmup_steps = steps_per_epoch * args.warmup_epochs
    
    print_main(f"\nğŸ“Š è®­ç»ƒæ­¥æ•°è®¡ç®—:")
    print_main(f"   æ¯ä¸ªepochæ­¥æ•°: {steps_per_epoch}")
    print_main(f"   æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}")
    print_main(f"   çƒ­èº«æ­¥æ•°: {num_warmup_steps}")
    
    # æ˜ å°„scheduler_typeåˆ°HuggingFaceçš„SchedulerType
    from transformers import SchedulerType
    scheduler_mapping = {
        'cosine': SchedulerType.COSINE,
        'cosine_restarts': SchedulerType.COSINE_WITH_RESTARTS,
        'linear': SchedulerType.LINEAR,
        'constant': SchedulerType.CONSTANT_WITH_WARMUP,
        'polynomial': SchedulerType.POLYNOMIAL,
    }
    lr_scheduler_type = scheduler_mapping.get(args.scheduler_type, SchedulerType.COSINE)
    
    lr_scheduler = get_scheduler(
        name=lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    
    # 4.7 åŠ è½½è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
    if scheduler_state_to_load is not None:
        try:
            lr_scheduler.load_state_dict(scheduler_state_to_load)
            print_main(f"\nğŸ’¾ å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²æ¢å¤")
        except Exception as e:
            print_main(f"\nâš ï¸  è°ƒåº¦å™¨çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
            print_main(f"   å°†ä½¿ç”¨æ–°çš„è°ƒåº¦å™¨çŠ¶æ€")
      
    # 5. åˆ›å»ºData Collator
    train_collator = ImageCollator(img_size=args.img_size, is_training=True)
    val_collator = ImageCollator(img_size=args.img_size, is_training=False)
    
    # 6. åˆ›å»ºCallbacks
    callbacks = []
    
    # MLflowé›†æˆï¼ˆé»˜è®¤å¯ç”¨ï¼Œå‚æ•°åä¸fastai/train.pyä¸€è‡´ï¼‰
    if not args.disable_mlflow:
        task_name = args.task_name
        if not task_name:
            import datetime
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            task_name = f"{args.arch}_{timestamp}"
        
        callbacks.append(
            MLflowCallback(
                project_name=args.project_name,
                task_name=task_name,
                skip_model_upload=args.skip_mlflow_model_upload,
                tracking_uri=args.mlflow_tracking_uri
            )
        )

    print_main(f"   é¡¹ç›® (Experiment): {args.project_name}")
    print_main(f"   ä»»åŠ¡ (Run): {task_name}")
    
    # ä¿å­˜æ¨¡å‹callbackï¼ˆä¿å­˜å®Œæ•´çš„checkpointä¿¡æ¯ï¼Œæ”¯æŒearly stoppingï¼‰
    save_model_callback = SaveModelCallback(
        img_size=args.img_size,
        arch=args.arch,
        resume_from_epoch=resume_from_epoch,
        patience=args.early_stopping,
        monitor='eval_loss',
        mode='min'
    )
    callbacks.append(save_model_callback)
    print_main(f"ğŸ’¾ å·²å¯ç”¨æ¨¡å‹ä¿å­˜ (early_stopping patience={args.early_stopping})")
    
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=train_collator,  # è®­ç»ƒç”¨collator
        val_collator=val_collator,  # éªŒè¯ç”¨collator
        compute_metrics=compute_metrics,
        callbacks=callbacks,
        optimizers=(optimizer, lr_scheduler),  # (optimizer, lr_scheduler)
    )
    
    # è®¾ç½®trainerå¼•ç”¨ç»™save_model_callbackï¼Œä»¥ä¾¿è®¿é—®optimizer
    save_model_callback.trainer = trainer
    
    # 8. è®­ç»ƒæˆ–éªŒè¯
    if args.only_val:
        print_main(f"\nğŸ“Š éªŒè¯æ¨¡å¼...")
        metrics = trainer.evaluate()
        print_main(f"\néªŒè¯ç»“æœ:")
        for key, value in metrics.items():
            print_main(f"  {key}: {value:.4f}")
    else:
        print_main(f"\nğŸ‹ï¸  å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
    # è®­ç»ƒç»“æŸååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    print_main(f"\nğŸ“Š æœ€ç»ˆéªŒè¯...")
    metrics = trainer.evaluate()
    print_main(f"\næœ€ç»ˆéªŒè¯ç»“æœ:")
    for key, value in metrics.items():
        print_main(f"  {key}: {value:.4f}")
    
    # 9. ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Šï¼ˆä»…ééªŒè¯æ¨¡å¼ä¸”ä¸»è¿›ç¨‹ï¼‰
    if not args.only_val and trainer.args.local_rank in [-1, 0]:
        print_main(f"\nğŸ“Š ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š...")
        
        # è·å–éªŒè¯é›†çš„é¢„æµ‹ç»“æœ
        predictions = trainer.predict(val_dataset)
        pred_logits = predictions.predictions
        pred_labels = predictions.label_ids
        
        # è·å–ç±»åˆ«åˆ—è¡¨ï¼ˆä»dataset_infoä¸­è·å–ï¼‰
        classes = dataset_info['labels']
        
        # è·å–MLflow run IDï¼ˆå¦‚æœå¯ç”¨ï¼‰
        mlflow_run_id = None
        if not args.disable_mlflow:
            for callback in trainer.callback_handler.callbacks:
                if isinstance(callback, MLflowCallback) and callback.run:
                    mlflow_run_id = callback.run.info.run_id
                    break
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        generate_evaluation_reports(
            predictions=pred_logits,
            targets=pred_labels,
            classes=classes,
            output_dir=Path(training_args.output_dir),
            mlflow_run_id=mlflow_run_id
        )
    
    print_main(f"\nâœ… å®Œæˆï¼")

    if not args.only_val:
        print_main(f"   æ¨¡å‹ä¿å­˜åœ¨: {training_args.output_dir}")


if __name__ == '__main__':
    main()
