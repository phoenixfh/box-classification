"""YOLOv11 æ£€æµ‹æ¨¡å‹è®­ç»ƒ - MLflow é›†æˆ"""
import warnings
import os
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥utils
sys.path.insert(0, str(Path(__file__).parent.parent))

# ç¦ç”¨ albumentations çš„ç‰ˆæœ¬æ£€æŸ¥ï¼ˆé¿å…ç½‘ç»œè¶…æ—¶è­¦å‘Šï¼‰
os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'

# è¿‡æ»¤æ‰ threadpoolctl çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='threadpoolctl')
warnings.filterwarnings('ignore', message='Error fetching version info')

import mlflow
from ultralytics import YOLO
import argparse
import yaml
import pandas as pd
import shutil
from utils import is_main_process

# å¯¼å…¥ ONNX å¯¼å‡ºå‡½æ•°
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))
from export_onnx import export_yolo_to_onnx


# export MLFLOW_TRACKING_URI=http://192.168.16.130:5000
# export AWS_ACCESS_KEY_ID=mlflow
# export AWS_SECRET_ACCESS_KEY=mlflow@SN
# export AWS_ENDPOINT_URL=http://192.168.16.130:9000
# export MLFLOW_S3_IGNORE_TLS=true


def create_mlflow_callbacks(run_id, mlflow_uri):
    """åˆ›å»º MLflow å›è°ƒå‡½æ•°ç”¨äºå®æ—¶ä¸ŠæŠ¥è®­ç»ƒæŒ‡æ ‡"""
    
    def on_fit_epoch_end(trainer):
        """æ¯ä¸ª epoch ç»“æŸæ—¶ä¸ŠæŠ¥æŒ‡æ ‡åˆ° MLflow"""
        # å…ˆæ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œç¡®è®¤å›è°ƒè¢«è°ƒç”¨
        rank = getattr(trainer.args, 'rank', -1)
        print(f"\nğŸ”” å›è°ƒè§¦å‘ - Epoch {trainer.epoch}, Rank {rank}, è¿›ç¨‹ PID {os.getpid()}")
        
        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰ rank 0 ä¸ŠæŠ¥
        if rank != 0:
            print(f"   â­ï¸  Rank {rank} è·³è¿‡ä¸ŠæŠ¥ï¼ˆä»… Rank 0 ä¸ŠæŠ¥ï¼‰")
            return
            
        try:
            # é‡æ–°è®¾ç½® MLflowï¼ˆå› ä¸ºåœ¨å­è¿›ç¨‹ä¸­ï¼‰
            import mlflow as mlf
            mlf.set_tracking_uri(mlflow_uri)
            
            # è·å–å½“å‰ epoch
            epoch = trainer.epoch
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"\nğŸ”” MLflow Callback è¢«è°ƒç”¨ - Epoch {epoch} (Rank {trainer.args.rank})")
            
            # æ„å»ºæŒ‡æ ‡å­—å…¸
            metrics = {}
            
            # è®­ç»ƒæŸå¤± (ä» trainer.label_loss_items è·å–)
            if hasattr(trainer, 'label_loss_items') and hasattr(trainer, 'tloss'):
                loss_items = trainer.label_loss_items(trainer.tloss, prefix="train")
                for k, v in loss_items.items():
                    metrics[k] = float(v)
            
            # éªŒè¯æŒ‡æ ‡ (ä» trainer.metrics è·å–)
            if hasattr(trainer, 'metrics') and trainer.metrics:
                metric_dict = trainer.metrics.results_dict
                for k, v in metric_dict.items():
                    # é‡å‘½åæŒ‡æ ‡ä»¥åŒ¹é… MLflow ä¹ æƒ¯
                    if 'metrics/' in k:
                        metrics[k] = float(v)
                    elif k.startswith('val/'):
                        metrics[k] = float(v)
            
            # å­¦ä¹ ç‡
            if hasattr(trainer, 'optimizer'):
                for i, param_group in enumerate(trainer.optimizer.param_groups):
                    metrics[f'lr/pg{i}'] = float(param_group['lr'])
            
            # æ‰¹é‡ä¸ŠæŠ¥æŒ‡æ ‡
            if metrics:
                print(f"ğŸ“Š ä¸ŠæŠ¥ {len(metrics)} ä¸ªæŒ‡æ ‡åˆ° MLflow (run_id={run_id}, step={epoch})")
                # ä½¿ç”¨ run_id ä¸ŠæŠ¥åˆ°æ­£ç¡®çš„ run
                with mlf.start_run(run_id=run_id):
                    mlf.log_metrics(metrics, step=epoch)
                print(f"âœ… æŒ‡æ ‡ä¸ŠæŠ¥æˆåŠŸ")
            else:
                print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯ä¸ŠæŠ¥çš„æŒ‡æ ‡")
                
        except Exception as e:
            print(f"âŒ Callback è®°å½•æŒ‡æ ‡å¤±è´¥ (epoch {epoch}): {e}")
            import traceback
            traceback.print_exc()
    
    return {
        'on_fit_epoch_end': on_fit_epoch_end
    }

def train(
    data_yaml: str,
    model: str = 'yolo11s.pt',
    epochs: int = 100,
    imgsz: int = 640,
    batch: int = 16,
    project_name: str = 'yolo-detection',
    task_name: str = 'experiment',
    mlflow_uri: str = 'http://192.168.16.130:5000/',
    overwrite: bool = False,
    mlflow_parent_run_id: str = None,  # NEW: ç”¨äºåµŒå¥—runs (è°ƒä¼˜æ¨¡å¼)
    skip_mlflow_model_upload: bool = False,  # NEW: è·³è¿‡æ¨¡å‹ä¸Šä¼  (è°ƒä¼˜æ¨¡å¼)
    model_size: str = 'medium',  # NEW: æ¨¡å‹è§„æ¨¡ (nano/small/medium/large/xlarge)
    use_advanced_aug: bool = False,  # NEW: ä½¿ç”¨é«˜çº§æ•°æ®å¢å¼º
    use_hard_mining: bool = False,  # NEW: ä½¿ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜
    **kwargs
):
    """
    è®­ç»ƒ YOLOv11 æ£€æµ‹æ¨¡å‹å¹¶è®°å½•åˆ° MLflow (å®«é¢ˆç»†èƒè¯†åˆ«ä¼˜åŒ–ç‰ˆæœ¬)
    
    Args:
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        model: æ¨¡å‹åç§°æˆ–è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°
        imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
        batch: å•ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼ˆä¼šè‡ªåŠ¨ä¹˜ä»¥GPUæ•°é‡ä½œä¸ºæ€»batchï¼‰
        project_name: MLflow é¡¹ç›®åç§°
        task_name: MLflow è¿è¡Œåç§°
        mlflow_uri: MLflow æœåŠ¡å™¨åœ°å€
        overwrite: å¼ºåˆ¶é‡æ–°å¼€å§‹è®­ç»ƒï¼ˆæ¸…ç©ºç°æœ‰ç›®å½•ï¼‰
        model_size: æ¨¡å‹è§„æ¨¡é€‰æ‹© (nano/small/medium/large/xlarge)
        use_advanced_aug: å¯ç”¨é«˜çº§æ•°æ®å¢å¼ºï¼ˆé’ˆå¯¹ç»†èƒè¯†åˆ«ä¼˜åŒ–ï¼‰
        use_hard_mining: å¯ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜
        **kwargs: å…¶ä»– YOLO è®­ç»ƒå‚æ•°
    """
    
    # ============================================================
    # å®«é¢ˆç»†èƒè¯†åˆ«ä¼˜åŒ–é…ç½®
    # ============================================================
    
    # 1. æ¨¡å‹è§„æ¨¡æ˜ å°„
    model_size_map = {
        'nano': 'yolo11n.pt',
        'small': 'yolo11s.pt',
        'medium': 'yolo11m.pt',
        'large': 'yolo11l.pt',
        'xlarge': 'yolo11x.pt'
    }
    
    # å¦‚æœæŒ‡å®šäº† model_sizeï¼Œè¦†ç›– model å‚æ•°
    if model_size and model_size.lower() in model_size_map:
        model = model_size_map[model_size.lower()]
        if is_main_process():
            print(f"\nğŸ”§ æ¨¡å‹è§„æ¨¡: {model_size.upper()} -> {model}")
    
    # 2. é«˜çº§æ•°æ®å¢å¼ºé…ç½®ï¼ˆé’ˆå¯¹ç»†èƒè¯†åˆ«ä¼˜åŒ–ï¼‰
    if use_advanced_aug:
        if is_main_process():
            print(f"\nğŸ¨ å¯ç”¨é«˜çº§æ•°æ®å¢å¼ºï¼ˆå®«é¢ˆç»†èƒè¯†åˆ«ä¼˜åŒ–ï¼‰")
        
        # ç»†èƒå›¾åƒç‰¹å®šçš„å¢å¼ºå‚æ•°
        advanced_aug_params = {
            # è‰²å½©å¢å¼º - ç»†èƒæŸ“è‰²å˜åŒ–
            'hsv_h': kwargs.get('hsv_h', 0.015),  # è‰²è°ƒå˜åŒ–ï¼ˆè€ƒè™‘æŸ“è‰²å·®å¼‚ï¼‰
            'hsv_s': kwargs.get('hsv_s', 0.7),    # é¥±å’Œåº¦å˜åŒ–
            'hsv_v': kwargs.get('hsv_v', 0.4),    # æ˜åº¦å˜åŒ–
            
            # å‡ ä½•å˜æ¢ - ç»†èƒå§¿æ€å¤šæ ·æ€§
            'degrees': kwargs.get('degrees', 0.0),      # æ—‹è½¬ï¼ˆç»†èƒæ— æ–¹å‘æ€§ï¼‰
            'translate': kwargs.get('translate', 0.1),  # å¹³ç§»
            'scale': kwargs.get('scale', 0.5),          # ç¼©æ”¾ï¼ˆç»†èƒå¤§å°å˜åŒ–ï¼‰
            'shear': kwargs.get('shear', 0.0),          # é”™åˆ‡
            'perspective': kwargs.get('perspective', 0.0), # é€è§†å˜æ¢
            'flipud': kwargs.get('flipud', 0.5),        # å‚ç›´ç¿»è½¬
            'fliplr': kwargs.get('fliplr', 0.5),        # æ°´å¹³ç¿»è½¬
            
            # Mosaic å’Œ MixUp
            'mosaic': kwargs.get('mosaic', 1.0),        # Mosaic å¢å¼º
            'mixup': kwargs.get('mixup', 0.1),          # MixUp å¢å¼º
            'copy_paste': kwargs.get('copy_paste', 0.1), # å¤åˆ¶ç²˜è´´å¢å¼º
        }
        
        # æ›´æ–° kwargs
        kwargs.update(advanced_aug_params)
        
        if is_main_process():
            print(f"   è‰²å½©å¢å¼º: hsv_h={advanced_aug_params['hsv_h']}, hsv_s={advanced_aug_params['hsv_s']}, hsv_v={advanced_aug_params['hsv_v']}")
            print(f"   å‡ ä½•å˜æ¢: scale={advanced_aug_params['scale']}, translate={advanced_aug_params['translate']}")
            print(f"   æ··åˆå¢å¼º: mosaic={advanced_aug_params['mosaic']}, mixup={advanced_aug_params['mixup']}, copy_paste={advanced_aug_params['copy_paste']}")
    
    # 3. å›°éš¾æ ·æœ¬æŒ–æ˜é…ç½®
    if use_hard_mining:
        if is_main_process():
            print(f"\nâ›ï¸  å¯ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜")
        
        # å›°éš¾æ ·æœ¬æŒ–æ˜å‚æ•°
        hard_mining_params = {
            # å¢åŠ å°ç›®æ ‡æƒé‡ï¼ˆç»†èƒå¯èƒ½è¾ƒå°ï¼‰
            'box': kwargs.get('box', 7.5),       # è¾¹æ¡†æŸå¤±æƒé‡
            'cls': kwargs.get('cls', 0.5),       # åˆ†ç±»æŸå¤±æƒé‡
            'dfl': kwargs.get('dfl', 1.5),       # DFLæŸå¤±æƒé‡
            
            # ä¼˜åŒ–å™¨é…ç½®
            'optimizer': kwargs.get('optimizer', 'AdamW'),  # AdamW å¯¹ç»†èŠ‚æ›´æ•æ„Ÿ
            'lr0': kwargs.get('lr0', 0.001),                # åˆå§‹å­¦ä¹ ç‡
            'lrf': kwargs.get('lrf', 0.01),                 # æœ€ç»ˆå­¦ä¹ ç‡ç³»æ•°
            'momentum': kwargs.get('momentum', 0.937),      # åŠ¨é‡
            'weight_decay': kwargs.get('weight_decay', 0.0005),  # æƒé‡è¡°å‡
            
            # å­¦ä¹ ç‡è°ƒåº¦
            'cos_lr': kwargs.get('cos_lr', True),           # ä½™å¼¦å­¦ä¹ ç‡
            'warmup_epochs': kwargs.get('warmup_epochs', 3.0),  # é¢„çƒ­è½®æ•°
            'warmup_momentum': kwargs.get('warmup_momentum', 0.8),
            'warmup_bias_lr': kwargs.get('warmup_bias_lr', 0.1),
            
            # æé«˜æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            'conf': kwargs.get('conf', 0.25),    # ç½®ä¿¡åº¦é˜ˆå€¼
            'iou': kwargs.get('iou', 0.7),       # NMS IoUé˜ˆå€¼
            
            # æ—©åœç­–ç•¥
            'patience': kwargs.get('patience', 100),  # å¢åŠ è€å¿ƒå€¼
        }
        
        # æ›´æ–° kwargs
        kwargs.update(hard_mining_params)
        
        if is_main_process():
            print(f"   æŸå¤±æƒé‡: box={hard_mining_params['box']}, cls={hard_mining_params['cls']}, dfl={hard_mining_params['dfl']}")
            print(f"   ä¼˜åŒ–å™¨: {hard_mining_params['optimizer']}, lr0={hard_mining_params['lr0']}, lrf={hard_mining_params['lrf']}")
            print(f"   å­¦ä¹ ç‡ç­–ç•¥: cos_lr={hard_mining_params['cos_lr']}, warmup_epochs={hard_mining_params['warmup_epochs']}")
            print(f"   æ£€æµ‹é˜ˆå€¼: conf={hard_mining_params['conf']}, iou={hard_mining_params['iou']}")
    
    # 4. é€šç”¨ç»†èƒè¯†åˆ«ä¼˜åŒ–
    cell_optimizations = {
        # å…³é—­ Mosaic çš„æ—¶æœºï¼ˆæœ€å10ä¸ªepochï¼‰
        'close_mosaic': kwargs.get('close_mosaic', 10),
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        'amp': kwargs.get('amp', True),
        
        # å¤šå°ºåº¦è®­ç»ƒ
        'multi_scale': kwargs.get('multi_scale', True),
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        'save': kwargs.get('save', True),
        'save_period': kwargs.get('save_period', -1),  # -1è¡¨ç¤ºåªä¿å­˜bestå’Œlast
    }
    
    kwargs.update(cell_optimizations)
    
    if is_main_process():
        print(f"\nğŸ”¬ ç»†èƒè¯†åˆ«é€šç”¨ä¼˜åŒ–:")
        print(f"   close_mosaic={cell_optimizations['close_mosaic']}, amp={cell_optimizations['amp']}")
        print(f"   multi_scale={cell_optimizations['multi_scale']}")
    
    # ============================================================
    
    # è®¡ç®—GPUæ•°é‡å¹¶è°ƒæ•´batch size
    # ç”¨æˆ·ä¼ å…¥çš„ batch ä½œä¸ºå•GPUçš„batchï¼Œéœ€è¦ä¹˜ä»¥GPUæ•°é‡
    device_str = kwargs.get('device', '0')
    if isinstance(device_str, str) and ',' in device_str:
        # å¤šGPU: "0,1,2,3" -> 4ä¸ªGPU
        world_size = len(device_str.split(','))
    elif isinstance(device_str, (list, tuple)):
        world_size = len(device_str)
    else:
        # å•GPU
        world_size = 1
    
    # ä¿å­˜ç”¨æˆ·è®¾ç½®çš„å•GPU batchï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    batch_per_gpu = batch
    # è®¡ç®—æ€»batchï¼ˆYOLOä¼šè‡ªåŠ¨é™¤ä»¥world_sizeï¼Œæ‰€ä»¥æˆ‘ä»¬é¢„å…ˆä¹˜ä¸Šå»ï¼‰
    total_batch = batch_per_gpu * world_size
    
    if is_main_process():
        print(f"\n{'='*80}")
        print(f"ğŸ”§ Batch Size è‡ªåŠ¨è°ƒæ•´")
        print(f"{'='*80}")
        print(f"æ£€æµ‹åˆ°GPUæ•°é‡: {world_size}")
        print(f"ç”¨æˆ·è®¾ç½® (å•GPU batch): {batch_per_gpu}")
        print(f"æ€»batch (ä¼ é€’ç»™YOLO): {total_batch}")
        print(f"YOLOå†…éƒ¨è®¡ç®— (æ¯GPUå®é™…batch): {total_batch} Ã· {world_size} = {batch_per_gpu}")
        print(f"{'='*80}\n")
    
    # ä½¿ç”¨è°ƒæ•´åçš„æ€»batch
    batch = total_batch
    
    # è®¾ç½® MinIO/S3 è®¿é—®å‡­æ®ï¼ˆç”¨äº MLflow artifacts å­˜å‚¨ï¼‰
    os.environ['AWS_ACCESS_KEY_ID'] = 'mlflow'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'mlflow@SN'
    os.environ['AWS_ENDPOINT_URL'] = 'http://192.168.16.130:9000'
    os.environ['AWS_REGION'] = ''
    os.environ['MLFLOW_S3_IGNORE_TLS'] = 'true'
    
    # è®¾ç½® MLflow
    mlflow.set_tracking_uri(mlflow_uri)
    
    # å¤„ç†å·²åˆ é™¤çš„å®éªŒ
    try:
        mlflow.set_experiment(project_name)
    except mlflow.exceptions.MlflowException as e:
        if "deleted experiment" in str(e):
            if is_main_process():
                print(f"âš ï¸  å®éªŒ '{project_name}' å·²è¢«åˆ é™¤ï¼Œæ­£åœ¨æ¢å¤...")
            # è·å– MLflow å®¢æˆ·ç«¯
            client = mlflow.tracking.MlflowClient()
            # æŸ¥æ‰¾å·²åˆ é™¤çš„å®éªŒ
            exp = client.get_experiment_by_name(project_name)
            if exp and exp.lifecycle_stage == 'deleted':
                # æ¢å¤å®éªŒ
                client.restore_experiment(exp.experiment_id)
                if is_main_process():
                    print(f"âœ… å·²æ¢å¤å®éªŒ '{project_name}' (ID: {exp.experiment_id})")
                mlflow.set_experiment(project_name)
            else:
                if is_main_process():
                    print(f"âŒ æ— æ³•æ¢å¤å®éªŒ '{project_name}'")
                raise
        else:
            raise
    
    # ç¡®å®šæ€§ç›®å½•ç®¡ç†
    run_dir = Path('runs') / project_name / task_name
    last_pt = run_dir / 'weights' / 'last.pt'
    resume_training = False
    training_mode = "æ–°è®­ç»ƒ"
    saved_args = None
    saved_epoch = -1
    
    # æ£€æŸ¥ç›®å½•å’Œæ¢å¤é€»è¾‘
    if run_dir.exists():
        if overwrite:
            if is_main_process():
                print(f"ğŸ”„ å¼ºåˆ¶è¦†ç›–ç°æœ‰è®­ç»ƒï¼Œä»å¤´å¼€å§‹...")
            shutil.rmtree(run_dir)
            training_mode = "è¦†ç›–è®­ç»ƒ"
        elif last_pt.exists():
            # å‘ç° checkpointï¼Œè‡ªåŠ¨æ¢å¤è®­ç»ƒ
            resume_training = True
            training_mode = "æ¢å¤è®­ç»ƒ"
            
            try:
                import torch
                checkpoint = torch.load(last_pt, map_location='cpu', weights_only=False)
                saved_epoch = checkpoint.get('epoch', -1)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢åŠ  epochs
                if saved_epoch >= epochs - 1:
                    if is_main_process():
                        print(f"âš ï¸  æ£€æµ‹åˆ°è®­ç»ƒå·²å®Œæˆ {saved_epoch + 1} ä¸ª epoch")
                        print(f"âš ï¸  å½“å‰ --epochs={epochs}ï¼Œè®­ç»ƒæ— æ³•ç»§ç»­")
                        print(f"ğŸ’¡ è§£å†³æ–¹æ³•ï¼š")
                        print(f"   1. å¢åŠ  epochs å‚æ•°ï¼š--epochs {saved_epoch + 100}")
                        print(f"   2. ä½¿ç”¨ --overwrite ä»å¤´å¼€å§‹è®­ç»ƒ")
                        print(f"   3. åˆ é™¤ checkpoint åé‡æ–°è®­ç»ƒ")
                    raise ValueError(f"è®­ç»ƒå·²å®Œæˆï¼Œéœ€è¦å¢åŠ  epochs æˆ–ä½¿ç”¨ --overwrite")
                
                if is_main_process():
                    print(f"ğŸ”„ æ£€æµ‹åˆ°ç°æœ‰è®­ç»ƒï¼Œä» epoch {saved_epoch + 1} æ¢å¤åˆ° epoch {epochs}...")
                
                # è¯»å–è®­ç»ƒå‚æ•°
                if 'train_args' in checkpoint:
                    saved_args = checkpoint['train_args']
                    if is_main_process():
                        print(f"ğŸ“‹ ä» checkpoint åŠ è½½è®­ç»ƒå‚æ•°...")
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä» args.yaml è¯»å–
                    args_yaml_path = run_dir / 'args.yaml'
                    if args_yaml_path.exists():
                        with open(args_yaml_path, 'r') as f:
                            saved_args = yaml.safe_load(f)
                        if is_main_process():
                            print(f"ğŸ“‹ ä» args.yaml åŠ è½½è®­ç»ƒå‚æ•°...")
                
                # æ˜¾ç¤ºä¼˜åŒ–å™¨çŠ¶æ€
                if 'optimizer' in checkpoint and checkpoint['optimizer'] is not None:
                    opt_state = checkpoint['optimizer']
                    if 'param_groups' in opt_state and len(opt_state['param_groups']) > 0:
                        current_lr = opt_state['param_groups'][0].get('lr', 'N/A')
                        if is_main_process():
                            print(f"ğŸ“Š ä¼˜åŒ–å™¨å½“å‰å­¦ä¹ ç‡: {current_lr}")
            except Exception as e:
                if is_main_process():
                    print(f"ğŸ”„ æ£€æµ‹åˆ°ç°æœ‰è®­ç»ƒï¼Œç»§ç»­è®­ç»ƒ...")
                    print(f"âš ï¸  è¯»å–è®­ç»ƒå‚æ•°å¤±è´¥: {e}")
                # å¦‚æœæ˜¯æˆ‘ä»¬ä¸»åŠ¨æŠ›å‡ºçš„é”™è¯¯ï¼Œå‘ä¸Šä¼ æ’­
                if isinstance(e, ValueError) and "è®­ç»ƒå·²å®Œæˆ" in str(e):
                    raise
        else:
            if is_main_process():
                print(f"âš ï¸  ç›®å½•å­˜åœ¨ä½†æ— æ£€æŸ¥ç‚¹ï¼Œå°†é‡æ–°è®­ç»ƒ")
            shutil.rmtree(run_dir)
            training_mode = "é‡æ–°è®­ç»ƒ"
    
    # å‚æ•°ä¼˜å…ˆçº§ï¼šcheckpoint > å‘½ä»¤è¡Œ
    # æ¢å¤è®­ç»ƒæ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ checkpoint ä¸­çš„å‚æ•°
    if resume_training and saved_args:
        if is_main_process():
            print(f"\nğŸ“‹ æ¢å¤è®­ç»ƒ - ä» checkpoint åŠ è½½å‚æ•°:")
            print(f"{'å‚æ•°':<20} {'Checkpointå€¼':<20} {'å‘½ä»¤è¡Œå€¼':<20} {'å®é™…ä½¿ç”¨':<20}")
            print(f"{'-'*80}")
        
        # ä» checkpoint æ¢å¤æ‰€æœ‰è®­ç»ƒå‚æ•°
        param_keys = ['lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_epochs', 
                      'warmup_momentum', 'warmup_bias_lr', 'box', 'cls', 'dfl', 
                      'optimizer', 'close_mosaic', 'amp', 'patience', 'cos_lr',
                      'hsv_h', 'hsv_s', 'hsv_v', 'degrees', 'translate', 
                      'scale', 'mosaic', 'mixup', 'fliplr']
        
        for key in param_keys:
            checkpoint_val = saved_args.get(key)
            cmdline_val = kwargs.get(key)
            
            if checkpoint_val is not None:
                # ä¼˜å…ˆä½¿ç”¨ checkpoint ä¸­çš„å€¼
                kwargs[key] = checkpoint_val
                if is_main_process():
                    if cmdline_val is not None and cmdline_val != checkpoint_val:
                        status = "âš ï¸  å·²å¿½ç•¥å‘½ä»¤è¡Œ"
                    else:
                        status = "âœ“ å·²æ¢å¤"
                    print(f"{key:<20} {str(checkpoint_val):<20} {str(cmdline_val):<20} {str(checkpoint_val):<20} [{status}]")
        
        if is_main_process():
            print(f"{'-'*80}")
        
        # åŸºç¡€å‚æ•°å¯¹æ¯”ï¼ˆepochs, imgsz, batch å¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
        for key in ['epochs', 'imgsz', 'batch']:
            checkpoint_val = saved_args.get(key)
            current_val = locals()[key]
            if checkpoint_val is not None:
                if checkpoint_val != current_val:
                    if is_main_process():
                        print(f"âš ï¸  {key}: checkpoint={checkpoint_val}, å‘½ä»¤è¡Œ={current_val}, ä½¿ç”¨å‘½ä»¤è¡Œå€¼ï¼ˆå¯èƒ½éœ€è¦è°ƒæ•´ï¼‰")
        
        if is_main_process():
            print()
    
    # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„å‚æ•°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    if is_main_process():
        print(f"\n{'='*80}")
        print(f"ğŸš€ YOLOv11 æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
        print(f"{'='*80}")
        print(f"æ¨¡å‹: {model}")
        print(f"æ•°æ®é›†: {data_yaml}")
        print(f"è®­ç»ƒæ¨¡å¼: {training_mode}")
        print(f"MLflow é¡¹ç›®: {project_name}")
        print(f"å®éªŒåç§°: {task_name}")
        print(f"è¿è¡Œç›®å½•: {run_dir}")
        
        # æ˜¾ç¤ºå‚æ•°æ¥æº
        if resume_training and saved_args:
            print(f"å‚æ•°æ¥æº: âœ“ Checkpoint (ä» epoch {saved_epoch + 1} æ¢å¤)")
        else:
            print(f"å‚æ•°æ¥æº: å‘½ä»¤è¡Œå‚æ•°")
        
        print(f"\nğŸ“Š åŸºç¡€è®­ç»ƒå‚æ•°:")
        print(f"   epochs: {epochs}")
        print(f"   imgsz: {imgsz}")
        print(f"   batch (æ€»batch): {batch}")
        print(f"   batch (æ¯GPU): {batch_per_gpu}")
        print(f"   GPUæ•°é‡: {world_size}")
        
        # æ˜¾ç¤ºè®­ç»ƒè¶…å‚æ•°ï¼ˆä¼˜å…ˆä» kwargs è·å–ï¼Œå¦‚æœæ˜¯ resume æ¨¡å¼ï¼Œkwargs å·²ç»åŒ…å« checkpoint çš„å€¼ï¼‰
        print(f"\nğŸ“š å­¦ä¹ ç‡ä¸ä¼˜åŒ–å™¨:")
        lr_params = ['lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_epochs', 
                     'warmup_momentum', 'warmup_bias_lr', 'optimizer', 'cos_lr']
        for key in lr_params:
            value = kwargs.get(key, 'é»˜è®¤')
            if value != 'é»˜è®¤' or (resume_training and saved_args and saved_args.get(key) is not None):
                print(f"   {key}: {value}")
        
        # æ˜¾ç¤ºæŸå¤±æƒé‡
        print(f"\nâš–ï¸  æŸå¤±æƒé‡:")
        loss_params = ['box', 'cls', 'dfl']
        for key in loss_params:
            value = kwargs.get(key, 'é»˜è®¤')
            if value != 'é»˜è®¤':
                print(f"   {key}: {value}")
        
        # æ˜¾ç¤ºæ•°æ®å¢å¼ºå‚æ•°
        print(f"\nğŸ¨ æ•°æ®å¢å¼º:")
        aug_param_keys = ['hsv_h', 'hsv_s', 'hsv_v', 'degrees', 'translate', 
                          'scale', 'mosaic', 'mixup', 'close_mosaic', 'fliplr']
        aug_params = {k: kwargs.get(k) for k in aug_param_keys if kwargs.get(k) is not None}
        if aug_params:
            for key, value in aug_params.items():
                print(f"   {key}: {value}")
        else:
            print(f"   ä½¿ç”¨é»˜è®¤å€¼")
        
        # æ˜¾ç¤ºå…¶ä»–å‚æ•°
        known_keys = set(['lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_epochs', 
                          'warmup_momentum', 'warmup_bias_lr', 'optimizer', 'cos_lr',
                          'box', 'cls', 'dfl', 'patience', 'amp', 'project', 'name'] + aug_param_keys)
        other_params = {k: v for k, v in kwargs.items() if k not in known_keys and v is not None}
        if other_params:
            print(f"\nâš™ï¸  å…¶ä»–å‚æ•°:")
            for key, value in other_params.items():
                print(f"   {key}: {value}")
        
        print(f"{'='*80}\n")
    
    # å¼€å§‹MLflowè¿½è¸ª
    if is_main_process():
        print(f"\nğŸ“Š å¯åŠ¨ MLflow è¿½è¸ª...")
        print(f"   æ¨¡å¼: {training_mode}")
        print(f"   é¡¹ç›®: {project_name}")
        print(f"   ä»»åŠ¡: {task_name}")
    
    # å¤„ç†åµŒå¥— run (è°ƒä¼˜æ¨¡å¼)
    if mlflow_parent_run_id:
        # è°ƒä¼˜æ¨¡å¼ï¼šé‡ç”¨çˆ¶ trial run
        existing_run = mlflow.active_run()
        if existing_run and existing_run.info.run_id == mlflow_parent_run_id:
            run = existing_run
            if is_main_process():
                print(f"   âœ… ä½¿ç”¨è°ƒä¼˜ Trial Run: {mlflow_parent_run_id}")
        else:
            # éä¸»è¿›ç¨‹æˆ– run ä¸æ´»è·ƒï¼šä»…ä¸»è¿›ç¨‹åˆ›å»º run
            if is_main_process():
                print(f"   âš ï¸  è­¦å‘Š: çˆ¶runä¸æ˜¯æ´»è·ƒçŠ¶æ€ï¼Œåˆ›å»ºæ–°run")
                run = mlflow.start_run(run_name=task_name)
            else:
                # éä¸»è¿›ç¨‹ï¼šåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿ contextï¼Œä¸å®é™…åˆ›å»º MLflow run
                from contextlib import nullcontext
                run = nullcontext()
    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼ï¼šä»…ä¸»è¿›ç¨‹åˆ›å»º run
        if is_main_process():
            run = mlflow.start_run(run_name=task_name)
            print(f"   âœ… åˆ›å»ºæ–°çš„ MLflow Run")
        else:
            # éä¸»è¿›ç¨‹ï¼šä¸åˆ›å»º MLflow run
            from contextlib import nullcontext
            run = nullcontext()
    
    with run:
        # è·å– run_idï¼ˆä»…ä¸»è¿›ç¨‹æˆ–æœ‰æ•ˆ runï¼‰
        run_id = None
        if hasattr(run, 'info'):
            run_id = run.info.run_id
            if is_main_process():
                print(f"ğŸ“Š MLflow Run ID: {run_id}")
        
        # è®°å½•è®­ç»ƒå‚æ•°ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process() and run_id:
            params = {
                'model': model,
                'epochs': epochs,
                'imgsz': imgsz,
                'batch_total': batch,
                'batch_per_gpu': batch_per_gpu,
                'world_size': world_size,
                'data_yaml': data_yaml,
                'model_size': model_size,
                'use_advanced_aug': use_advanced_aug,
                'use_hard_mining': use_hard_mining,
            }
            
            # æ·»åŠ å…¶ä»–è®­ç»ƒå‚æ•°
            for key, value in kwargs.items():
                if value is not None and key not in ['project', 'name']:
                    params[f'train/{key}'] = value
            
            mlflow.log_params(params)
        
        # è¯»å–æ•°æ®é›†ä¿¡æ¯
        try:
            with open(data_yaml, 'r') as f:
                data_config = yaml.safe_load(f)
            
            # è®°å½•æ•°æ®é›†ä¿¡æ¯ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and run_id:
                mlflow.log_param('dataset/classes', str(data_config.get('names', [])))
                mlflow.log_param('dataset/path', data_config.get('path', ''))
            
            if is_main_process():
                print(f"ğŸ“ æ•°æ®é›†ä¿¡æ¯:")
                print(f"   ç±»åˆ«æ•°: {len(data_config.get('names', []))}")
                print(f"   ç±»åˆ«: {data_config.get('names', [])}")
                print(f"   æ•°æ®è·¯å¾„: {data_config.get('path', '')}")
                print()
        except Exception as e:
            if is_main_process():
                print(f"âš ï¸  è¯»å–æ•°æ®é›†é…ç½®å¤±è´¥: {e}")
                print(f"å½“å‰è·¯å¾„: {os.getcwd()}")
            exit(-1)
        
        # åŠ è½½æ¨¡å‹
        if is_main_process():
            print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {model}")
        yolo_model = YOLO(model)
        
        # è®­ç»ƒï¼ˆYOLO ä¼šè‡ªåŠ¨è®°å½•åˆ° runs/ ç›®å½•ï¼‰
        if is_main_process():
            print(f"\n{'='*80}")
            print(f"ğŸƒ å¼€å§‹è®­ç»ƒ...")
            print(f"{'='*80}\n")
        
        # ç¦ç”¨ Ultralytics å†…ç½®çš„ MLflow é›†æˆ
        from ultralytics.utils import SETTINGS
        mlflow_enabled_backup = SETTINGS.get('mlflow', True)
        SETTINGS['mlflow'] = False
        
        mlflow_uri_backup = os.environ.get('MLFLOW_TRACKING_URI', '')
        os.environ['MLFLOW_TRACKING_URI'] = 'http://192.168.16.130:5000'
        os.environ['AWS_ACCESS_KEY_ID'] = 'mlflow'
        os.environ['AWS_SECRET_ACCESS_KEY'] = 'mlflow@SN'
        os.environ['AWS_ENDPOINT_URL'] = 'http://192.168.16.130:9000'
        os.environ['MLFLOW_S3_IGNORE_TLS'] = 'true'

        # æ„å»ºè®­ç»ƒå‚æ•°
        train_kwargs = {
            'data': data_yaml,
            'epochs': epochs,
            'imgsz': imgsz,
            'batch': batch,
            'project': str(run_dir.parent),
            'name': run_dir.name,
            'exist_ok': True,
            **kwargs
        }
        
        # å¦‚æœéœ€è¦æ¢å¤è®­ç»ƒï¼Œè®¾ç½® resume
        if resume_training:
            train_kwargs['resume'] = True
            # é‡æ–°åŠ è½½æ¨¡å‹ç”¨äºæ¢å¤
            yolo_model = YOLO(str(last_pt))
        
        # æ·»åŠ  MLflow å›è°ƒï¼ˆå®æ—¶ä¸ŠæŠ¥æŒ‡æ ‡ï¼‰
        # å¿…é¡»åœ¨æ¨¡å‹æœ€ç»ˆåŠ è½½åæ³¨å†Œï¼Œé¿å…è¢«è¦†ç›–
        if run_id:
            print(f"ğŸ“Š æ³¨å†Œ MLflow å›è°ƒå‡½æ•°ï¼ˆå®æ—¶ä¸ŠæŠ¥è®­ç»ƒæŒ‡æ ‡ï¼‰")
            print(f"   Run ID: {run_id}")
            print(f"   MLflow URI: {mlflow_uri}")
            mlflow_callbacks = create_mlflow_callbacks(run_id, mlflow_uri)
            for event, func in mlflow_callbacks.items():
                yolo_model.add_callback(event, func)
            print(f"   âœ… å·²æ³¨å†Œ {len(mlflow_callbacks)} ä¸ªå›è°ƒäº‹ä»¶")
        
        results = yolo_model.train(**train_kwargs)
        
        # æ¢å¤ MLflow è®¾ç½®
        SETTINGS['mlflow'] = mlflow_enabled_backup
        if mlflow_uri_backup:
            os.environ['MLFLOW_TRACKING_URI'] = mlflow_uri_backup
        
        # è·å–è®­ç»ƒç»“æœè·¯å¾„
        # YOLO æŸäº›ç‰ˆæœ¬ train() å¯èƒ½è¿”å› Noneï¼Œä½¿ç”¨æˆ‘ä»¬é¢„è®¾çš„ run_dir
        save_dir = Path(results.save_dir) if results and hasattr(results, 'save_dir') else run_dir
        if is_main_process():
            print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
        
        # è®°å½•æœ€ç»ˆæŒ‡æ ‡ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            print(f"\nğŸ“Š è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ° MLflow...")
            
        if (save_dir / 'results.csv').exists() and is_main_process() and run_id:
            results_df = pd.read_csv(save_dir / 'results.csv')
            results_df = results_df.fillna(0)  # å¡«å…… NaN
            
            # è®°å½•æœ€åä¸€è½®çš„æŒ‡æ ‡
            last_metrics = results_df.iloc[-1]
            final_metrics = {
                'final/mAP50': float(last_metrics.get('metrics/mAP50(B)', 0)),
                'final/mAP50-95': float(last_metrics.get('metrics/mAP50-95(B)', 0)),
                'final/precision': float(last_metrics.get('metrics/precision(B)', 0)),
                'final/recall': float(last_metrics.get('metrics/recall(B)', 0)),
                'final/box_loss': float(last_metrics.get('train/box_loss', 0)),
                'final/cls_loss': float(last_metrics.get('train/cls_loss', 0)),
                'final/dfl_loss': float(last_metrics.get('train/dfl_loss', 0)),
            }
            mlflow.log_metrics(final_metrics)
            
            print(f"   âœ… æœ€ç»ˆæŒ‡æ ‡:")
            print(f"      mAP@0.5: {final_metrics['final/mAP50']:.4f}")
            print(f"      mAP@0.5:0.95: {final_metrics['final/mAP50-95']:.4f}")
            print(f"      Precision: {final_metrics['final/precision']:.4f}")
            print(f"      Recall: {final_metrics['final/recall']:.4f}")
            
            # ä¸Šä¼ å®Œæ•´ç»“æœï¼ˆè·³è¿‡è°ƒä¼˜æ¨¡å¼ï¼‰
            if not skip_mlflow_model_upload:
                mlflow.log_artifact(str(save_dir / 'results.csv'), 'training_results')
        elif is_main_process() and (save_dir / 'results.csv').exists() == False:
                print(f"   âš ï¸  æœªæ‰¾åˆ° results.csv")
        
        # ä¸Šä¼ æ¨¡å‹å’Œ artifactsï¼ˆä»…ä¸»è¿›ç¨‹ä¸”éè°ƒä¼˜æ¨¡å¼ï¼‰
        if is_main_process() and run_id and not skip_mlflow_model_upload:
            print(f"\nğŸ“¦ ä¸Šä¼ æ¨¡å‹åˆ° MLflow...")
            best_model = save_dir / 'weights' / 'best.pt'
            last_model = save_dir / 'weights' / 'last.pt'
            
            if best_model.exists():
                mlflow.log_artifact(str(best_model), 'models')
                print(f"   âœ… best.pt å·²ä¸Šä¼ ")
                
                # å¯¼å‡º ONNX æ ¼å¼
                print(f"\nğŸ”„ å¯¼å‡º best.pt ä¸º ONNX æ ¼å¼...")
                try:
                    onnx_path = best_model.with_suffix('.onnx')
                    # ä½¿ç”¨ä¸“ç”¨çš„å¯¼å‡ºå‡½æ•°ï¼ˆåŠ¨æ€ batchï¼Œå›ºå®šå…¶ä»–ç»´åº¦ï¼‰
                    export_yolo_to_onnx(
                        model_path=str(best_model),
                        imgsz=imgsz,
                        output_path=str(onnx_path)
                    )
                    
                    if onnx_path.exists():
                        print(f"   âœ… ONNX å¯¼å‡ºæˆåŠŸ: {onnx_path.name}")
                        # ä¸Šä¼  ONNX æ¨¡å‹åˆ° MLflow
                        mlflow.log_artifact(str(onnx_path), 'models')
                        print(f"   âœ… ONNX æ¨¡å‹å·²ä¸Šä¼ åˆ° MLflow")
                    else:
                        print(f"   âš ï¸  ONNX æ–‡ä»¶æœªæ‰¾åˆ°")
                except Exception as e:
                    print(f"   âš ï¸  ONNX å¯¼å‡ºå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            if last_model.exists():
                mlflow.log_artifact(str(last_model), 'models')
                print(f"   âœ… last.pt å·²ä¸Šä¼ ")
            
            # ä¸Šä¼ å¯è§†åŒ–å›¾è¡¨
            print(f"\nğŸ“ˆ ä¸Šä¼ å¯è§†åŒ–å›¾è¡¨...")
            plots = ['confusion_matrix.png', 'results.png', 'PR_curve.png', 
                    'F1_curve.png', 'labels.jpg', 'labels_correlogram.jpg']
            
            uploaded_plots = 0
            for plot in plots:
                plot_path = save_dir / plot
                if plot_path.exists():
                    mlflow.log_artifact(str(plot_path), 'plots')
                    uploaded_plots += 1
            
            print(f"   âœ… ä¸Šä¼ äº† {uploaded_plots} ä¸ªå›¾è¡¨")
            # ä¸Šä¼ è®­ç»ƒé…ç½®
            args_yaml = save_dir / 'args.yaml'
            if args_yaml.exists():
                mlflow.log_artifact(str(args_yaml), 'config')
        
        if is_main_process():
            print(f"\n{'='*80}")
            print(f"âœ… è®­ç»ƒå®Œæˆå¹¶è®°å½•åˆ° MLflowï¼")
            print(f"{'='*80}")
            if run_id:
                print(f"ğŸ“Š MLflow Run ID: {run_id}")
                print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {save_dir}")
                if hasattr(run, 'info'):
                    print(f"ğŸŒ MLflow UI: {mlflow_uri}#/experiments/{run.info.experiment_id}/runs/{run_id}")
            else:
                print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {save_dir}")
            print(f"{'='*80}\n")
        
        return results, run_id

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='YOLOv11 æ£€æµ‹æ¨¡å‹è®­ç»ƒ - MLflow é›†æˆ (æ”¯æŒæ–­ç‚¹ç»­è®­)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®«é¢ˆç»†èƒè¯†åˆ« - é«˜ç²¾åº¦è®­ç»ƒ
  python yolo/train.py \\
      --data docs/cell-core/cell.yaml \\
      --model yolo11n.pt \\
      --epochs 5000 \\
      --imgsz 1024 \\
      --batch 8 \\
      --device 0,1,2,3,4 \\
      --model_size medium \\
      --use_advanced_aug \\
      --use_hard_mining \\
      --project_name cell-box-modify \\
      --task_name high-accuracy
  
  # åŸºç¡€è®­ç»ƒ (å•GPU)
  python yolo/train.py --data pk-dataset.yaml --model yolo11s.pt --epochs 100 --batch 16

  # å¤šGPUè®­ç»ƒ (batchå‚æ•°ä¸ºå•GPUçš„batchï¼Œè‡ªåŠ¨ä¹˜ä»¥GPUæ•°)
  python yolo/train.py \\
      --data dna-classify-cluster.yaml \\
      --model yolo11m.pt \\
      --epochs 200 \\
      --batch 4 \\          # å•GPU batch=4, 8ä¸ªGPUæ€»batch=32
      --device 0,1,2,3,4,5,6,7 \\
      --project_name dna-detection \\
      --task_name yolo11m-exp1
  
  # æ¢å¤è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹ last.ptï¼‰
  python yolo/train.py \\
      --data dataset.yaml \\
      --project_name my-project \\
      --task_name exp-001 \\
      --epochs 300
  
  # å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰
  python yolo/train.py \\
      --data dataset.yaml \\
      --project_name my-project \\
      --task_name exp-001 \\
      --epochs 100 \\
      --overwrite

å®«é¢ˆç»†èƒè¯†åˆ«ä¼˜åŒ–:
  --model_size: é€‰æ‹©æ¨¡å‹è§„æ¨¡ (nano/small/medium/large/xlarge)
  --use_advanced_aug: å¯ç”¨é’ˆå¯¹ç»†èƒå›¾åƒçš„é«˜çº§æ•°æ®å¢å¼º
                      - è‰²å½©å¢å¼ºï¼šæ¨¡æ‹ŸæŸ“è‰²å·®å¼‚
                      - å‡ ä½•å˜æ¢ï¼šç»†èƒå§¿æ€å¤šæ ·æ€§
                      - æ··åˆå¢å¼ºï¼šMosaic, MixUp, Copy-Paste
  --use_hard_mining: å¯ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜
                     - Focal Losså…³æ³¨å›°éš¾æ ·æœ¬
                     - å¢åŠ å°ç›®æ ‡æ£€æµ‹æƒé‡
                     - ä¼˜åŒ–å­¦ä¹ ç‡è°ƒåº¦
                     - æé«˜æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼

é‡è¦è¯´æ˜:
  --batch å‚æ•°ä»£è¡¨"å•ä¸ªGPU"çš„batch sizeï¼Œç¨‹åºä¼šè‡ªåŠ¨ä¹˜ä»¥GPUæ•°é‡
  ä¾‹å¦‚: --batch 4 --device 0,1,2,3,4,5,6,7
       å®é™…æ€»batch = 4 Ã— 8 = 32
       æ¯ä¸ªGPUåˆ†é… = 32 Ã· 8 = 4 âœ“
      
æ”¯æŒæ‰€æœ‰ YOLO è®­ç»ƒå‚æ•°ï¼Œå¦‚: device, workers, patience, optimizer, lr0, lrf, 
hsv_h, hsv_s, hsv_v, degrees, translate, scale, mosaic, mixup, close_mosaic ç­‰

ç›®å½•ç®¡ç†:
  - è®­ç»ƒç»“æœä¿å­˜åœ¨: runs/{project_name}/{task_name}/
  - å¦‚æœç›®å½•å­˜åœ¨ä¸”åŒ…å« last.ptï¼Œè‡ªåŠ¨æ¢å¤è®­ç»ƒ
  - ä½¿ç”¨ --overwrite å¼ºåˆ¶ä»å¤´å¼€å§‹
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--data', type=str, required=True, 
                       help='æ•°æ®é›† YAML é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--model', type=str, default='yolo11s.pt',
                       help='YOLO æ¨¡å‹ (é»˜è®¤: yolo11s.pt, ä¼šè¢« --model_size è¦†ç›–)')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 100)')
    parser.add_argument('--imgsz', type=int, default=640, 
                       help='è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤: 640)')
    parser.add_argument('--batch', type=int, default=16, 
                       help='å•GPUçš„æ‰¹æ¬¡å¤§å° (é»˜è®¤: 16, ä¼šè‡ªåŠ¨ä¹˜ä»¥GPUæ•°é‡)')
    
    # å®«é¢ˆç»†èƒè¯†åˆ«ä¸“ç”¨å‚æ•°
    parser.add_argument('--model_size', type=str, 
                       choices=['nano', 'small', 'medium', 'large', 'xlarge'],
                       help='æ¨¡å‹è§„æ¨¡ (nano/small/medium/large/xlarge)')
    parser.add_argument('--use_advanced_aug', action='store_true',
                       help='å¯ç”¨é«˜çº§æ•°æ®å¢å¼ºï¼ˆé’ˆå¯¹ç»†èƒè¯†åˆ«ä¼˜åŒ–ï¼‰')
    parser.add_argument('--use_hard_mining', action='store_true',
                       help='å¯ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜')
    
    # MLflow å‚æ•°
    parser.add_argument('--project_name', type=str, default='yolo-detection',
                       help='MLflow é¡¹ç›®åç§° (é»˜è®¤: yolo-detection)')
    parser.add_argument('--task_name', type=str, default='experiment',
                       help='MLflow è¿è¡Œåç§° (é»˜è®¤: experiment)')
    parser.add_argument('--mlflow_uri', type=str, 
                       default='http://192.168.16.130:5000/',
                       help='MLflow æœåŠ¡å™¨åœ°å€')
    
    # ç›®å½•ç®¡ç†å‚æ•°
    parser.add_argument('--overwrite', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°å¼€å§‹è®­ç»ƒï¼Œæ¸…ç©ºç°æœ‰ç›®å½•ï¼ˆé»˜è®¤: Falseï¼Œè‡ªåŠ¨æ¢å¤è®­ç»ƒï¼‰')
    
    # è§£æå·²çŸ¥å‚æ•°ï¼Œå…¶ä½™å‚æ•°ä¼ é€’ç»™ YOLO
    args, unknown = parser.parse_known_args()
    
    # è§£ææœªçŸ¥å‚æ•° (YOLO å‚æ•°)
    yolo_kwargs = {}
    i = 0
    while i < len(unknown):
        arg = unknown[i]
        if arg.startswith('--'):
            key = arg[2:]
            # æ£€æŸ¥ä¸‹ä¸€ä¸ªæ˜¯å¦æ˜¯å€¼
            if i + 1 < len(unknown) and not unknown[i + 1].startswith('--'):
                value = unknown[i + 1]
                # å°è¯•è½¬æ¢ç±»å‹
                try:
                    # å°è¯•è½¬ä¸º int
                    yolo_kwargs[key] = int(value)
                except ValueError:
                    try:
                        # å°è¯•è½¬ä¸º float
                        yolo_kwargs[key] = float(value)
                    except ValueError:
                        # å¸ƒå°”å€¼
                        if value.lower() in ['true', 'false']:
                            yolo_kwargs[key] = value.lower() == 'true'
                        else:
                            yolo_kwargs[key] = value
                i += 2
            else:
                # æ— å€¼çš„å‚æ•°ï¼Œå½“ä½œ True
                yolo_kwargs[key] = True
                i += 1
        else:
            i += 1
    
    # æ‰“å°æ‰€æœ‰å‚æ•°
    if yolo_kwargs:
        print(f"\nğŸ“ é¢å¤– YOLO å‚æ•°: {yolo_kwargs}\n")
    
    # è®­ç»ƒ
    train(
        data_yaml=args.data,
        model=args.model,
        epochs=args.epochs,
        imgsz=args.imgsz,
        batch=args.batch,
        project_name=args.project_name,
        task_name=args.task_name,
        mlflow_uri=args.mlflow_uri,
        overwrite=args.overwrite,
        model_size=args.model_size,
        use_advanced_aug=args.use_advanced_aug,
        use_hard_mining=args.use_hard_mining,
        **yolo_kwargs
    )
