"""
YOLOè¶…å‚æ•°è°ƒä¼˜ä¸»è„šæœ¬

æ”¯æŒç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ã€è´å¶æ–¯ä¼˜åŒ–ï¼ˆOptunaï¼‰
æ”¯æŒåˆ†å¸ƒå¼å¤šGPUè®­ç»ƒ
"""

import argparse
import yaml
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional
import mlflow
import torch
import torch.distributed as dist

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥å…±äº«å·¥å…·æ¨¡å—
from utils.tuning import create_search_strategy, OptunaSearchStrategy
from utils.mlflow_tuning import (
    TuningExperimentManager, 
    select_best_run,
    analyze_parameter_importance,
    TuningCheckpoint,
    log_tuning_summary,
    compare_runs
)
from utils import is_main_process


def setup_distrib(gpu=None):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆæ›¿ä»£ fastai.distributed.setup_distribï¼‰"""
    import os
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        if gpu is not None:
            torch.cuda.set_device(gpu)
        
        if not dist.is_initialized():
            dist.init_process_group(backend='nccl')
        
        return rank, world_size, local_rank
    return 0, 1, 0


def teardown_distrib():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆæ›¿ä»£ fastai.distributed.teardown_distribï¼‰"""
    if dist.is_initialized():
        dist.destroy_process_group()


# å¯¼å…¥ YOLO train å‡½æ•°
from yolo.train import train


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    if 'search_space' not in config:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»åŒ…å« 'search_space' å­—æ®µ")
    
    if 'base_args' not in config:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»åŒ…å« 'base_args' å­—æ®µ")
    
    return config


def merge_params(base_args: Dict[str, Any], trial_params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆå¹¶åŸºç¡€å‚æ•°å’Œtrialå‚æ•°"""
    merged = base_args.copy()
    
    # è¿‡æ»¤æ‰å†…éƒ¨å­—æ®µ
    for key, value in trial_params.items():
        if not key.startswith('_'):
            merged[key] = value
    
    # ç§»é™¤ä¸åº”ä¼ é€’ç»™ train çš„å‚æ•°
    exclude_keys = {'_tuning_metric', 'mlflow_uri', 'distributed'}
    for key in exclude_keys:
        merged.pop(key, None)
    
    return merged


def run_single_trial(trial_idx: int, trial_params: Dict[str, Any], 
                     base_args: Dict[str, Any], exp_manager: TuningExperimentManager,
                     trial_early_stop: bool = False, distributed: bool = False) -> Optional[float]:
    """
    è¿è¡Œå•ä¸ªtrial
    
    Args:
        trial_idx: trialç´¢å¼•
        trial_params: trialå‚æ•°
        base_args: åŸºç¡€å‚æ•°
        exp_manager: å®éªŒç®¡ç†å™¨
        trial_early_stop: æ˜¯å¦å¯ç”¨trialæ—©åœ
        distributed: æ˜¯å¦åˆ†å¸ƒå¼è®­ç»ƒ
    
    Returns:
        éªŒè¯æŒ‡æ ‡å€¼ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    if is_main_process():
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ Trial {trial_idx + 1}")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºå‚æ•°
        print("å‚æ•°:")
        for key, value in trial_params.items():
            if not key.startswith('_'):
                print(f"  {key}: {value}")
    
    # å¯åŠ¨MLflow trial runï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    trial_run = None
    mlflow_uri = base_args.get('mlflow_uri', 'http://192.168.16.130:5000/')
    
    if is_main_process():
        # ä¸´æ—¶è®¾ç½® MLflow URI
        import os
        mlflow_backup = os.environ.get('MLFLOW_TRACKING_URI', '')
        os.environ['MLFLOW_TRACKING_URI'] = mlflow_uri
        mlflow.set_tracking_uri(mlflow_uri)
        
        trial_run = exp_manager.start_trial_run(trial_idx, trial_params)
    
    try:
        # åˆå¹¶å‚æ•°
        train_args = merge_params(base_args, trial_params)
        
        # ä¸ºæ¯ä¸ª trial è®¾ç½®å”¯ä¸€çš„æ¨¡å‹ä¿å­˜è·¯å¾„
        project_name = train_args.get('project_name', 'hyperparameter-tuning')
        task_name = train_args.get('task_name', 'tuning')
        trial_task_name = f"{task_name}_trial_{trial_idx:03d}"
        train_args['task_name'] = trial_task_name  # æ¯ä¸ª trial æœ‰ç‹¬ç«‹çš„ç›®å½•
        
        # è°ƒä¼˜æ¨¡å¼ï¼šç¦ç”¨æ¨¡å‹ä¸Šä¼ åˆ° MLflowï¼ˆé¿å… S3 å‡­è¯é—®é¢˜ï¼‰
        train_args['skip_mlflow_model_upload'] = True
        
        # ç¦ç”¨ auto_resumeï¼ˆæ¯ä¸ª trial éƒ½æ˜¯å…¨æ–°è®­ç»ƒï¼‰
        train_args['overwrite'] = True  # YOLO ä½¿ç”¨ overwrite è€Œä¸æ˜¯ auto_resume
        
        # å°† trial run ID ä¼ é€’ç»™è®­ç»ƒï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process() and trial_run is not None:
            train_args['mlflow_parent_run_id'] = trial_run.info.run_id
        
        # è°ƒç”¨ YOLO è®­ç»ƒï¼ˆMLflow URI å·²ç»åœ¨å¯åŠ¨ trial run æ—¶è®¾ç½®ï¼‰
        train(**train_args)
        
        # è·å–æŒ‡æ ‡å€¼ï¼ˆä»MLflowï¼Œä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            run = mlflow.get_run(trial_run.info.run_id)
            metric_name = base_args.get('_tuning_metric', 'metrics/mAP50-95(B)')
            
            # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰å¯ç”¨çš„æŒ‡æ ‡
            available_metrics = list(run.data.metrics.keys())
            print(f"ğŸ“Š å¯ç”¨æŒ‡æ ‡: {available_metrics}")
            
            metric_value = run.data.metrics.get(metric_name)
            
            if metric_value is None:
                print(f"âš ï¸ æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_name}'ï¼Œtrialå¤±è´¥")
                print(f"   å¯ç”¨æŒ‡æ ‡: {available_metrics}")
                return None
            
            print(f"\nâœ… Trialå®Œæˆ: {metric_name} = {metric_value:.6f}")
            return metric_value
        else:
            # éä¸»è¿›ç¨‹è¿”å›None
            return None
        
    except Exception as e:
        if is_main_process():
            print(f"\nâŒ Trialå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        return None
    finally:
        # ç»“æŸtrial run å¹¶æ¢å¤ç¯å¢ƒï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            exp_manager.end_trial_run()
            # æ¢å¤ MLflow URI
            os.environ['MLFLOW_TRACKING_URI'] = mlflow_backup


def _run_tuning_impl(config: Dict[str, Any], resume_run_id: Optional[str] = None,
                     dry_run: bool = False, mlflow_uri: str = '', distributed: bool = False):
    """è¿è¡Œè°ƒä¼˜ä¸»å¾ªç¯çš„å®é™…å®ç°
    
    Args:
        config: è°ƒä¼˜é…ç½®
        resume_run_id: æ¢å¤çš„run ID
        dry_run: ä»…æ˜¾ç¤ºå‚æ•°ç»„åˆ
        mlflow_uri: MLflow tracking URI
        distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    """
    
    # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼ˆåœ¨ä»»ä½•åˆ†å¸ƒå¼æ“ä½œä¹‹å‰ï¼‰
    if distributed and not dist.is_initialized():
        # è·å–å½“å‰è¿›ç¨‹çš„ GPU
        import os
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        setup_distrib(gpu=local_rank)
        # åˆå§‹åŒ–åå†æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
        is_main = is_main_process()
        if is_main:
            print(f"ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ– (GPUs: {torch.cuda.device_count()})")
    else:
        is_main = True  # éåˆ†å¸ƒå¼æ¨¡å¼ï¼Œå½“å‰è¿›ç¨‹å°±æ˜¯ä¸»è¿›ç¨‹
    
    # æå–é…ç½®
    base_args = config['base_args']
    strategy_name = config.get('strategy', 'grid')
    metric = config.get('metric', 'metrics/mAP50-95(B)')
    mode = config.get('mode', 'maximize')
    
    # å°†metricä¿¡æ¯ä¼ é€’ç»™train
    base_args['_tuning_metric'] = metric
    
    # å°†distributedå‚æ•°ä¼ é€’ç»™train_model
    base_args['distributed'] = distributed
    
    # åˆ›å»ºæœç´¢ç­–ç•¥
    strategy = create_search_strategy(config)
    total_trials = strategy.get_total_trials()
    
    # ä»…ä¸»è¿›ç¨‹æ‰“å°ä¿¡æ¯
    if is_main_process():
        print(f"\nğŸ” æœç´¢ç­–ç•¥: {strategy_name}")
        if distributed:
            print(f"ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒ: {torch.cuda.device_count()} GPUs")
        print(f"ğŸ“Š æ€»è¯•éªŒæ•°: {total_trials}")
    
    # Dry runæ¨¡å¼ï¼šä»…æ˜¾ç¤ºå‚æ•°ç»„åˆ
    if dry_run:
        if is_main_process():
            print(f"\n{'='*80}")
            print(f"ğŸ” Dry Run: å‚æ•°ç»„åˆé¢„è§ˆ")
            print(f"{'='*80}\n")
            
            for i, params in enumerate(strategy.generate_trials()):
                if i >= total_trials:
                    break
                print(f"Trial {i + 1}:")
                for key, value in params.items():
                    if not key.startswith('_'):
                        print(f"  {key}: {value}")
                print()
        
        return
    
    # æ­£å¸¸è°ƒä¼˜æ¨¡å¼
    # æå–é¡¹ç›®å’Œä»»åŠ¡åç§°
    project_name = base_args.get('project_name', 'yolo-tuning')
    task_name = base_args.get('task_name', f'{strategy_name}-tuning')
    
    # åˆå§‹åŒ–å®éªŒç®¡ç†å™¨ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    exp_manager = None
    parent_run_id = None
    
    if is_main_process():
        # è®¾ç½® MLflow tracking URI
        import os
        os.environ['MLFLOW_TRACKING_URI'] = mlflow_uri
        mlflow.set_tracking_uri(mlflow_uri)
        
        exp_manager = TuningExperimentManager(project_name, task_name, config)
        
        # å¯åŠ¨æˆ–æ¢å¤çˆ¶run
        if resume_run_id:
            print(f"ğŸ“¥ æ¢å¤è°ƒä¼˜: {resume_run_id}")
            parent_run = mlflow.start_run(run_id=resume_run_id)
            exp_manager.parent_run = parent_run
        else:
            parent_run = exp_manager.start_parent_run()
        
        parent_run_id = parent_run.info.run_id
        print(f"âœ… Parent Run ID: {parent_run_id}")
    
    # åˆå§‹åŒ–checkpointï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    checkpoint = None
    completed_trials = set()
    
    if is_main_process():
        checkpoint_path = Path('runs') / project_name / task_name / 'tuning_checkpoint.json'
        checkpoint = TuningCheckpoint(checkpoint_path)
        
        # åŠ è½½checkpointï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if checkpoint.exists():
            state = checkpoint.load()
            if state['parent_run_id'] == parent_run_id:
                completed_trials = set(state.get('completed_trials', []))
                print(f"ğŸ“¥ ä»checkpointæ¢å¤: å·²å®Œæˆ {len(completed_trials)} ä¸ªtrials")
    
    # è¿è¡Œtrials
    trial_results = []
    start_time = time.time()
    
    # åˆ†å¸ƒå¼æ¨¡å¼ï¼šä¸»è¿›ç¨‹ç”Ÿæˆå‚æ•°ï¼Œå¹¿æ’­ç»™æ‰€æœ‰è¿›ç¨‹
    if distributed:
        # ä¸»è¿›ç¨‹ç”Ÿæˆæ‰€æœ‰trials
        all_trials = []
        if is_main_process():
            for trial_idx, trial_params in enumerate(strategy.generate_trials()):
                if trial_idx >= total_trials:
                    break
                # è·³è¿‡å·²å®Œæˆçš„
                if trial_idx in completed_trials:
                    print(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„ Trial {trial_idx + 1}")
                    continue
                all_trials.append((trial_idx, trial_params))
        
        # å¹¿æ’­trialæ•°é‡
        trial_count = torch.tensor(len(all_trials) if is_main_process() else 0, dtype=torch.long)
        if torch.cuda.is_available():
            trial_count = trial_count.cuda()
        dist.broadcast(trial_count, src=0)
        
        # éä¸»è¿›ç¨‹å‡†å¤‡æ¥æ”¶
        if not is_main_process():
            all_trials = [None] * trial_count.item()
        
        # å¹¿æ’­æ¯ä¸ªtrialå‚æ•°
        for i in range(trial_count.item()):
            if is_main_process():
                # åºåˆ—åŒ–å‚æ•°
                import pickle
                params_bytes = pickle.dumps(all_trials[i])
                params_tensor = torch.ByteTensor(list(params_bytes))
                params_size = torch.tensor(len(params_tensor), dtype=torch.long)
            else:
                params_size = torch.tensor(0, dtype=torch.long)
            
            if torch.cuda.is_available():
                params_size = params_size.cuda()
            dist.broadcast(params_size, src=0)
            
            if is_main_process():
                params_tensor = params_tensor
            else:
                params_tensor = torch.ByteTensor(params_size.item())
            
            if torch.cuda.is_available():
                params_tensor = params_tensor.cuda()
            dist.broadcast(params_tensor, src=0)
            
            # ååºåˆ—åŒ–
            if not is_main_process():
                import pickle
                params_bytes = bytes(params_tensor.cpu().numpy())
                trial_idx, trial_params = pickle.loads(params_bytes)
                all_trials[i] = (trial_idx, trial_params)
        
        # æ‰§è¡Œæ‰€æœ‰trial
        try:
            for trial_idx, trial_params in all_trials:
                # è¿è¡Œtrial (æ‰€æœ‰è¿›ç¨‹å‚ä¸)
                metric_value = run_single_trial(
                    trial_idx, trial_params, base_args, exp_manager,
                    trial_early_stop=config.get('tuning_options', {}).get('trial_early_stop', False),
                    distributed=distributed
                )
                
                # è®°å½•ç»“æœ (ä»…ä¸»è¿›ç¨‹)
                if is_main_process() and metric_value is not None:
                    # ç¡®ä¿ trial_params å¯ä»¥ JSON åºåˆ—åŒ–
                    serializable_params = {}
                    for k, v in trial_params.items():
                        if isinstance(v, (int, float, str, bool, list, dict, type(None))):
                            serializable_params[k] = v
                        else:
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            serializable_params[k] = str(v)
                    
                    trial_results.append({
                        'trial_idx': trial_idx,
                        'params': serializable_params,
                        'metric_value': metric_value
                    })
                    
                    # Optunaåé¦ˆ
                    if isinstance(strategy, OptunaSearchStrategy):
                        strategy.report_result(trial_params, metric_value)
                    
                    # ä¿å­˜checkpoint
                    completed_trials.add(trial_idx)
                    checkpoint.save({
                        'parent_run_id': parent_run_id,
                        'completed_trials': list(completed_trials),
                        'trial_results': trial_results
                    })
        except KeyboardInterrupt:
            if is_main_process():
                print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è°ƒä¼˜")
            raise
        except Exception as e:
            if is_main_process():
                print(f"\nâŒ è°ƒä¼˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    else:
        # éåˆ†å¸ƒå¼æ¨¡å¼
        try:
            for trial_idx, trial_params in enumerate(strategy.generate_trials()):
                # è·³è¿‡å·²å®Œæˆçš„ (ä»…ä¸»è¿›ç¨‹æ£€æŸ¥)
                if is_main_process() and trial_idx in completed_trials:
                    print(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„ Trial {trial_idx + 1}")
                    continue
                
                # è¿è¡Œtrial (æ‰€æœ‰è¿›ç¨‹å‚ä¸)
                metric_value = run_single_trial(
                    trial_idx, trial_params, base_args, exp_manager,
                    trial_early_stop=config.get('tuning_options', {}).get('trial_early_stop', False),
                    distributed=distributed
                )
                
                # è®°å½•ç»“æœ (ä»…ä¸»è¿›ç¨‹)
                if is_main_process() and metric_value is not None:
                    # ç¡®ä¿ trial_params å¯ä»¥ JSON åºåˆ—åŒ–
                    serializable_params = {}
                    for k, v in trial_params.items():
                        if isinstance(v, (int, float, str, bool, list, dict, type(None))):
                            serializable_params[k] = v
                        else:
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            serializable_params[k] = str(v)
                    
                    trial_results.append({
                        'trial_idx': trial_idx,
                        'params': serializable_params,
                        'metric_value': metric_value
                    })
                    
                    # Optunaåé¦ˆ
                    if isinstance(strategy, OptunaSearchStrategy):
                        strategy.report_result(trial_params, metric_value)
                    
                    # ä¿å­˜checkpoint
                    completed_trials.add(trial_idx)
                    checkpoint.save({
                        'parent_run_id': parent_run_id,
                        'completed_trials': list(completed_trials),
                        'trial_results': trial_results
                    })
        except KeyboardInterrupt:
            if is_main_process():
                print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è°ƒä¼˜")
            raise
        except Exception as e:
            if is_main_process():
                print(f"\nâŒ è°ƒä¼˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    
    # è°ƒä¼˜å®Œæˆ (ä»…ä¸»è¿›ç¨‹å¤„ç†)
    if is_main_process():
        duration = time.time() - start_time
        print(f"\n{'='*80}")
        print(f"âœ… è°ƒä¼˜å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æ€»è€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
        print(f"å®Œæˆè¯•éªŒ: {len(trial_results)}/{total_trials}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_result = select_best_run(parent_run_id, metric, mode)
        
        # å‚æ•°é‡è¦æ€§åˆ†æ
        if len(trial_results) >= 10:
            output_dir = Path('runs') / project_name / task_name
            analyze_parameter_importance(parent_run_id, metric, output_dir)
            compare_runs(parent_run_id, metric, output_dir)
        
        # å…ˆç»“æŸ parent runï¼Œå†è®°å½•æ€»ç»“ï¼ˆé¿å… run å†²çªï¼‰
        if exp_manager is not None:
            exp_manager.end_parent_run()
        
        # è®°å½•æ€»ç»“
        log_tuning_summary(parent_run_id, len(trial_results), best_result, duration)
        
        # æ¸…ç†checkpoint
        checkpoint.clear()
    
    # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if distributed:
        dist.barrier()
        if is_main_process():
            print("ğŸ”§ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ...")
        teardown_distrib()


def run_tuning(config: Dict[str, Any], resume_run_id: Optional[str] = None,
               dry_run: bool = False, distributed: bool = False):
    """è¿è¡Œè°ƒä¼˜çš„åŒ…è£…å‡½æ•°"""
    mlflow_uri = config.get('base_args', {}).get('mlflow_uri', 'http://192.168.16.130:5000/')
    
    try:
        _run_tuning_impl(config, resume_run_id, dry_run, mlflow_uri, distributed)
    except KeyboardInterrupt:
        if is_main_process():
            print("\n\nâš ï¸ è°ƒä¼˜è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        if is_main_process():
            print(f"\n\nâŒ è°ƒä¼˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        raise


def main():
    parser = argparse.ArgumentParser(description='YOLOè¶…å‚æ•°è°ƒä¼˜')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, required=True,
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    
    # æ¢å¤
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤ä¹‹å‰çš„è°ƒä¼˜runï¼ˆæä¾›parent run IDï¼‰')
    
    # Dry run
    parser.add_argument('--dry-run', action='store_true',
                       help='ä»…æ˜¾ç¤ºå‚æ•°ç»„åˆï¼Œä¸å®é™…è®­ç»ƒ')
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument('--distributed', action='store_true',
                       help='å¯ç”¨å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ')
    
    # å‚æ•°è¦†ç›–
    parser.add_argument('--override', type=str, nargs='+',
                       help='è¦†ç›–é…ç½®å‚æ•°ï¼Œæ ¼å¼: key=value')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®ï¼ˆä»…ä¸»è¿›ç¨‹æ‰“å°ï¼‰
    if is_main_process():
        print(f"ğŸ“ åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    # å‚æ•°è¦†ç›–
    if args.override:
        if is_main_process():
            print("\nâš™ï¸ å‚æ•°è¦†ç›–:")
        for override in args.override:
            if '=' not in override:
                if is_main_process():
                    print(f"âš ï¸ å¿½ç•¥æ— æ•ˆè¦†ç›–: {override}")
                continue
            
            key, value = override.split('=', 1)
            
            # å°è¯•è§£æå€¼
            try:
                # å°è¯•evalï¼ˆæ”¯æŒæ•°å­—ã€åˆ—è¡¨ç­‰ï¼‰
                value = eval(value)
            except:
                # ä¿æŒå­—ç¬¦ä¸²
                pass
            
            # è¦†ç›–åˆ°base_args
            config['base_args'][key] = value
            if is_main_process():
                print(f"  {key} = {value}")
    
    # æ˜¾ç¤ºé…ç½®æ‘˜è¦ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if is_main_process():
        print(f"\nğŸ“‹ é…ç½®æ‘˜è¦:")
        print(f"  ç­–ç•¥: {config.get('strategy', 'grid')}")
        print(f"  è¯•éªŒæ•°: {config.get('n_trials', 'all')}")
        print(f"  ä¼˜åŒ–æŒ‡æ ‡: {config.get('metric', 'metrics/mAP50-95(B)')} ({config.get('mode', 'maximize')})")
        print(f"  æœç´¢å‚æ•°: {', '.join(config['search_space'].keys())}")
        if args.distributed:
            import torch
            print(f"  åˆ†å¸ƒå¼è®­ç»ƒ: {torch.cuda.device_count()} GPUs")
    
    # è¿è¡Œè°ƒä¼˜
    run_tuning(config, resume_run_id=args.resume, dry_run=args.dry_run, distributed=args.distributed)


if __name__ == '__main__':
    main()
