"""
MLflowè°ƒä¼˜å®éªŒç®¡ç†

æ”¯æŒåµŒå¥—å®éªŒã€æœ€ä½³æ¨¡å‹é€‰æ‹©ã€å‚æ•°é‡è¦æ€§åˆ†æç­‰
"""

import mlflow
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import json
import pandas as pd
import numpy as np
from datetime import datetime


class TuningExperimentManager:
    """MLflowè°ƒä¼˜å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, project_name: str, task_name: str, config: Dict[str, Any]):
        """
        Args:
            project_name: MLflowå®éªŒåç§°
            task_name: è°ƒä¼˜ä»»åŠ¡åç§°
            config: è°ƒä¼˜é…ç½®
        """
        self.project_name = project_name
        self.task_name = task_name
        self.config = config
        self.parent_run = None
        self.trial_runs = []
        
    def start_parent_run(self) -> mlflow.ActiveRun:
        """å¯åŠ¨çˆ¶çº§MLflow run"""
        import os
        
        mlflow.set_experiment(self.project_name)
        self.parent_run = mlflow.start_run(run_name=self.task_name)
        
        # è®°å½•è°ƒä¼˜é…ç½®
        mlflow.log_params({
            'tuning_strategy': self.config.get('strategy', 'grid'),
            'n_trials': self.config.get('n_trials', 'all'),
            'metric': self.config.get('metric', 'valid_loss'),
            'mode': self.config.get('mode', 'minimize'),
        })
        
        # è®°å½•æœç´¢ç©ºé—´ï¼ˆåªä½¿ç”¨å‚æ•°ï¼Œé¿å… S3 ä¸Šä¼ ï¼‰
        search_space = self.config.get('search_space', {})
        for param_name, param_config in search_space.items():
            if isinstance(param_config, dict):
                param_type = param_config.get('type', 'unknown')
                mlflow.log_param(f'search_space/{param_name}/type', param_type)
                if 'values' in param_config:
                    mlflow.log_param(f'search_space/{param_name}/values', str(param_config['values'])[:250])
                elif 'min' in param_config and 'max' in param_config:
                    mlflow.log_param(f'search_space/{param_name}/min', str(param_config['min']))
                    mlflow.log_param(f'search_space/{param_name}/max', str(param_config['max']))
        
        # è®°å½•å›ºå®šå‚æ•°
        base_args = self.config.get('base_args', {})
        for key, value in base_args.items():
            if not key.startswith('_') and key not in ['mlflow_uri']:  # è·³è¿‡å†…éƒ¨å‚æ•°
                try:
                    mlflow.log_param(f'base/{key}', str(value)[:250])  # MLflow é™åˆ¶å‚æ•°é•¿åº¦
                except Exception:
                    pass  # å¿½ç•¥å‚æ•°è®°å½•é”™è¯¯
        
        print(f"âœ… å¯åŠ¨è°ƒä¼˜å®éªŒ: {self.task_name}")
        print(f"   Parent Run ID: {self.parent_run.info.run_id}")
        
        return self.parent_run
    
    def start_trial_run(self, trial_idx: int, trial_params: Dict[str, Any]) -> mlflow.ActiveRun:
        """å¯åŠ¨å­trial run"""
        # ç”Ÿæˆtrialåç§°
        param_summary = self._format_param_summary(trial_params)
        trial_name = f"trial_{trial_idx:03d}_{param_summary}"
        
        # å¯åŠ¨åµŒå¥—run
        trial_run = mlflow.start_run(
            run_name=trial_name,
            nested=True,
            parent_run_id=self.parent_run.info.run_id
        )
        
        # è®°å½•trialå‚æ•°
        for key, value in trial_params.items():
            if not key.startswith('_'):  # è·³è¿‡å†…éƒ¨å­—æ®µ
                mlflow.log_param(key, value)
        
        mlflow.log_param('trial_idx', trial_idx)
        
        self.trial_runs.append({
            'run_id': trial_run.info.run_id,
            'trial_idx': trial_idx,
            'params': trial_params.copy()
        })
        
        return trial_run
    
    def end_trial_run(self):
        """ç»“æŸå½“å‰trial run"""
        mlflow.end_run()
    
    def end_parent_run(self):
        """ç»“æŸçˆ¶çº§run"""
        if self.parent_run:
            mlflow.end_run()
            print(f"âœ… è°ƒä¼˜å®éªŒå®Œæˆ")
    
    def _format_param_summary(self, params: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å‚æ•°æ‘˜è¦ï¼ˆç”¨äºrunåç§°ï¼‰"""
        summary_parts = []
        
        # è‡ªå®šä¹‰æ˜¾ç¤ºè§„åˆ™ï¼ˆæ›´ç®€æ´çš„æ ¼å¼ï¼‰
        # æ”¯æŒ FastAI å’Œ YOLO å‚æ•°
        param_mappings = {
            # FastAI å‚æ•°
            'lr': ('lr', lambda v: f"{v:.4f}" if isinstance(v, float) else str(v)),
            'batch_size': ('bs', lambda v: str(v)),
            'img_size': ('img', lambda v: str(v)),
            'arch': ('', lambda v: str(v)),  # æ¶æ„ç›´æ¥æ˜¾ç¤º
            'wd': ('wd', lambda v: f"{v:.1e}" if isinstance(v, float) else str(v)),
            'scale': ('sc', lambda v: f"{v:.2f}" if isinstance(v, float) else str(v)),
            
            # YOLO å‚æ•°
            'lr0': ('lr', lambda v: f"{v:.4f}" if isinstance(v, float) else str(v)),
            'batch': ('bs', lambda v: str(v)),
            'imgsz': ('img', lambda v: str(v)),
            'model': ('', lambda v: str(v).replace('.pt', '')),  # æ¨¡å‹åç§°ï¼Œå»æ‰ .pt
            'optimizer': ('opt', lambda v: str(v)),
            'weight_decay': ('wd', lambda v: f"{v:.1e}" if isinstance(v, float) else str(v)),
            'degrees': ('deg', lambda v: f"{v:.0f}" if isinstance(v, float) else str(v)),
            'mosaic': ('mos', lambda v: f"{v:.2f}" if isinstance(v, float) else str(v)),
        }
        
        # æŒ‰é¡ºåºå¤„ç†å‚æ•°
        for key, (prefix, formatter) in param_mappings.items():
            if key in params:
                value = params[key]
                formatted = formatter(value)
                if prefix:
                    summary_parts.append(f"{prefix}{formatted}")
                else:
                    summary_parts.append(formatted)
        
        # æœ€å¤šæ˜¾ç¤º5ä¸ªå‚æ•°é¿å…åç§°è¿‡é•¿
        return '_'.join(summary_parts[:5])


def select_best_run(parent_run_id: str, metric: str = 'valid_loss', 
                    mode: str = 'minimize') -> Optional[Dict[str, Any]]:
    """
    ä»è°ƒä¼˜å®éªŒä¸­é€‰æ‹©æœ€ä½³run
    
    Args:
        parent_run_id: çˆ¶çº§run ID
        metric: ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡
        mode: 'minimize' æˆ– 'maximize'
        
    Returns:
        æœ€ä½³runä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« run_id, params, metrics
    """
    # è·å–æ‰€æœ‰å­runs
    client = mlflow.tracking.MlflowClient()
    
    # æœç´¢åµŒå¥—runs
    runs = client.search_runs(
        experiment_ids=[client.get_run(parent_run_id).info.experiment_id],
        filter_string=f"tags.mlflow.parentRunId = '{parent_run_id}'",
        order_by=[f"metrics.{metric} {'ASC' if mode == 'minimize' else 'DESC'}"]
    )
    
    if not runs:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å­runs")
        return None
    
    best_run = runs[0]
    
    # æå–å‚æ•°å’ŒæŒ‡æ ‡
    best_info = {
        'run_id': best_run.info.run_id,
        'params': best_run.data.params,
        'metrics': best_run.data.metrics,
        'metric_value': best_run.data.metrics.get(metric)
    }
    
    # æ ‡è®°æœ€ä½³run
    client.set_tag(best_run.info.run_id, 'best_run', 'true')
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹:")
    print(f"   Run ID: {best_info['run_id']}")
    if best_info['metric_value'] is not None:
        print(f"   {metric}: {best_info['metric_value']:.6f}")
    else:
        print(f"   {metric}: N/A (æœªè®°å½•)")
    print(f"   å‚æ•°:")
    for key, value in best_info['params'].items():
        if not key.startswith('base_') and not key.startswith('search_space/'):
            print(f"     - {key}: {value}")
    
    return best_info


def analyze_parameter_importance(parent_run_id: str, metric: str = 'valid_loss',
                                  output_dir: Optional[Path] = None) -> pd.DataFrame:
    """
    åˆ†æå‚æ•°é‡è¦æ€§
    
    Args:
        parent_run_id: çˆ¶çº§run ID
        metric: ç›®æ ‡æŒ‡æ ‡
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        å‚æ•°é‡è¦æ€§DataFrame
    """
    client = mlflow.tracking.MlflowClient()
    
    # è·å–æ‰€æœ‰å­runs
    runs = client.search_runs(
        experiment_ids=[client.get_run(parent_run_id).info.experiment_id],
        filter_string=f"tags.mlflow.parentRunId = '{parent_run_id}'"
    )
    
    if len(runs) < 10:
        print(f"âš ï¸ è¯•éªŒæ•°é‡å¤ªå°‘ï¼ˆ{len(runs)}ï¼‰ï¼Œå»ºè®®è‡³å°‘10ä¸ªè¯•éªŒæ‰èƒ½è¿›è¡Œå‚æ•°é‡è¦æ€§åˆ†æ")
        return pd.DataFrame()
    
    # æ”¶é›†æ•°æ®
    data = []
    for run in runs:
        row = {**run.data.params, metric: run.data.metrics.get(metric)}
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # è®¡ç®—ç›¸å…³æ€§ï¼ˆç®€å•æ–¹æ³•ï¼‰
    param_cols = [col for col in df.columns if col != metric]
    importance = {}
    
    for param in param_cols:
        if param.startswith('base_'):
            continue
        
        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
        try:
            param_values = pd.to_numeric(df[param], errors='coerce')
            metric_values = pd.to_numeric(df[metric], errors='coerce')
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            corr = param_values.corr(metric_values)
            importance[param] = abs(corr) if not np.isnan(corr) else 0
        except:
            # åˆ†ç±»å‚æ•°ï¼Œè®¡ç®—æ–¹å·®åˆ†æ
            groups = df.groupby(param)[metric].apply(list)
            if len(groups) > 1:
                # ç®€å•çš„æ–¹å·®æ¯”
                between_var = groups.apply(np.mean).var()
                within_var = groups.apply(np.var).mean()
                importance[param] = between_var / (within_var + 1e-10)
            else:
                importance[param] = 0
    
    # æ’åº
    importance_df = pd.DataFrame([
        {'parameter': k, 'importance': v}
        for k, v in sorted(importance.items(), key=lambda x: x[1], reverse=True)
    ])
    
    print(f"\nğŸ“Š å‚æ•°é‡è¦æ€§åˆ†æ (åŸºäº{len(runs)}ä¸ªè¯•éªŒ):")
    for _, row in importance_df.iterrows():
        print(f"   {row['parameter']}: {row['importance']:.4f}")
    
    # ä¿å­˜ç»“æœ
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        importance_df.to_csv(output_dir / 'parameter_importance.csv', index=False)
        print(f"   ä¿å­˜åˆ°: {output_dir / 'parameter_importance.csv'}")
    
    return importance_df


class TuningCheckpoint:
    """è°ƒä¼˜è¿›åº¦è·Ÿè¸ªå’Œæ¢å¤"""
    
    def __init__(self, checkpoint_path: Path):
        self.checkpoint_path = Path(checkpoint_path)
        self.checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
    
    def save(self, state: Dict[str, Any]):
        """ä¿å­˜checkpoint"""
        state['timestamp'] = datetime.now().isoformat()
        with open(self.checkpoint_path, 'w') as f:
            json.dump(state, f, indent=2)
    
    def load(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½checkpoint"""
        if not self.checkpoint_path.exists():
            return None
        
        with open(self.checkpoint_path, 'r') as f:
            return json.load(f)
    
    def exists(self) -> bool:
        """æ£€æŸ¥checkpointæ˜¯å¦å­˜åœ¨"""
        return self.checkpoint_path.exists()
    
    def clear(self):
        """æ¸…é™¤checkpoint"""
        if self.checkpoint_path.exists():
            self.checkpoint_path.unlink()


def log_tuning_summary(parent_run_id: str, total_trials: int, 
                       best_result: Dict[str, Any], duration: float):
    """
    è®°å½•è°ƒä¼˜æ€»ç»“åˆ°çˆ¶run
    
    Args:
        parent_run_id: çˆ¶çº§run ID
        total_trials: æ€»è¯•éªŒæ•°
        best_result: æœ€ä½³ç»“æœ
        duration: æ€»è€—æ—¶ï¼ˆç§’ï¼‰
    """
    try:
        with mlflow.start_run(run_id=parent_run_id):
            mlflow.log_metric('total_trials', total_trials)
            mlflow.log_metric('total_duration_sec', duration)
            mlflow.log_metric('avg_duration_per_trial', duration / total_trials if total_trials > 0 else 0)
            
            if best_result:
                mlflow.log_metric(f"best_{best_result.get('metric', 'metric')}", 
                                best_result.get('metric_value', 0))
                
                # è®°å½•æœ€ä½³å‚æ•°ï¼ˆä½¿ç”¨ log_param è€Œä¸æ˜¯ log_textï¼Œé¿å… S3 ä¸Šä¼ ï¼‰
                best_params = best_result.get('params', {})
                for key, value in best_params.items():
                    try:
                        # MLflow å‚æ•°é•¿åº¦é™åˆ¶ä¸º 500 å­—ç¬¦
                        param_value = str(value)[:500]
                        mlflow.log_param(f'best_{key}', param_value)
                    except Exception:
                        pass  # å¿½ç•¥è®°å½•å¤±è´¥
                
                # è®°å½•æœ€ä½³ run ID
                if 'run_id' in best_result:
                    mlflow.log_param('best_run_id', best_result['run_id'])
        
        print(f"âœ… è°ƒä¼˜æ€»ç»“å·²è®°å½•åˆ° MLflow")
    except Exception as e:
        print(f"âš ï¸  è®°å½•è°ƒä¼˜æ€»ç»“å¤±è´¥: {e}")


def compare_runs(parent_run_id: str, metric: str = 'valid_loss',
                output_dir: Optional[Path] = None) -> pd.DataFrame:
    """
    å¯¹æ¯”æ‰€æœ‰runsçš„æ€§èƒ½
    
    Args:
        parent_run_id: çˆ¶çº§run ID  
        metric: å¯¹æ¯”æŒ‡æ ‡
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        å¯¹æ¯”ç»“æœDataFrame
    """
    client = mlflow.tracking.MlflowClient()
    
    runs = client.search_runs(
        experiment_ids=[client.get_run(parent_run_id).info.experiment_id],
        filter_string=f"tags.mlflow.parentRunId = '{parent_run_id}'"
    )
    
    data = []
    for run in runs:
        row = {
            'run_id': run.info.run_id,
            'trial_idx': run.data.params.get('trial_idx', ''),
            metric: run.data.metrics.get(metric),
            **{k: v for k, v in run.data.params.items() if not k.startswith('base_')}
        }
        data.append(row)
    
    df = pd.DataFrame(data).sort_values(metric)
    
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_dir / 'runs_comparison.csv', index=False)
        print(f"ğŸ“Š ä¿å­˜å¯¹æ¯”ç»“æœåˆ°: {output_dir / 'runs_comparison.csv'}")
    
    return df
