"""
å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸“ç”¨æ•°æ®åŠ è½½æ¨¡å—
"""

import numpy as np
import torch
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset as TorchDataset
from fastai.vision.all import TensorImage, TensorMask, TfmdDL, DataLoaders

from .data_loading import is_main_process


class SegmentationDataset(TorchDataset):
    """UNet åˆ†å‰²æ•°æ®é›†ï¼ˆå®æ—¶åŠ è½½ + å¯é€‰ç£ç›˜ç¼“å­˜ï¼‰"""
    def __init__(self, img_dir, mask_dir, img_files, img_size=2048, scale=1.0, use_disk_cache=True):
        self.img_dir = Path(img_dir)
        self.mask_dir = Path(mask_dir)
        self.img_files = img_files
        self.img_size = img_size
        self.scale = scale
        self.use_disk_cache = use_disk_cache
        
        # ç£ç›˜ç¼“å­˜ç›®å½•ï¼ˆNPZ å‹ç¼©æ ¼å¼ï¼‰
        if use_disk_cache:
            cache_root = img_dir.parent.parent / '.cache'
            self.disk_cache_dir = cache_root / f'npz_{img_dir.parent.name}_{img_dir.name}_size{img_size}_scale{scale}'
            self.disk_cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.disk_cache_dir = None
        
    def __len__(self):
        return len(self.img_files)
    
    def _load_and_preprocess(self, img_name):
        """åŠ è½½å¹¶é¢„å¤„ç†å•ä¸ªæ ·æœ¬"""
        # åŠ è½½å›¾åƒ
        img_path = self.img_dir / img_name
        img = Image.open(img_path).convert('RGB')
        
        # åŠ è½½mask
        base_name = img_name.rsplit('.', 1)[0]
        mask_name = f"{base_name}_mask.png"
        mask_path = self.mask_dir / mask_name
        
        if not mask_path.exists():
            mask_name = f"{base_name}.png"
            mask_path = self.mask_dir / mask_name
        
        mask = Image.open(mask_path).convert('L')
        
        # é¢„å¤„ç†ï¼šç¼©æ”¾
        if self.scale != 1.0:
            w, h = img.size
            new_w, new_h = int(w * self.scale), int(h * self.scale)
            img = img.resize((new_w, new_h), Image.BICUBIC)
            mask = mask.resize((new_w, new_h), Image.NEAREST)
        
        # Resizeåˆ°ç›®æ ‡å¤§å°
        img = img.resize((self.img_size, self.img_size), Image.BICUBIC)
        mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)
        
        # è½¬æ¢ä¸ºnumpy
        img_np = np.array(img, dtype=np.uint8)
        mask_np = np.array(mask, dtype=np.uint8)
        mask_np = (mask_np > 127).astype(np.uint8)
        
        return img_np, mask_np
    
    def __getitem__(self, idx):
        img_name = self.img_files[idx]
        
        # å°è¯•ä»ç£ç›˜ç¼“å­˜åŠ è½½
        if self.use_disk_cache and self.disk_cache_dir:
            cache_file = self.disk_cache_dir / f'{idx:05d}.npz'
            
            if cache_file.exists():
                try:
                    data = np.load(cache_file)
                    img_np = data['img']
                    mask_np = data['mask']
                except Exception:
                    # ç¼“å­˜æŸåï¼Œé‡æ–°åŠ è½½
                    img_np, mask_np = self._load_and_preprocess(img_name)
                    np.savez_compressed(cache_file, img=img_np, mask=mask_np)
            else:
                # ç¼“å­˜ä¸å­˜åœ¨ï¼Œå®æ—¶åŠ è½½å¹¶ä¿å­˜
                img_np, mask_np = self._load_and_preprocess(img_name)
                try:
                    np.savez_compressed(cache_file, img=img_np, mask=mask_np)
                except Exception:
                    pass  # ä¿å­˜å¤±è´¥ä¸å½±å“è®­ç»ƒ
        else:
            # ä¸ä½¿ç”¨ç¼“å­˜ï¼Œç›´æ¥å®æ—¶åŠ è½½
            img_np, mask_np = self._load_and_preprocess(img_name)
        
        # è½¬æ¢ä¸ºtensor
        img = TensorImage(torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0)
        mask = TensorMask(torch.from_numpy(mask_np.astype(np.int64)))
        
        return img, mask


def get_segmentation_dls(data_dir, batch_size=4, img_size=2048, scale=0.5, num_workers=8, use_disk_cache=True):
    """
    åˆ›å»ºåˆ†å‰²ä»»åŠ¡çš„ DataLoadersï¼ˆç£ç›˜ç¼“å­˜ï¼‰
    
    Args:
        data_dir: æ•°æ®æ ¹ç›®å½•ï¼Œåº”åŒ…å« imgs/ å’Œ masks/ å­ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        img_size: å›¾åƒå°ºå¯¸
        scale: å›¾åƒç¼©æ”¾æ¯”ä¾‹ï¼ˆ< 1.0 èŠ‚çœæ˜¾å­˜ï¼‰
        num_workers: DataLoader å·¥ä½œè¿›ç¨‹æ•°
        use_disk_cache: æ˜¯å¦ä½¿ç”¨ç£ç›˜ç¼“å­˜
        
    Returns:
        DataLoaders: FastAI DataLoaders å¯¹è±¡
    """
    data_dir = Path(data_dir)
    
    # è·å–è®­ç»ƒå’ŒéªŒè¯å›¾åƒåˆ—è¡¨
    train_img_dir = data_dir / 'imgs' / 'train'
    val_img_dir = data_dir / 'imgs' / 'val'
    train_mask_dir = data_dir / 'masks' / 'train'
    val_mask_dir = data_dir / 'masks' / 'val'
    
    train_imgs = sorted([f.name for f in train_img_dir.iterdir() if f.is_file() and not f.name.startswith('.')])
    val_imgs = sorted([f.name for f in val_img_dir.iterdir() if f.is_file() and not f.name.startswith('.')])
    
    if is_main_process():
        print(f"  è®­ç»ƒé›†å›¾åƒæ•°é‡: {len(train_imgs)}")
        print(f"  éªŒè¯é›†å›¾åƒæ•°é‡: {len(val_imgs)}")
        if use_disk_cache:
            print(f"  ğŸ’¾ ç£ç›˜ç¼“å­˜: å¯ç”¨ï¼ˆNPZå‹ç¼©æ ¼å¼ï¼ŒæŒ‰éœ€åˆ›å»ºï¼‰")
        else:
            print(f"  âš¡ å®æ—¶åŠ è½½: å¯ç”¨ï¼ˆæ— ç¼“å­˜ï¼‰")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¯ç”¨ç£ç›˜ç¼“å­˜ï¼‰
    train_ds = SegmentationDataset(train_img_dir, train_mask_dir, train_imgs, img_size, scale, use_disk_cache)
    val_ds = SegmentationDataset(val_img_dir, val_mask_dir, val_imgs, img_size, scale, use_disk_cache)
    
    # åˆ›å»º DataLoaders
    train_dl = TfmdDL(train_ds, batch_size=batch_size, shuffle=True, 
                      num_workers=num_workers, pin_memory=True, drop_last=True)
    val_dl = TfmdDL(val_ds, batch_size=batch_size, shuffle=False, 
                    num_workers=num_workers, pin_memory=True, drop_last=True)
    
    dls = DataLoaders(train_dl, val_dl)
    return dls
