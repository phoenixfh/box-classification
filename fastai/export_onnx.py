"""ç‹¬ç«‹çš„ONNXå¯¼å‡ºæ¨¡å— - ä»predict.pyä¸­æå–"""

import os
import sys
import torch
import time
from pathlib import Path
from fastai.vision.all import *

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
try:
    from models import get_model, is_custom_model
    CUSTOM_MODELS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    CUSTOM_MODELS_AVAILABLE = False
    is_custom_model = lambda x: False


def detect_dataset_structure(data_dir):
    """æ£€æµ‹æ•°æ®é›†çš„ç»„ç»‡ç»“æ„ï¼Œæ”¯æŒä¸åŒçš„ç›®å½•ç»“æ„æ¨¡å¼
    
    æ”¯æŒçš„ç»“æ„:
    1. data_dir/train/class1, data_dir/train/class2, ...
    2. data_dir/val/class1, data_dir/val/class2, ...
    3. data_dir/class1/train, data_dir/class2/train, ...
    4. data_dir/class1/val, data_dir/class2/val, ...
    
    Returns:
        tuple: (structure_type, classes)
        structure_type: 1 = æ•°æ®é›†æŒ‰å­é›†åˆ†ç»„, 2 = æ•°æ®é›†æŒ‰ç±»åˆ«åˆ†ç»„
        classes: ç±»åˆ«åˆ—è¡¨
    """
    data_dir = Path(data_dir)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨train/valå­ç›®å½•ï¼Œç»“æ„ç±»å‹1
    train_dir = data_dir / "train"
    val_dir = data_dir / "val"
    
    if val_dir.exists() and val_dir.is_dir():
        # æ£€æŸ¥valç›®å½•ä¸‹æ˜¯å¦æœ‰ç±»åˆ«å­ç›®å½•
        class_dirs = [d for d in val_dir.iterdir() if d.is_dir()]
        if class_dirs:
            classes = [d.name for d in class_dirs]
            print(f"æ£€æµ‹åˆ°æ•°æ®é›†ç»“æ„: data_dir/val/class, ç±»åˆ«: {classes}")
            return 1, classes
    
    if train_dir.exists() and train_dir.is_dir():
        # æ£€æŸ¥trainç›®å½•ä¸‹æ˜¯å¦æœ‰ç±»åˆ«å­ç›®å½•
        class_dirs = [d for d in train_dir.iterdir() if d.is_dir()]
        if class_dirs:
            classes = [d.name for d in class_dirs]
            print(f"æ£€æµ‹åˆ°æ•°æ®é›†ç»“æ„: data_dir/train/class, ç±»åˆ«: {classes}")
            return 1, classes
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç±»åˆ«ç›®å½•ï¼Œæ¯ä¸ªç±»åˆ«ç›®å½•ä¸‹æœ‰train/valï¼Œç»“æ„ç±»å‹2
    subdirs = [d for d in data_dir.iterdir() if d.is_dir()]
    for subdir in subdirs:
        train_subdir = subdir / "train"
        val_subdir = subdir / "val"
        if (train_subdir.exists() and train_subdir.is_dir()) or \
           (val_subdir.exists() and val_subdir.is_dir()):
            classes = [d.name for d in subdirs]
            print(f"æ£€æµ‹åˆ°æ•°æ®é›†ç»“æ„: data_dir/class/train|val, ç±»åˆ«: {classes}")
            return 2, classes
    
    # å°è¯•ç¬¬ä¸‰ç§ç»“æ„ï¼Œæ£€æŸ¥ä¸€çº§å­ç›®å½•ä¸‹çš„äºŒçº§å­ç›®å½•
    all_subdirs = []
    for subdir in subdirs:
        sub_subdirs = [d for d in subdir.iterdir() if d.is_dir()]
        all_subdirs.extend(sub_subdirs)
    
    # ä»æ‰€æœ‰äºŒçº§å­ç›®å½•æ”¶é›†æ½œåœ¨çš„ç±»åˆ«
    if all_subdirs:
        potential_classes = set()
        for d in all_subdirs:
            potential_class = d.parent.name
            if potential_class not in ['train', 'val', 'test']:
                potential_classes.add(potential_class)
        
        if potential_classes:
            classes = list(potential_classes)
            print(f"æ£€æµ‹åˆ°å¯èƒ½çš„ç±»åˆ«: {classes}")
            return 3, classes
    
    # æ‰¾ä¸åˆ°æ¸…æ™°çš„ç»“æ„
    print("æ— æ³•è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç»“æ„")
    return 0, []


def load_model(model_path, arch, device=None, data_path=None, img_size=320):
    """åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒåˆ†ç±»å’Œåˆ†å‰²ï¼‰
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        arch: æ¨¡å‹æ¶æ„ (å¦‚ 'resnet18', 'unet_seg' ç­‰)
        device: è®¾å¤‡ç±»å‹('cuda'æˆ–'cpu')
        data_path: æ•°æ®é›†è·¯å¾„ï¼Œç”¨äºä»æ•°æ®ç›®å½•è·å–ç±»åˆ«ä¿¡æ¯
        img_size: å›¾åƒå°ºå¯¸ï¼ˆåˆ†å‰²æ¨¡å‹éœ€è¦ï¼‰
    """
    print("å°è¯•åŠ è½½æ¨¡å‹...")
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†å‰²æ¨¡å‹
    is_segmentation = arch.lower().endswith('_seg')
    
    if is_segmentation:
        print(f"ğŸ¯ æ£€æµ‹åˆ°åˆ†å‰²æ¨¡å‹: {arch}")
        return load_segmentation_model(model_path, arch, device, img_size)
    else:
        print(f"ğŸ¯ æ£€æµ‹åˆ°åˆ†ç±»æ¨¡å‹: {arch}")
        return load_classification_model(model_path, arch, device, data_path)


def load_segmentation_model(model_path, arch, device=None, img_size=2048):
    """åŠ è½½åˆ†å‰²æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        arch: æ¨¡å‹æ¶æ„åç§°
        device: è®¾å¤‡ç±»å‹
        img_size: å›¾åƒå°ºå¯¸
    """
    from PIL import Image
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®ç”¨äºåˆå§‹åŒ–
    print("åˆ›å»ºä¸´æ—¶åˆ†å‰²æ¨¡å‹...")
    temp_path = Path('.') / "temp_images"
    temp_path.mkdir(exist_ok=True)
    
    # åˆ›å»ºä¸´æ—¶å›¾åƒ
    if not list(temp_path.glob("*.jpg")):
        img = Image.new('RGB', (img_size, img_size), color='white')
        img.save(temp_path / "dummy.jpg")
        mask = Image.new('L', (img_size, img_size), color=0)
        mask.save(temp_path / "dummy_mask.png")
    
    # ä½¿ç”¨ get_model åˆ›å»ºåˆ†å‰²æ¨¡å‹
    if CUSTOM_MODELS_AVAILABLE:
        model = get_model(arch, n_classes=1, n_channels=3)
    else:
        raise ValueError(f"åˆ†å‰²æ¨¡å‹ '{arch}' éœ€è¦è‡ªå®šä¹‰æ¨¡å‹æ¨¡å—æ”¯æŒ")
    
    # åˆ›å»ºç®€å•çš„æ•°æ®åŠ è½½å™¨ç”¨äºåŒ…è£…
    from torch.utils.data import Dataset, DataLoader
    
    class DummySegDataset(Dataset):
        def __len__(self):
            return 1
        def __getitem__(self, idx):
            img = torch.rand(3, img_size, img_size)
            mask = torch.zeros(img_size, img_size)
            return TensorImage(img), TensorMask(mask)
    
    dummy_ds = DummySegDataset()
    dummy_dl = DataLoader(dummy_ds, batch_size=1)
    
    from fastai.vision.all import DataLoaders
    dls = DataLoaders(dummy_dl, dummy_dl)
    
    # åˆ›å»º Learnerï¼Œéœ€è¦æŒ‡å®šæŸå¤±å‡½æ•°
    from fastai.learner import Learner
    from torch.nn import BCEWithLogitsLoss
    
    loss_func = BCEWithLogitsLoss()
    learn = Learner(dls, model, loss_func=loss_func)
    
    # åŠ è½½æƒé‡
    print(f"ä» {model_path} åŠ è½½æƒé‡...")
    state_dict = torch.load(model_path, map_location='cpu' if device != 'cuda' else 'cuda')
    
    if isinstance(state_dict, dict) and 'model' in state_dict:
        state_dict = state_dict['model']
    
    learn.model.load_state_dict(state_dict)
    print("åˆ†å‰²æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    learn.model.eval()
    
    return learn


def load_classification_model(model_path, arch, device=None, data_path=None):
    """åŠ è½½åˆ†ç±»æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        arch: æ¨¡å‹æ¶æ„
        device: è®¾å¤‡ç±»å‹('cuda'æˆ–'cpu')
        data_path: æ•°æ®é›†è·¯å¾„ï¼Œç”¨äºä»æ•°æ®ç›®å½•è·å–ç±»åˆ«ä¿¡æ¯
    """
    print("å°è¯•åŠ è½½æ¨¡å‹...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„å®šä¹‰ç±»åˆ«
    categories = None
    predefined_classes = os.environ.get('MODEL_CLASSES', None)
    if predefined_classes:
        categories = predefined_classes.split(',')
        print(f"ä½¿ç”¨é¢„å®šä¹‰ç±»åˆ«: {categories}")
    else:
        # å¦‚æœæä¾›äº†æ•°æ®è·¯å¾„ï¼Œä»æ•°æ®è·¯å¾„è·å–ç±»åˆ«
        if data_path:
            try:
                # ä»æŒ‡å®šçš„æ•°æ®è·¯å¾„è·å–ç±»åˆ«ä¿¡æ¯
                print(f"ä»æŒ‡å®šçš„æ•°æ®è·¯å¾„è·å–ç±»åˆ«: {data_path}")
                _, categories = detect_dataset_structure(data_path)
                if categories:
                    print(f"ä»æ•°æ®è·¯å¾„è·å–åˆ°ç±»åˆ«: {categories}")
                else:
                    print("ä»æ•°æ®è·¯å¾„æœªæ£€æµ‹åˆ°ç±»åˆ«ï¼Œå°†å°è¯•å…¶ä»–æ–¹æ³•...")
            except Exception as dpe:
                print(f"ä»æŒ‡å®šæ•°æ®è·¯å¾„è·å–ç±»åˆ«å¤±è´¥: {dpe}")
                categories = None
    
    if categories is None:
        print("é”™è¯¯ï¼šæœªæŒ‡å®šç±»åˆ«ã€‚è¯·ä½¿ç”¨ --classes æŒ‡å®šç±»åˆ«ï¼Œæˆ–ä½¿ç”¨ --data_path æŒ‡å®šæ•°æ®é›†è·¯å¾„ã€‚")
        exit(1)

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ•°æ®å—å¹¶æ„å»ºå­¦ä¹ å™¨
    print("åˆ›å»ºä¸´æ—¶æ¨¡å‹...")
    dblock = DataBlock(
        blocks=(ImageBlock, CategoryBlock(categories)),
        get_items=get_image_files,
        get_y=lambda x: categories[0]  # é»˜è®¤ç±»åˆ«ï¼Œç¨åä¼šè¢«é¢„æµ‹è¦†ç›–
    )
    
    # å°è¯•åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ•°æ®åŠ è½½å™¨
    path = Path('.')
    temp_path = path / "temp_images"
    temp_path.mkdir(exist_ok=True)
    
    # ç¡®ä¿æœ‰è‡³å°‘ä¸€ä¸ªå›¾åƒè¿›è¡Œåˆå§‹åŒ–
    if not list(temp_path.glob("*.jpg")):
        # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒ
        from PIL import Image
        img = Image.new('RGB', (320, 320), color='white')
        img.save(temp_path / "dummy.jpg")

    dls = dblock.dataloaders(temp_path, bs=1)

    print(dls.vocab)
    
    # åŠ è½½æƒé‡
    print(f"ä» {model_path} åŠ è½½æƒé‡...")
    state_dict = torch.load(model_path, map_location='cpu' if device != 'cuda' else 'cuda')
    
    print(f"æƒé‡é”®å: {state_dict.keys()}")

    # å¤„ç†å¯èƒ½çš„åŒ…è£…æ ¼å¼
    if isinstance(state_dict, dict) and 'model' in state_dict:
        state_dict = state_dict['model']
    
    # å¤„ç† DistributedDataParallel (DDP) çš„ 'module.' å‰ç¼€
    if any(key.startswith('module.') for key in state_dict.keys()):
        print("æ£€æµ‹åˆ° DDP æ¨¡å‹ï¼Œç§»é™¤ 'module.' å‰ç¼€...")
        new_state_dict = {}
        for key, value in state_dict.items():
            # ç§»é™¤ 'module.' å‰ç¼€
            new_key = key.replace('module.', '') if key.startswith('module.') else key
            new_state_dict[new_key] = value
        state_dict = new_state_dict
        print(f"âœ“ å·²ç§»é™¤ 'module.' å‰ç¼€ï¼Œæ–°çš„é”®åç¤ºä¾‹: {list(state_dict.keys())[:3]}")
    
    # ç»Ÿä¸€ä½¿ç”¨vision_learneråˆ›å»ºæ¨¡å‹
    print(f"âœ“ ä½¿ç”¨vision_learneråˆ›å»ºæ¨¡å‹: {arch}")
    try:
        learn = vision_learner(dls, arch=arch, pretrained=False, n_out=len(categories))
        learn.model.load_state_dict(state_dict)
        print("âœ“ æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âš ï¸  ä½¿ç”¨vision_learneråŠ è½½å¤±è´¥: {e}")
        print(f"   å°è¯•æ£€æµ‹æ¨¡å‹ç±»å‹...")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§çš„ç›´æ¥Learnerä¿å­˜çš„æƒé‡ï¼ˆå‘åå…¼å®¹ï¼‰
        # Timmæ¨¡å‹çš„é”®åé€šå¸¸æ˜¯: stem.0.weight, stages.0.blocks.0.xxxï¼ˆæ— '0.'å‰ç¼€ï¼‰
        # vision_learnerçš„é”®åæ˜¯: 0.stem.0.weightï¼ˆæœ‰'0.'å‰ç¼€ï¼‰
        is_old_timm_model = any('stem' in key or 'stages' in key or 'blocks' in key for key in state_dict.keys()) and \
                           not any(key.startswith('0.') for key in state_dict.keys())
        
        if is_old_timm_model:
            print(f"âœ“ æ£€æµ‹åˆ°æ—§æ ¼å¼Timmæ¨¡å‹ï¼ˆç›´æ¥Learnerä¿å­˜ï¼‰ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")
            try:
                import timm
                # ç›´æ¥ä½¿ç”¨timmåˆ›å»ºæ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
                model = timm.create_model(arch, pretrained=False, num_classes=len(categories))
                model.load_state_dict(state_dict)
                print("âœ“ æ—§æ ¼å¼Timmæ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„LearneråŒ…è£…
                from fastai.learner import Learner
                learn = Learner(dls, model)
            except ImportError:
                print("âŒ æœªå®‰è£…timmåº“")
                raise
        else:
            # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
            raise e
    
    return learn


def detect_input_size(model, common_sizes=[192, 224, 256, 320, 384, 448, 512, 640]):
    """é€šè¿‡æµ‹è¯•å¸¸è§å°ºå¯¸æ¥æ¨æ–­æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„è¾“å…¥å°ºå¯¸
    
    Args:
        model: PyTorchæ¨¡å‹
        common_sizes: å¸¸ç”¨çš„å›¾åƒå°ºå¯¸åˆ—è¡¨
    
    Returns:
        int: æ¨æ–­å‡ºçš„å›¾åƒå°ºå¯¸ï¼Œå¦‚æœæ— æ³•æ¨æ–­åˆ™è¿”å›224ä½œä¸ºé»˜è®¤å€¼
    """
    print("\nå°è¯•è‡ªåŠ¨æ¨æ–­æ¨¡å‹è¾“å…¥å°ºå¯¸...")
    model.eval()
    model.cpu()
    
    # CNNæ¨¡å‹é€šå¸¸å¯ä»¥æ¥å—ä»»æ„å°ºå¯¸ï¼Œæˆ‘ä»¬æµ‹è¯•å¸¸è§å°ºå¯¸
    # ä¼˜å…ˆå°è¯•æœ€å¸¸ç”¨çš„å°ºå¯¸
    for size in common_sizes:
        try:
            with torch.no_grad():
                dummy_input = torch.randn(1, 3, size, size)
                output = model(dummy_input)
                print(f"âœ“ æ¨æ–­æˆåŠŸï¼æ¨¡å‹è¾“å…¥å°ºå¯¸: {size}x{size}")
                return size
        except Exception as e:
            # è¿™ä¸ªå°ºå¯¸å¯èƒ½ä¸é€‚åˆï¼Œç»§ç»­å°è¯•
            continue
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›æœ€å¸¸ç”¨çš„224
    print(f"âš ï¸  æ— æ³•è‡ªåŠ¨æ¨æ–­ï¼Œä½¿ç”¨é»˜è®¤å€¼: 224x224")
    return 224


def export_to_onnx(model_path, arch=None, output_path=None, img_size=None, device=None, data_path=None, classes=None):
    """å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼å¹¶åµŒå…¥ç±»åˆ«ä¿¡æ¯
    
    Args:
        model_path: PyTorchæ¨¡å‹è·¯å¾„ (.pthæ–‡ä»¶)
        arch: æ¨¡å‹æ¶æ„ï¼Œå¦‚æœä¸ºNoneåˆ™ä»checkpointè‡ªåŠ¨è¯»å–
        output_path: ONNXæ¨¡å‹è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨è¾“å…¥è·¯å¾„æ›¿æ¢åç¼€
        img_size: è¾“å…¥å›¾åƒå¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä»checkpointè‡ªåŠ¨è¯»å–
        device: è®¾å¤‡ç±»å‹ ('cuda' æˆ– 'cpu')
        data_path: æ•°æ®é›†è·¯å¾„ï¼Œç”¨äºè·å–ç±»åˆ«ä¿¡æ¯
        classes: ç±»åˆ«åˆ—è¡¨å­—ç¬¦ä¸²ï¼Œç”¨é€—å·åˆ†éš”
    
    Returns:
        str: å¯¼å‡ºçš„ONNXæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    print(f"\n===== å¯¼å‡ºONNXæ¨¡å‹ =====")
    print(f"æºæ¨¡å‹: {model_path}")
    
    # å…ˆå°è¯•ä»checkpointè¯»å–archå’Œimg_size
    try:
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # è¯»å–arch
        if arch is None and isinstance(checkpoint, dict) and 'arch' in checkpoint:
            arch = checkpoint['arch']
            print(f"âœ“ ä»checkpointè¯»å–åˆ° arch: {arch}")
        
        # è¯»å–img_size
        if img_size is None and isinstance(checkpoint, dict) and 'img_size' in checkpoint:
            img_size = checkpoint['img_size']
            print(f"âœ“ ä»checkpointè¯»å–åˆ° img_size: {img_size}")
    except Exception as e:
        print(f"âš ï¸  è¯»å–checkpointä¿¡æ¯å¤±è´¥: {e}")
    
    # å¦‚æœä»ç„¶æ²¡æœ‰archï¼Œä½¿ç”¨é»˜è®¤å€¼
    if arch is None:
        arch = 'resnet18'
        print(f"âš ï¸  æœªæ‰¾åˆ°archä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼: {arch}")
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if output_path is None:
        output_path = Path(model_path).with_suffix('.onnx')
    
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†å‰²æ¨¡å‹
    is_segmentation = arch.lower().endswith('_seg')
    
    # åˆ†å‰²æ¨¡å‹çš„é»˜è®¤å›¾åƒå°ºå¯¸
    if is_segmentation and img_size is None:
        img_size = 2048
        print(f"åˆ†å‰²ä»»åŠ¡ï¼Œä½¿ç”¨é»˜è®¤å›¾åƒå°ºå¯¸: {img_size}")
    
    # åŠ è½½æ¨¡å‹
    start_time = time.time()
    
    try:
        # åŠ è½½æ¨¡å‹
        learn = load_model(model_path, arch, device, data_path, img_size)
        load_time = time.time() - start_time
        print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {load_time*1000:.2f}ms")
        
        # è‡ªåŠ¨æ¨æ–­æˆ–ä½¿ç”¨æŒ‡å®šçš„å›¾åƒå°ºå¯¸
        if img_size is None:
            img_size = detect_input_size(learn.model)
        else:
            print(f"ä½¿ç”¨è¯»å–/æŒ‡å®šçš„å›¾åƒå°ºå¯¸: {img_size}x{img_size}")
        
        # å‡†å¤‡å¯¼å‡º
        export_start = time.time()
        dummy_input = torch.randn(1, 3, img_size, img_size).cpu()
        
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶åœ¨CPUä¸Š
        learn.model.eval().cpu()
        
        # è·å–ç±»åˆ«ä¿¡æ¯ï¼ˆä»…ç”¨äºåˆ†ç±»æ¨¡å‹ï¼‰
        categories = None
        if not is_segmentation:
            if classes:
                categories = [c.strip() for c in classes.split(',')]
            elif hasattr(learn, 'dls') and hasattr(learn.dls, 'vocab'):
                categories = learn.dls.vocab
            else:
                # ä»æ•°æ®è·¯å¾„è·å–ç±»åˆ«
                _, categories = detect_dataset_structure(data_path)
        
        # å¯¼å‡ºæ¨¡å‹
        torch.onnx.export(
            learn.model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=13,
            do_constant_folding=True,
            input_names=["input"],
            output_names=["output"],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        # åµŒå…¥å…ƒæ•°æ®åˆ°ONNXæ¨¡å‹
        try:
            import onnx
            model_onnx = onnx.load(output_path)
            
            # 1. æ·»åŠ ç±»åˆ«ä¿¡æ¯ï¼ˆä»…ç”¨äºåˆ†ç±»æ¨¡å‹ï¼‰
            if not is_segmentation and categories:
                meta = model_onnx.metadata_props.add()
                meta.key = "classes"
                meta.value = ",".join([str(c) for c in categories])
                
                meta = model_onnx.metadata_props.add()
                meta.key = "class_indices"
                meta.value = ",".join([f"{i}:{c}" for i, c in enumerate(categories)])
                
                print(f"âœ“ å·²åµŒå…¥ç±»åˆ«ä¿¡æ¯: {len(categories)} ä¸ªç±»åˆ«")
            
            # 2. æ·»åŠ ä»»åŠ¡ç±»å‹ï¼ˆåˆ†å‰²æ¨¡å‹ï¼‰
            if is_segmentation:
                meta = model_onnx.metadata_props.add()
                meta.key = "task_type"
                meta.value = "segmentation"
                
                meta = model_onnx.metadata_props.add()
                meta.key = "n_classes"
                meta.value = "1"
                
                print(f"âœ“ å·²åµŒå…¥åˆ†å‰²ä»»åŠ¡ä¿¡æ¯")
            
            # # 3. å°è¯•æ·»åŠ timmé¢„å¤„ç†å‚æ•°
            # try:
            #     import timm
            #     import timm.data
                
            #     # æ£€æŸ¥æ˜¯å¦æ˜¯timmæ¨¡å‹
            #     if arch in timm.list_models():
            #         print(f"\nğŸ“Š æ£€æµ‹åˆ°Timmæ¨¡å‹ï¼Œè·å–é¢„å¤„ç†é…ç½®...")
                    
            #         # åˆ›å»ºä¸´æ—¶æ¨¡å‹è·å–é¢„å¤„ç†é…ç½®
            #         temp_model = timm.create_model(arch, pretrained=False, num_classes=0)
            #         data_config = timm.data.resolve_model_data_config(temp_model)
                    
            #         # å°†é¢„å¤„ç†å‚æ•°å†™å…¥metadata
            #         for key, value in data_config.items():
            #             meta = model_onnx.metadata_props.add()
            #             meta.key = f"preprocessing/{key}"
            #             # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            #             meta.value = str(value)
                    
            #         print(f"âœ“ å·²åµŒå…¥é¢„å¤„ç†é…ç½®:")
            #         print(f"   - input_size: {data_config.get('input_size')}")
            #         print(f"   - mean: {data_config.get('mean')}")
            #         print(f"   - std: {data_config.get('std')}")
            #         print(f"   - interpolation: {data_config.get('interpolation')}")
            #     else:
            #         print(f"â„¹ï¸  éTimmæ¨¡å‹ï¼Œè·³è¿‡é¢„å¤„ç†é…ç½®åµŒå…¥")
            # except ImportError:
            #     print(f"â„¹ï¸  æœªå®‰è£…timmåº“ï¼Œè·³è¿‡é¢„å¤„ç†é…ç½®åµŒå…¥")
            # except Exception as e:
            #     print(f"âš ï¸  è·å–é¢„å¤„ç†é…ç½®å¤±è´¥: {e}")
            
            # ä¿å­˜æ›´æ–°åçš„ONNXæ¨¡å‹
            onnx.save(model_onnx, output_path)
            
        except Exception as e:
            print(f"âš ï¸  åµŒå…¥å…ƒæ•°æ®å¤±è´¥: {e}")

        export_time = time.time() - export_start
        total_time = time.time() - start_time
        
        print(f"å¯¼å‡ºONNXè€—æ—¶: {export_time*1000:.2f}ms")
        print(f"æ€»è€—æ—¶: {total_time*1000:.2f}ms")
        print(f"ONNXæ¨¡å‹å·²æˆåŠŸå¯¼å‡ºåˆ°: {Path(output_path).absolute()}")
        
        # éªŒè¯å¯¼å‡ºçš„æ¨¡å‹
        try:
            import onnx
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)
            print("ONNXæ¨¡å‹æ ¡éªŒé€šè¿‡")
            
            # ä»…å¯¹åˆ†ç±»æ¨¡å‹éªŒè¯ç±»åˆ«é¡ºåº
            if not is_segmentation:
                # PyTorchæ¨¡å‹ç±»åˆ«
                pth_categories = learn.dls.vocab
                
                # ONNXæ¨¡å‹ç±»åˆ«
                onnx_model = onnx.load(output_path)
                onnx_categories = None
                for meta in onnx_model.metadata_props:
                    if meta.key == "classes":
                        onnx_categories = meta.value.split(',')
                
                # éªŒè¯æ˜¯å¦ä¸€è‡´
                if pth_categories == onnx_categories:
                    print("ç±»åˆ«é¡ºåºä¸€è‡´!")
                else:
                    print("ç±»åˆ«é¡ºåºä¸ä¸€è‡´!")
                    print(f"PyTorchç±»åˆ«: {pth_categories}")
                    print(f"ONNXç±»åˆ«: {onnx_categories}")
            else:
                print("åˆ†å‰²æ¨¡å‹å¯¼å‡ºå®Œæˆï¼Œæ— éœ€éªŒè¯ç±»åˆ«é¡ºåº")
                
        except ImportError:
            print("æœªå®‰è£…onnxï¼Œè·³è¿‡æ¨¡å‹éªŒè¯")
        except Exception as ve:
            print(f"ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {ve}")
        
        return output_path
        
    except Exception as e:
        print(f"å¯¼å‡ºONNXæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='å°†PyTorchæ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼')
    
    parser.add_argument('--model_path', type=str, required=True, help='PyTorchæ¨¡å‹è·¯å¾„ (.pthæ–‡ä»¶)')
    parser.add_argument('--arch', type=str, default=None, help='æ¨¡å‹æ¶æ„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä»checkpointè‡ªåŠ¨è¯»å– (é»˜è®¤: Noneï¼Œè‡ªåŠ¨è¯»å–)')
    parser.add_argument('--output_path', type=str, default=None, help='ONNXæ¨¡å‹è¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤ä¸è¾“å…¥è·¯å¾„ç›¸åŒä½†åç¼€ä¸º.onnx')
    parser.add_argument('--img_size', type=int, default=None, help='è¾“å…¥å›¾åƒå¤§å°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä»checkpointè‡ªåŠ¨è¯»å–æˆ–æ¨æ–­ (é»˜è®¤: Noneï¼Œè‡ªåŠ¨è¯»å–)')
    parser.add_argument('--device', type=str, choices=['cuda', 'cpu'], default=None, 
                       help='è¿è¡Œè®¾å¤‡ã€‚é»˜è®¤è‡ªåŠ¨é€‰æ‹©ï¼Œå¦‚æœæœ‰GPUåˆ™ä½¿ç”¨GPU')
    parser.add_argument('--data_path', type=str, help='æ•°æ®é›†è·¯å¾„ï¼Œç”¨äºè·å–ç±»åˆ«ä¿¡æ¯')
    parser.add_argument('--classes', type=str, help='ç±»åˆ«åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚"cat,dog,horse"ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("è­¦å‘Šï¼šCUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        args.device = 'cpu'
    
    # å¦‚æœæä¾›äº†ç±»åˆ«åˆ—è¡¨ï¼Œè§£æå¹¶å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­
    if args.classes:
        classes = [c.strip() for c in args.classes.split(',')]
        os.environ['MODEL_CLASSES'] = ','.join(classes)
        print(f"ä½¿ç”¨æŒ‡å®šçš„ç±»åˆ«åˆ—è¡¨: {classes}")
    
    # æ‰§è¡Œå¯¼å‡º
    export_to_onnx(
        model_path=args.model_path,
        arch=args.arch,
        output_path=args.output_path,
        img_size=args.img_size,
        device=args.device,
        data_path=args.data_path,
        classes=args.classes
    )
