from fastai.vision.all import *
from fastai.callback.all import *
from fastai.distributed import *
from pathlib import Path
import pandas as pd
import os
import torch
import torch.nn as nn
from torch.optim import SGD as TorchSGD, Adam as TorchAdam, AdamW as TorchAdamW, RMSprop as TorchRMSprop
import argparse
import sys
import traceback
from fastai.callback.tracker import TrackerCallback
import mlflow
# import mlflow.pytorch
import matplotlib.pyplot as plt
# from sklearn.metrics import classification_report
import warnings
import numpy as np
# from PIL import Image
from torch.utils.data import Dataset as TorchDataset
from functools import partial
from accelerate.utils import write_basic_config, DistributedDataParallelKwargs

write_basic_config()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.pathï¼ˆç¡®ä¿å¯ä»¥å¯¼å…¥ utilsï¼‰
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹æ¨¡å—ï¼ˆåœ¨ fastai/models ç›®å½•ä¸‹ï¼‰
# ä½¿ç”¨ importlib æ¥é¿å…è·¯å¾„å†²çª
try:
    import importlib.util
    _models_path = Path(__file__).parent / 'models' / '__init__.py'
    _spec = importlib.util.spec_from_file_location('custom_models', _models_path)
    _custom_models = importlib.util.module_from_spec(_spec)
    sys.modules['custom_models'] = _custom_models  # æ³¨å†Œåˆ° sys.modules
    _spec.loader.exec_module(_custom_models)
    
    get_model = _custom_models.get_model
    list_models = _custom_models.list_models
    is_custom_model = _custom_models.is_custom_model
    CUSTOM_MODELS_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸  è‡ªå®šä¹‰æ¨¡å‹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("   å°†ä»…ä½¿ç”¨fastaiå†…ç½®æ¨¡å‹")
    CUSTOM_MODELS_AVAILABLE = False
    is_custom_model = lambda x: False

# è¿‡æ»¤ sklearn çš„ UndefinedMetricWarning
warnings.filterwarnings('ignore', message='.*Precision is ill-defined.*')
warnings.filterwarnings('ignore', message='.*Recall is ill-defined.*')
warnings.filterwarnings('ignore', message='.*F1 score is ill-defined.*')

# è¿‡æ»¤ pandas FutureWarning
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', message='.*Series.__getitem__.*')
warnings.filterwarnings('ignore', message='.*treating keys as positions.*')

# è¿‡æ»¤ DDP gradient strides è­¦å‘Šï¼ˆè¿™æ˜¯ PyTorch å†…éƒ¨æ€§èƒ½æç¤ºï¼Œä¸å½±å“è®­ç»ƒï¼‰
warnings.filterwarnings('ignore', message='.*Grad strides do not match bucket view strides.*')

# æ³¨æ„: threadpoolctl çš„ AttributeError æ˜¯ ctypes callback ä¸­çš„å¼‚å¸¸ï¼Œ
# æ— æ³•é€šè¿‡ Python çš„ warnings æ¨¡å—æŠ‘åˆ¶ã€‚è¿™æ˜¯ Python 3.12 çš„å·²çŸ¥å…¼å®¹æ€§é—®é¢˜ï¼Œ
# ä¸å½±å“è®­ç»ƒåŠŸèƒ½ã€‚å¦‚éœ€æŠ‘åˆ¶ï¼Œå¯ä»¥ï¼š
# 1. å‡çº§ threadpoolctl: pip install -U threadpoolctl
# 2. æˆ–åœ¨å¯åŠ¨æ—¶é‡å®šå‘ stderr: python train.py 2>/dev/null

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils import (
    is_main_process, setup_mlflow,
    get_segmentation_dls,
    DiceMetric, CombinedLoss,
    YOLOv11LRScheduler, LoadOptimizerStateCallback, ResumeEpochCallback,
    EarlyStoppingWithEvalCallback, SaveModelWithEpochCallback,
    MLflowMetricsCallback,
    DistributedValidationDiagnosticCallback,
)

def print_model_structure(model, model_name='Model', max_depth=5, input_size=(1, 3, 224, 224), show_shape=True):
    """
    é€šç”¨æ¨¡å‹ç»“æ„æ‰“å°å‡½æ•°ï¼Œæ”¯æŒæ‰€æœ‰ç±»å‹çš„æ¨¡å‹ï¼ˆTimmã€YOLOã€è‡ªå®šä¹‰ç­‰ï¼‰
    
    å‚æ•°:
        model: PyTorchæ¨¡å‹
        model_name: æ¨¡å‹åç§°
        max_depth: æœ€å¤§é€’å½’æ·±åº¦ï¼ˆé¿å…è¿‡æ·±ï¼‰
        input_size: è¾“å…¥å¼ é‡å¤§å° (batch, channels, height, width)
        show_shape: æ˜¯å¦æ˜¾ç¤ºæ¯å±‚çš„è¾“å…¥/è¾“å‡ºshape
    
    ç‰¹ç‚¹:
        - è‡ªåŠ¨é€’å½’å±•å¼€æ‰€æœ‰å±‚
        - æ ‘å½¢ç»“æ„æ˜¾ç¤ºå±‚çº§å…³ç³»
        - è‡ªåŠ¨æå–å±‚å‚æ•°ä¿¡æ¯
        - æ˜¾ç¤ºæ¯å±‚çš„è¾“å…¥/è¾“å‡ºshapeï¼ˆå¯é€‰ï¼‰
        - ç»Ÿä¸€æ ¼å¼ï¼Œæ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹
    """
    
    # å¦‚æœéœ€è¦æ˜¾ç¤ºshapeï¼Œå…ˆé€šè¿‡forward hookæ”¶é›†ä¿¡æ¯
    shape_info = {}
    hooks = []
    
    if show_shape:
        def hook_fn(name):
            def hook(module, input, output):
                try:
                    # è·å–è¾“å…¥shape
                    if isinstance(input, tuple) and len(input) > 0:
                        in_shape = tuple(input[0].shape) if hasattr(input[0], 'shape') else None
                    else:
                        in_shape = tuple(input.shape) if hasattr(input, 'shape') else None
                    
                    # è·å–è¾“å‡ºshape
                    if isinstance(output, tuple):
                        out_shape = tuple(output[0].shape) if hasattr(output[0], 'shape') else None
                    else:
                        out_shape = tuple(output.shape) if hasattr(output, 'shape') else None
                    
                    shape_info[name] = {
                        'input': in_shape,
                        'output': out_shape
                    }
                except:
                    pass
            return hook
        
        # æ³¨å†Œhooks
        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # åªå¯¹å¶å­èŠ‚ç‚¹æ³¨å†Œ
                hooks.append(module.register_forward_hook(hook_fn(name)))
        
        # æ‰§è¡Œä¸€æ¬¡forwardè·å–shapeä¿¡æ¯
        try:
            import torch
            device = next(model.parameters()).device
            dummy_input = torch.randn(input_size).to(device)
            model.eval()
            with torch.no_grad():
                _ = model(dummy_input)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–shapeä¿¡æ¯: {e}")
            show_shape = False
        finally:
            # ç§»é™¤hooks
            for hook in hooks:
                hook.remove()
    
    # æ‰“å°è¡¨å¤´
    print(f"\n{'='*150}")
    print(f"{model_name} - æ¨¡å‹ç»“æ„è¯¦è§£")
    print(f"{'='*150}")
    if show_shape:
        print(f"{'idx':<5} {'layer_name':<45} {'type':<25} {'params':>12} {'input_shape':<25} {'output_shape':<25} {'details':<15}")
        print(f"{'-'*150}")
    else:
        print(f"{'idx':<5} {'layer_name':<50} {'type':<30} {'params':>12} {'details':<30}")
        print(f"{'-'*120}")
    
    layer_idx = 0
    total_params = 0
    total_trainable = 0
    layer_count = 0
    
    def format_number(num):
        """æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º"""
        if num >= 1_000_000:
            return f"{num/1_000_000:.2f}M"
        elif num >= 1_000:
            return f"{num/1_000:.2f}K"
        else:
            return str(num)
    
    def extract_layer_info(module):
        """æå–å±‚çš„å…³é”®ä¿¡æ¯"""
        info = []
        
        # å·ç§¯å±‚
        if hasattr(module, 'in_channels') and hasattr(module, 'out_channels'):
            info.append(f"C:{module.in_channels}â†’{module.out_channels}")
            if hasattr(module, 'kernel_size'):
                k = module.kernel_size
                k_str = f"{k[0]}" if isinstance(k, (tuple, list)) else f"{k}"
                info.append(f"K:{k_str}")
            if hasattr(module, 'stride'):
                s = module.stride
                s_val = s[0] if isinstance(s, (tuple, list)) else s
                if s_val > 1:
                    info.append(f"S:{s_val}")
            if hasattr(module, 'padding'):
                p = module.padding
                p_val = p[0] if isinstance(p, (tuple, list)) else p
                if p_val > 0:
                    info.append(f"P:{p_val}")
        
        # å…¨è¿æ¥å±‚
        elif hasattr(module, 'in_features') and hasattr(module, 'out_features'):
            info.append(f"FC:{module.in_features}â†’{module.out_features}")
        
        # å½’ä¸€åŒ–å±‚
        elif hasattr(module, 'num_features'):
            info.append(f"Norm:{module.num_features}")
        elif hasattr(module, 'normalized_shape'):
            shape = module.normalized_shape
            if isinstance(shape, (tuple, list)):
                shape_str = f"{shape[0]}" if len(shape) == 1 else f"{shape}"
            else:
                shape_str = f"{shape}"
            info.append(f"Norm:{shape_str}")
        
        # Poolingå±‚
        elif hasattr(module, 'kernel_size') and 'Pool' in module.__class__.__name__:
            k = module.kernel_size
            k_str = f"{k[0]}" if isinstance(k, (tuple, list)) else f"{k}"
            info.append(f"Pool:K{k_str}")
            if hasattr(module, 'stride'):
                s = module.stride
                s_val = s[0] if isinstance(s, (tuple, list)) else s
                if s_val and s_val > 1:
                    info.append(f"S:{s_val}")
        
        # Dropout
        elif hasattr(module, 'p') and 'Dropout' in module.__class__.__name__:
            info.append(f"Drop:{module.p:.2f}")
        
        # Embedding
        elif hasattr(module, 'num_embeddings') and hasattr(module, 'embedding_dim'):
            info.append(f"Emb:{module.num_embeddings}Ã—{module.embedding_dim}")
        
        # RNN/LSTM/GRU
        elif hasattr(module, 'input_size') and hasattr(module, 'hidden_size'):
            info.append(f"RNN:{module.input_size}â†’{module.hidden_size}")
            if hasattr(module, 'num_layers'):
                info.append(f"L:{module.num_layers}")
        
        # Attention (Transformer)
        elif hasattr(module, 'embed_dim'):
            info.append(f"Attn:D{module.embed_dim}")
            if hasattr(module, 'num_heads'):
                info.append(f"H:{module.num_heads}")
        
        return ", ".join(info) if info else ""
    
    def format_shape(shape):
        """æ ¼å¼åŒ–shapeä¿¡æ¯"""
        if shape is None:
            return "-"
        # åªæ˜¾ç¤º (C, H, W) æˆ–å…³é”®ç»´åº¦ï¼Œçœç•¥batch
        if len(shape) == 4:  # (B, C, H, W)
            return f"({shape[1]},{shape[2]},{shape[3]})"
        elif len(shape) == 3:  # (B, C, L) æˆ– (C, H, W)
            return f"({shape[1]},{shape[2]})"
        elif len(shape) == 2:  # (B, D)
            return f"({shape[1]})"
        else:
            return str(shape)
    
    def print_layers(module, prefix='', depth=0):
        """é€’å½’æ‰“å°æ‰€æœ‰å±‚"""
        nonlocal layer_idx, total_params, total_trainable, layer_count
        
        # æ·±åº¦é™åˆ¶
        if depth >= max_depth:
            return
        
        # éå†æ‰€æœ‰å­æ¨¡å—
        for name, child in module.named_children():
            # è®¡ç®—å‚æ•°é‡
            params = sum(p.numel() for p in child.parameters())
            trainable_params = sum(p.numel() for p in child.parameters() if p.requires_grad)
            
            # è·å–æ¨¡å—ç±»å‹ï¼ˆç®€åŒ–åç§°ï¼‰
            module_type = child.__class__.__name__
            
            # æå–è¯¦ç»†ä¿¡æ¯
            details = extract_layer_info(child)
            
            # æ„å»ºå±‚åç§°ï¼ˆå¸¦ç¼©è¿›ï¼‰
            indent = "  " * depth
            if depth == 0:
                layer_name = f"{name}"
            else:
                layer_name = f"{indent}â””â”€ {name}"
            
            # åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºè¯¥å±‚
            # 1. æœ‰å‚æ•°çš„å±‚å¿…é¡»æ˜¾ç¤º
            # 2. é¡¶å±‚å®¹å™¨å¿…é¡»æ˜¾ç¤º
            # 3. é‡è¦çš„æ— å‚æ•°å±‚ä¹Ÿæ˜¾ç¤ºï¼ˆPool, Dropout, Activationç­‰ï¼‰
            is_leaf = len(list(child.children())) == 0
            is_important_container = depth <= 1 and not is_leaf
            is_important_layer = is_leaf and (
                'Pool' in module_type or 
                'Dropout' in module_type or 
                'Activation' in module_type or
                'ReLU' in module_type or
                'Sigmoid' in module_type or
                'Tanh' in module_type or
                'Softmax' in module_type or
                'Flatten' in module_type or
                'Identity' in module_type
            )
            
            show_layer = params > 0 or is_important_container or is_important_layer
            
            if show_layer:
                # è·å–shapeä¿¡æ¯
                full_name = f"{prefix}.{name}" if prefix else name
                shapes = shape_info.get(full_name, {'input': None, 'output': None})
                
                # å¦‚æœå±‚æœ‰å‚æ•°ä½†ä¸å¯è®­ç»ƒï¼Œæ ‡è®°å‡ºæ¥
                frozen_mark = ""
                if params > 0 and trainable_params == 0:
                    frozen_mark = " [FROZEN]"
                
                if show_shape:
                    # æ ¼å¼åŒ–è¾“å‡ºï¼ˆå¸¦shapeï¼‰
                    layer_name_str = f"{layer_name:<45}"
                    type_str = f"{module_type:<25}"
                    params_str = f"{format_number(params):>12}"
                    in_shape_str = f"{format_shape(shapes['input']):<25}"
                    out_shape_str = f"{format_shape(shapes['output']):<25}"
                    details_str = f"{details:<15}"
                    
                    print(f"{layer_idx:<5} {layer_name_str} {type_str} {params_str} {in_shape_str} {out_shape_str} {details_str}{frozen_mark}")
                else:
                    # æ ¼å¼åŒ–è¾“å‡ºï¼ˆä¸å¸¦shapeï¼‰
                    layer_name_str = f"{layer_name:<50}"
                    type_str = f"{module_type:<30}"
                    params_str = f"{format_number(params):>12}"
                    details_str = f"{details:<30}"
                    
                    print(f"{layer_idx:<5} {layer_name_str} {type_str} {params_str} {details_str}{frozen_mark}")
                
                # ç´¯è®¡ç»Ÿè®¡
                if params > 0:
                    total_params += params
                    total_trainable += trainable_params
                    layer_count += 1
                
                layer_idx += 1
            
            # é€’å½’å¤„ç†å­æ¨¡å—
            if len(list(child.children())) > 0 and depth < max_depth - 1:
                print_layers(child, f"{prefix}.{name}" if prefix else name, depth + 1)
    
    # å¼€å§‹æ‰“å°
    try:
        print_layers(model)
    except Exception as e:
        print(f"âš ï¸  æ‰“å°æ¨¡å‹ç»“æ„æ—¶å‡ºé”™: {e}")
        print(f"   å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹å¼...")
        # å¤‡ç”¨æ–¹å¼ï¼šä½¿ç”¨named_modules
        for idx, (name, module) in enumerate(model.named_modules()):
            if len(list(module.children())) == 0:  # åªæ˜¾ç¤ºå¶å­èŠ‚ç‚¹
                params = sum(p.numel() for p in module.parameters())
                if params > 0:
                    module_type = module.__class__.__name__
                    details = extract_layer_info(module)
                    print(f"{idx:<5} {name:<50} {module_type:<30} {format_number(params):>12} {details:<30}")
                    total_params += params
                    layer_count += 1
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    sep_line = '='*150 if show_shape else '='*120
    print(sep_line)
    print(f"æ€»è®¡: {layer_count} å±‚")
    print(f"å‚æ•°: {format_number(total_params)} ({total_params:,}) - å¯è®­ç»ƒ: {format_number(total_trainable)} ({total_trainable:,})")
    
    # ä¼°ç®—æ¨¡å‹å¤§å°
    model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
    print(f"æ¨¡å‹å¤§å°: ~{model_size_mb:.2f} MB (float32)")
    
    if show_shape:
        print(f"è¾“å…¥å°ºå¯¸: {input_size}")
    
    print(sep_line)
    print()
    # ä¼°ç®—æ¨¡å‹å¤§å°å’ŒFLOPs
    model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
    estimated_flops = total_params * 2 / 1e9  # ç²—ç•¥ä¼°ç®—
    
    print(f"æ¨¡å‹å¤§å°: ~{model_size_mb:.2f} MB (float32)")
    print(f"ä¼°ç®—FLOPs: ~{estimated_flops:.2f} GFLOPs")
    print(f"{'='*120}\n")

# mlflow.config.enable_system_metrics_logging()
# mlflow.config.set_system_metrics_sampling_interval(1)
# mlflow.pytorch.autolog(log_models=False)  # å¯ç”¨autologä½†ç¦ç”¨è‡ªåŠ¨æ¨¡å‹ä¿å­˜ï¼Œé¿å…ä¸æ‰‹åŠ¨log_modelå†²çª

def call_evaluation_script(
    learn,
    model_path,
    mlflow_run_id,
    project_name='default'
):
    """
    è°ƒç”¨ç‹¬ç«‹çš„è¯„ä¼°æ¨¡å—ï¼ˆå¤ç”¨è®­ç»ƒæ•°æ®ï¼Œç»Ÿä¸€ä½¿ç”¨ç›´æ¥è°ƒç”¨æ–¹å¼ï¼‰
    
    Args:
        learn: FastAI Learner å¯¹è±¡ï¼ˆå¤ç”¨è®­ç»ƒæ—¶çš„æ•°æ®ï¼‰
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºåŠ è½½bestæƒé‡ï¼‰
        mlflow_run_id: MLflowè¿è¡ŒID
        project_name: é¡¹ç›®åç§°ï¼ˆç”¨äºç»„ç»‡è¾“å‡ºç›®å½•ï¼‰
    """
    
    try:
        print("\nğŸ“Š è°ƒç”¨è¯„ä¼°æ¨¡å—ï¼ˆå¤ç”¨è®­ç»ƒæ•°æ®ï¼‰...")
        
        # åŠ¨æ€å¯¼å…¥evaluateæ¨¡å—
        import importlib.util
        spec = importlib.util.spec_from_file_location(
            "evaluate", 
            Path(__file__).parent / "evaluate.py"
        )
        evaluate_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(evaluate_module)
        
        # æ„å»ºè¾“å‡ºç›®å½•
        output_dir = f"./evaluation_results/{project_name}/{Path(model_path).parent.name}"
        
        # ç›´æ¥è°ƒç”¨è¯„ä¼°å‡½æ•°ï¼ˆä¼ å…¥learnå¯¹è±¡ï¼Œå¤ç”¨æ•°æ®ï¼‰
        # evaluate.pyå†…éƒ¨ä¼šæ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        evaluate_module.evaluate_with_learner(
            learn=learn,
            model_path=str(model_path),
            output_dir=output_dir,
            mlflow_run_id=mlflow_run_id
        )
        
        print("   âœ… è¯„ä¼°å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def load_classification_data(data_path, train_size=None, val_size=None, mlflow_run=None):
    """
    åŠ è½½åˆ†ç±»æ•°æ®é›†ï¼Œè¿”å› train_df å’Œ valid_df
    
    Args:
        data_path: æ•°æ®é›†æ ¹è·¯å¾„ (åŒ…å« train/ å’Œ val/ å­ç›®å½•)
        train_size: æ¯ä¸ªç±»åˆ«ä¿ç•™çš„è®­ç»ƒæ ·æœ¬æ•°ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨
        val_size: æ¯ä¸ªç±»åˆ«ä¿ç•™çš„éªŒè¯æ ·æœ¬æ•°ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨
        mlflow_run: MLflow run å¯¹è±¡ï¼Œå¦‚æœæä¾›åˆ™ä¸Šä¼ æ•°æ®åˆ†å¸ƒå›¾
    
    Returns:
        tuple: (train_df, valid_df) - ä¸¤ä¸ª DataFrameï¼ŒåŒ…å« 'filename' å’Œ 'label' åˆ—
    """
    path = Path(data_path).absolute()
    cache_dir = path / ".cache"
    cache_dir.mkdir(exist_ok=True)

    def build_df(root_dir, subset):
        """æ„å»ºæ•°æ®é›†DataFrameï¼Œæ”¯æŒç¼“å­˜å’Œç‰ˆæœ¬æ£€æµ‹"""
        cache_file = cache_dir / f"{subset}_df.pkl"
        version_file = cache_dir / f"{subset}_version.txt"
        base_path = root_dir / subset
        
        # è®¡ç®—æ•°æ®é›†ç‰ˆæœ¬ï¼ˆåŸºäºæ–‡ä»¶æ•°é‡å’Œæ€»å¤§å°ï¼‰
        if base_path.exists():
            all_images = list(base_path.rglob("*.*"))
            image_files = [f for f in all_images if f.is_file() and f.suffix.lower() in ['.jpg', '.png', '.jpeg', '.bmp']]
            # ç‰ˆæœ¬ = æ–‡ä»¶æ•°é‡ + æ€»å¤§å° + æœ€æ–°ä¿®æ”¹æ—¶é—´
            total_size = sum(f.stat().st_size for f in image_files)
            latest_mtime = max((f.stat().st_mtime for f in image_files), default=0)
            current_version = f"{len(image_files)}_{total_size}_{latest_mtime}"
        else:
            current_version = "0"
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if cache_file.exists() and version_file.exists():
            cached_version = version_file.read_text().strip()
            if cached_version == current_version:
                if is_main_process():
                    print(f"âœ“ ä½¿ç”¨ç¼“å­˜çš„ {subset} æ•°æ®é›† (æ–‡ä»¶æ•°: {current_version.split('_')[0]})")
                return pd.read_pickle(cache_file)
        
        # é‡æ–°æ„å»ºæ•°æ®é›†
        if is_main_process():
            print(f"âš™ï¸  æ„å»º {subset} æ•°æ®é›†ï¼ˆæ£€æµ‹åˆ°æ•°æ®å˜æ›´ï¼‰...")
        
        records = []
        for img_path in base_path.rglob("*.*"):
            if img_path.is_file() and img_path.suffix.lower() in ['.jpg', '.png', '.jpeg', '.bmp']:
                rel_path = img_path.relative_to(root_dir)
                class_label = img_path.parent.name
                records.append({
                    "filename": str(rel_path),
                    "label": class_label
                })
        
        df = pd.DataFrame(records)
        
        # ä¿å­˜ç¼“å­˜å’Œç‰ˆæœ¬
        df.to_pickle(cache_file)
        version_file.write_text(current_version)
        
        if is_main_process():
            print(f"âœ“ {subset} æ•°æ®é›†æ„å»ºå®Œæˆ: {len(df)} å¼ å›¾ç‰‡")
        
        return df

    train_df = build_df(path, "train")
    valid_df = build_df(path, "val")
    
    # é™åˆ¶æ•°æ®é›†å¤§å°ï¼ˆæ¯ä¸ªç±»åˆ«ä¿ç•™æŒ‡å®šæ•°é‡çš„æ ·æœ¬ï¼‰
    if train_size and train_size > 0:
        original_train = len(train_df)
        sampled_dfs = []
        for label in train_df['label'].unique():
            label_df = train_df[train_df['label'] == label]
            n_samples = min(train_size, len(label_df))
            sampled_dfs.append(label_df.sample(n=n_samples, random_state=42))
        train_df = pd.concat(sampled_dfs, ignore_index=True)
        if is_main_process():
            print(f"âš ï¸  è®­ç»ƒé›†å·²é™åˆ¶ï¼ˆæ¯ç±»{train_size}ä¸ªæ ·æœ¬ï¼‰: {original_train} â†’ {len(train_df)}")
    
    if val_size and val_size > 0:
        original_val = len(valid_df)
        sampled_dfs = []
        for label in valid_df['label'].unique():
            label_df = valid_df[valid_df['label'] == label]
            n_samples = min(val_size, len(label_df))
            sampled_dfs.append(label_df.sample(n=n_samples, random_state=42))
        valid_df = pd.concat(sampled_dfs, ignore_index=True)
        if is_main_process():
            print(f"âš ï¸  éªŒè¯é›†å·²é™åˆ¶ï¼ˆæ¯ç±»{val_size}ä¸ªæ ·æœ¬ï¼‰: {original_val} â†’ {len(valid_df)}")

    train_df = train_df.reset_index(drop=True)
    valid_df = valid_df.reset_index(drop=True)
    
    # æ‰“å°æ•°é‡
    if is_main_process():
        print(f"  è®­ç»ƒé›†æ•°é‡: {len(train_df)}")
        print(f"  éªŒè¯é›†æ•°é‡: {len(valid_df)}")
    
    # ç”Ÿæˆæ•°æ®é›†åˆ†å¸ƒå¯è§†åŒ–å›¾è¡¨å¹¶ä¸ŠæŠ¥åˆ° MLflow (åªåœ¨ä¸»è¿›ç¨‹)
    if mlflow_run is not None and is_main_process():
        try:
            # è·å–ç±»åˆ«åˆ†å¸ƒ
            train_class_dist = train_df['label'].value_counts().sort_index()
            val_class_dist = valid_df['label'].value_counts().sort_index()
            
            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(12, 6))
            
            # è·å–æ‰€æœ‰ç±»åˆ«ï¼ˆæŒ‰å­—æ¯æ’åºï¼‰
            all_classes = sorted(set(train_class_dist.index) | set(val_class_dist.index))
            x = np.arange(len(all_classes))
            width = 0.35
            
            # å‡†å¤‡æ•°æ®
            train_counts = [train_class_dist.get(cls, 0) for cls in all_classes]
            val_counts = [val_class_dist.get(cls, 0) for cls in all_classes]
            
            # ç»˜åˆ¶æŸ±çŠ¶å›¾
            bars1 = ax.bar(x - width/2, train_counts, width, label='Train', alpha=0.8, color='#1f77b4')
            bars2 = ax.bar(x + width/2, val_counts, width, label='Validation', alpha=0.8, color='#ff7f0e')
            
            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            ax.set_xlabel('Classes', fontsize=12, fontweight='bold')
            ax.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')
            ax.set_title('Dataset Distribution by Class', fontsize=14, fontweight='bold', pad=20)
            ax.set_xticks(x)
            ax.set_xticklabels(all_classes, rotation=45, ha='right')
            ax.legend(fontsize=10)
            ax.grid(axis='y', alpha=0.3, linestyle='--')
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            def add_value_labels(bars):
                for bar in bars:
                    height = bar.get_height()
                    if height > 0:  # åªæ˜¾ç¤ºéé›¶å€¼
                        ax.text(bar.get_x() + bar.get_width()/2., height,
                               f'{int(height)}',
                               ha='center', va='bottom', fontsize=8)
            
            add_value_labels(bars1)
            add_value_labels(bars2)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
            stats_text = f'Total Train: {len(train_df)}  |  Total Val: {len(valid_df)}  |  Classes: {len(all_classes)}'
            ax.text(0.5, 1.05, stats_text, transform=ax.transAxes,
                   ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
            
            plt.tight_layout()
            
            # ä¸Šä¼ å›¾è¡¨åˆ° MLflow
            mlflow.log_figure(fig, "dataset/class_distribution.png")
            plt.close(fig)
            
            print(f"\nğŸ“Š æ•°æ®é›†åˆ†å¸ƒå›¾å·²ä¸Šä¼ åˆ° MLflow")
            print(f"   - è®­ç»ƒé›†æ€»æ•°: {len(train_df)}")
            print(f"   - éªŒè¯é›†æ€»æ•°: {len(valid_df)}")
            print(f"   - ç±»åˆ«æ•°é‡: {len(all_classes)}\n")
            
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆæ•°æ®é›†åˆ†å¸ƒå›¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    return train_df, valid_df


def calculate_grad_acc(batch_size, n_gpus, target_batch=256):
    """
    è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œä¿æŒæœ‰æ•ˆ batch_size æ’å®š
    
    Args:
        batch_size: æ¯ä¸ª GPU çš„ batch size
        n_gpus: GPU æ•°é‡
        target_batch: ç›®æ ‡æœ‰æ•ˆ batch sizeï¼ˆé»˜è®¤ 256ï¼‰
    
    Returns:
        int: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    
    ç¤ºä¾‹:
        å•å¡ batch=64  â†’ grad_acc=4  â†’ æœ‰æ•ˆbatch=256
        4å¡ batch=64   â†’ grad_acc=1  â†’ æœ‰æ•ˆbatch=256
        å•å¡ batch=32  â†’ grad_acc=8  â†’ æœ‰æ•ˆbatch=256
    """
    effective_batch_per_step = batch_size * n_gpus
    grad_acc = max(1, target_batch // effective_batch_per_step)
    
    actual_batch = effective_batch_per_step * grad_acc
    
    # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡º
    if is_main_process():
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ¢¯åº¦ç´¯ç§¯è‡ªåŠ¨é…ç½®:")
        print(f"{'='*60}")
        print(f"  è¾“å…¥å‚æ•°:")
        print(f"    - batch_size (per GPU): {batch_size}")
        print(f"    - n_gpus: {n_gpus}")
        print(f"    - ç›®æ ‡æœ‰æ•ˆ batch: {target_batch}")
        print(f"  è®¡ç®—ç»“æœ:")
        print(f"    - grad_acc: {grad_acc}")
        print(f"    - å®é™…æœ‰æ•ˆ batch: {actual_batch}")
        print(f"  å†…å­˜ä¼°ç®—:")
        print(f"    - æ¢¯åº¦å†…å­˜å€æ•°: ~{grad_acc}x")
        
        # è­¦å‘Šæ£€æŸ¥
        if actual_batch < target_batch * 0.8:
            print(f"  âš ï¸  å®é™… batch ({actual_batch}) å°äºç›®æ ‡çš„ 80%")
        if actual_batch > target_batch * 2:
            print(f"  âš ï¸  å®é™… batch ({actual_batch}) è¿‡å¤§ï¼Œå¯èƒ½å½±å“æ”¶æ•›")
        
        print(f"{'='*60}\n")
    
    return grad_acc

def train_model(
    data_path,
    model_path='last',
    img_size=320,
    batch_size=256,
    epochs=100,
    lr0=1e-3,
    lrf=0.1,  # æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹ï¼ˆç›¸å¯¹äºåˆå§‹å­¦ä¹ ç‡ï¼‰ï¼Œé»˜è®¤0.1ï¼ˆ10%ï¼‰
    arch='resnet18',
    wd=1e-3,
    early_stopping=5,
    grad_acc=4,
    load_model=None,
    auto_resume=True,  # è‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„bestæ¨¡å‹ç»§ç»­è®­ç»ƒ
    task_name='resnet18',
    project_name='ai-classifier',
    train_size=None,  # æ¯ä¸ªç±»åˆ«ä¿ç•™çš„è®­ç»ƒæ ·æœ¬æ•°ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨
    val_size=None,    # æ¯ä¸ªç±»åˆ«ä¿ç•™çš„éªŒè¯æ ·æœ¬æ•°ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨
    device=None,      # æŒ‡å®šGPUè®¾å¤‡ï¼Œä¾‹å¦‚ 'cuda:0', 'cuda:1' æˆ– None (è‡ªåŠ¨é€‰æ‹©)
    scheduler_type='cosine',  # å­¦ä¹ ç‡è°ƒåº¦ç±»å‹: 'cosine', 'cosine_restarts', 'step'
    min_lr=None,  # æœ€å°å­¦ä¹ ç‡ï¼ŒNoneåˆ™ä½¿ç”¨lr*0.01
    distributed=False,  # æ˜¯å¦å¯ç”¨å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
    models_base_dir='runs',  # ç»Ÿä¸€çš„æ¨¡å‹ä¿å­˜åŸºç¡€ç›®å½•
    only_val=False,  # ä»…è¿›è¡ŒéªŒè¯ï¼Œä¸è®­ç»ƒ
    scale=1.0,  # å›¾åƒç¼©æ”¾æ¯”ä¾‹ï¼ˆä¸»è¦ç”¨äºåˆ†å‰²ä»»åŠ¡ï¼‰
    export_onnx=True,  # è®­ç»ƒå®Œæˆåè‡ªåŠ¨å¯¼å‡ºONNX
    mlflow_parent_run_id=None,  # MLflowçˆ¶run IDï¼ˆç”¨äºåµŒå¥—runsï¼‰
    skip_mlflow_model_upload=False,  # è·³è¿‡æ¨¡å‹ä¸Šä¼ åˆ°MLflowï¼ˆè°ƒä¼˜æ¨¡å¼ï¼‰
    optimizer='Adam',  # ä¼˜åŒ–å™¨ç±»å‹: 'SGD', 'Adam', 'AdamW', 'RMSprop'
    drop_path_rate=0.1,  # DropPathæ­£åˆ™åŒ–æ¦‚ç‡ï¼ˆç”¨äºConvNeXtç­‰æ¨¡å‹ï¼Œ0=ç¦ç”¨ï¼‰
):
    
    # æ ¹æ®æ¶æ„è‡ªåŠ¨åˆ¤æ–­ä»»åŠ¡ç±»å‹
    is_segmentation = arch.lower().endswith('_seg')
    task_type = 'segmentation' if is_segmentation else 'classification'
    
    # å¤„ç† lr0 å‚æ•°ï¼ˆå…¼å®¹YOLOä¹ æƒ¯ï¼‰
    if is_main_process():
        print(f"âš™ï¸  ä½¿ç”¨ lr0 å‚æ•°: {lr0}")
    
    # è·å–ä¼˜åŒ–å™¨å‡½æ•°
    # æ³¨æ„ï¼šFastAI éœ€è¦ä½¿ç”¨ partial åŒ…è£…çš„ä¼˜åŒ–å™¨ç±»ï¼ˆä¸æ˜¯å®ä¾‹ï¼‰
    optimizer_map = {
        'SGD': SGD,
        'Adam': Adam,
        'RAdam': RAdam,
        'RMSprop': RMSProp,
    }
    
    opt_func = optimizer_map.get(optimizer, partial(TorchAdam, betas=(0.9, 0.999), eps=1e-8))
    if is_main_process():
        print(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {task_type} ({'åˆ†å‰²' if is_segmentation else 'åˆ†ç±»'})")
        print(f"âš™ï¸  ä¼˜åŒ–å™¨: {optimizer}")
        print(f"ğŸ“Š åˆå§‹å­¦ä¹ ç‡: {lr0}")
    
    # åˆ†å‰²ä»»åŠ¡çš„é»˜è®¤å‚æ•°è°ƒæ•´
    if is_segmentation:
        if is_main_process():
            print(f"   åˆ†å‰²ä»»åŠ¡é»˜è®¤ img_size ä¸º {img_size}")
            print(f"   åˆ†å‰²ä»»åŠ¡é»˜è®¤ batch_size ä¸º {batch_size}")

    
    # æ„å»ºç»Ÿä¸€çš„æ¨¡å‹ä¿å­˜ç›®å½•: models_base_dir/project_name/task_name/
    model_save_dir = Path(models_base_dir) / project_name / task_name
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    if is_main_process():
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•: {model_save_dir.absolute()}")
    
    # è®¾ç½®GPUè®¾å¤‡
    if not distributed:
        if device is not None:
            os.environ['CUDA_VISIBLE_DEVICES'] = device.split(':')[-1] if ':' in device else device
            print(f"ä½¿ç”¨æŒ‡å®šGPU: {device}")
        else:
            print(f"ä½¿ç”¨é»˜è®¤GPUé…ç½®")
    else:
        # åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼
        n_gpus = torch.cuda.device_count()
        if is_main_process():
            print(f"ğŸš€ å¯ç”¨å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ: {n_gpus} GPUs")
            print(f"   è¿›ç¨‹ Rank: {rank_distrib() if num_distrib() > 1 else 0}/{num_distrib()}")
            
            # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œbatch_sizeæ˜¯æ¯ä¸ªGPUçš„batchå¤§å°
            # æ€»batch_size = batch_size * n_gpus
            print(f"   æ¯GPU batch_size: {batch_size}")
            print(f"   æ€»æœ‰æ•ˆ batch_size: {batch_size * n_gpus}")
    
    # åˆå§‹åŒ– MLflow (åªåœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–)
    mlflow_run = None
    if is_main_process():
        try:
            # å¦‚æœæœ‰çˆ¶run IDï¼ˆè°ƒä¼˜æ¨¡å¼ï¼‰ï¼Œåˆ™é‡ç”¨è¯¥run
            if mlflow_parent_run_id:
                existing_run = mlflow.active_run()
                if existing_run and existing_run.info.run_id == mlflow_parent_run_id:
                    mlflow_run = existing_run
                    print(f"âœ… ä½¿ç”¨è°ƒä¼˜ Trial Run: {mlflow_parent_run_id}")
                else:
                    # ä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œå› ä¸ºtune.pyå·²ç»å¯åŠ¨äº†run
                    print(f"âš ï¸  è­¦å‘Š: çˆ¶runä¸æ˜¯æ´»è·ƒçŠ¶æ€ï¼Œä½¿ç”¨ç°æœ‰run")
                    mlflow_run = existing_run
            else:
                # æ­£å¸¸è®­ç»ƒæ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ active runï¼ˆä¾‹å¦‚åœ¨è°ƒä¼˜æ—¶ï¼‰
                existing_run = mlflow.active_run()
                if existing_run:
                    print(f"âœ… ä½¿ç”¨ç°æœ‰ MLflow Run: {existing_run.info.run_id}")
                    mlflow_run = existing_run
                else:
                    mlflow_run = setup_mlflow(project_name, task_name)
                    print(f"âœ… MLflow Run ID: {mlflow_run.info.run_id}")
                    print(f"   Tracking URI: {mlflow.get_tracking_uri()}")
                    print(f"   Experiment: {project_name}")
                    print(f"   Run Name: {task_name}")
        except Exception as e:
            print(f"âš ï¸  MLflow åˆå§‹åŒ–å¤±è´¥: {e}")
            print("   å°†ç»§ç»­è®­ç»ƒä½†ä¸è®°å½•åˆ° MLflow")
    
    # è®°å½•è¶…å‚æ•° (åªåœ¨ä¸»è¿›ç¨‹)
    if mlflow_run is not None:
        try:
            mlflow.log_params({
                'data_path': str(data_path),
                'model_path': model_path,
                'models_base_dir': str(model_save_dir),
                'img_size': img_size,
                'batch_size': batch_size,
                'epochs': epochs,
                'lr0': lr0,
                'lrf': lrf,
                'arch': arch,
                'task_type': task_type,
                'wd': wd,
                'optimizer': optimizer,
                'early_stopping': early_stopping,
                'grad_acc': grad_acc if not is_segmentation else 1,
                'auto_resume': auto_resume,
                'train_size': train_size,
                'val_size': val_size,
                'device': device,
                'scheduler_type': scheduler_type,
                'min_lr': min_lr if min_lr is not None else lr0 * 0.01,
                'distributed': distributed,
                'n_gpus': torch.cuda.device_count() if distributed else 1,
                'scale': scale if is_segmentation else 1.0,
            })
        except Exception as e:
            print(f"âš ï¸  è®°å½•å‚æ•°åˆ° MLflow å¤±è´¥: {e}")
    
    path = Path(data_path).absolute()
    
    # è®¡ç®—åˆé€‚çš„ num_workers
    if distributed:
        world_size = int(os.environ.get('WORLD_SIZE', 1))
        cpus_per_process = os.cpu_count() // world_size
        num_workers = max(8, max(4, cpus_per_process // 8))
        if is_main_process():
            print(f"ğŸ’¾ DataLoader num_workers: {num_workers} (CPUæ ¸å¿ƒ: {os.cpu_count()}, åˆ†å¸ƒå¼ {world_size} è¿›ç¨‹, æ¯è¿›ç¨‹ {cpus_per_process} æ ¸)")
    else:
        gpu_count = max(1, torch.cuda.device_count())
        num_workers = max(8, max(4, os.cpu_count() // gpu_count // 4))
        if is_main_process():
            print(f"ğŸ’¾ DataLoader num_workers: {num_workers} (CPUæ ¸å¿ƒ: {os.cpu_count()}, å•æœº {gpu_count} GPU)")
    
    if is_main_process():
        print(f"\nğŸ”§ å‡†å¤‡ DataLoaders...")
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºä¸åŒçš„ DataLoaders
    if is_segmentation:
        # åˆ†å‰²ä»»åŠ¡ï¼šä½¿ç”¨ imgs/ å’Œ masks/ ç›®å½•
        dls = get_segmentation_dls(data_dir=data_path, batch_size=batch_size, img_size=img_size, 
                                   scale=scale, num_workers=num_workers, use_disk_cache=True)
        
        if is_main_process():
            print(f"   è®­ç»ƒé›†å¤§å°: {len(dls.train_ds)}")
            print(f"   éªŒè¯é›†å¤§å°: {len(dls.valid_ds)}")
        
        # åˆ†å‰²ä»»åŠ¡ä¸éœ€è¦è®°å½•ç±»åˆ«ä¿¡æ¯åˆ° MLflow
        
    else:
        # åˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨ train/ å’Œ val/ ç›®å½•
        train_df, valid_df = load_classification_data(
            data_path=data_path,
            train_size=train_size,
            val_size=val_size,
            mlflow_run=mlflow_run
        )
        
        # ğŸ”§ ä¿®å¤: æ‰“ä¹±éªŒè¯é›†ï¼Œç¡®ä¿å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒæ—¶éªŒè¯å‡†ç¡®
        # é—®é¢˜: éªŒè¯é›†æŒ‰ç±»åˆ«æ’åº + DistributedSampleræŒ‰é¡ºåºåˆ†é…
        #      â†’ ä¸åŒGPUå¤„ç†ä¸åŒç±»åˆ« â†’ ç±»åˆ«éš¾åº¦å·®å¼‚å¯¼è‡´éªŒè¯lossä¸å‡†ç¡®
        # è§£å†³: æ‰“ä¹±éªŒè¯é›†ï¼Œä½¿æ‰€æœ‰GPUçœ‹åˆ°ç›¸ä¼¼çš„ç±»åˆ«åˆ†å¸ƒ
        if is_main_process():
            print(f"ğŸ”€ æ‰“ä¹±éªŒè¯é›†ä»¥ç¡®ä¿å¤šGPUéªŒè¯å‡†ç¡®æ€§...")
        valid_df = valid_df.sample(frac=1.0, random_state=42).reset_index(drop=True)

        # åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæ·»åŠ  is_valid åˆ—ç”¨äºåˆ†å‰²
        train_df['is_valid'] = False
        valid_df['is_valid'] = True
        combined_df = pd.concat([train_df, valid_df], ignore_index=True)
        
        if is_main_process():
            print(f"   åˆå¹¶å DataFrame é•¿åº¦: {len(combined_df)}")
            print(f"   è®­ç»ƒé›†: {(~combined_df['is_valid']).sum()}")
            print(f"   éªŒè¯é›†: {combined_df['is_valid'].sum()}")

        # ä½¿ç”¨ ImageDataLoaders.from_df åˆ›å»º DataLoaders
        dls = ImageDataLoaders.from_df(
            combined_df,
            path=path,
            valid_col='is_valid',
            fn_col='filename',
            label_col='label',
            num_workers=num_workers,
            item_tfms=Resize(img_size, method='bicubic'),  # ä½¿ç”¨ bicubic æ’å€¼ï¼ˆTimm ConvNeXt æ¨èï¼‰
            bs=batch_size,
            batch_tfms=aug_transforms(flip_vert=False, max_rotate=5, max_zoom=1.05, max_lighting=0.1)
        )
        
        if is_main_process():
            print(f"   åˆ†ç±»: {dls.vocab}")
            print(f"   train_df é•¿åº¦: {len(train_df)}")
            print(f"   dls.train.dataset é•¿åº¦: {len(dls.train.dataset)}")
            print(f"   æ˜¯å¦ç›¸ç­‰: {len(train_df) == len(dls.train.dataset)}") 

        # è®°å½•ç±»åˆ«ä¿¡æ¯ (åªåœ¨ä¸»è¿›ç¨‹)
        if mlflow_run is not None:
            try:
                mlflow.log_metric('dataset/num_classes', len(dls.vocab))
                import json
                class_names_path = model_save_dir / 'class_names.json'
                with open(class_names_path, 'w') as f:
                    json.dump(list(dls.vocab), f)
                print(f"   ä¿å­˜ç±»åˆ«åç§°åˆ°: {class_names_path}")
                mlflow.log_artifact(str(class_names_path))
            except Exception as e:
                print(f"âš ï¸  è®°å½•ç±»åˆ«ä¿¡æ¯åˆ° MLflow å¤±è´¥: {e}")
    
    # è‡ªåŠ¨åŠ è½½ä¹‹å‰çš„æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ä¸”å¯ç”¨äº†auto_resumeï¼‰
    resume_from_epoch = 0
    resume_best_metric = None
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®š load_modelï¼Œä¸”å¯ç”¨äº† auto_resumeï¼Œå°è¯•è‡ªåŠ¨åŠ è½½
    if load_model is None and auto_resume:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¹‹å‰çš„ best æ¨¡å‹
        auto_load_path = model_save_dir / 'best.pth'
        if auto_load_path.exists():
            if is_main_process():
                print(f"ğŸ” å‘ç°å·²å­˜åœ¨çš„æ¨¡å‹: {auto_load_path}")
                print(f"   è‡ªåŠ¨åŠ è½½ä»¥ç»§ç»­è®­ç»ƒ...")
            load_model = str(auto_load_path)
    
    # ä»checkpointæ¢å¤epochä¿¡æ¯å’Œbest_metric (åªåœ¨ä¸»è¿›ç¨‹æ‰“å°)
    if load_model is not None:
        if is_main_process():
            print(f"\n{'='*80}")
            print(f"ğŸ“¦ ä» checkpoint åŠ è½½æ¨¡å‹")
            print(f"{'='*80}")
            print(f"æ¨¡å‹è·¯å¾„: {load_model}")
        
        # å…ˆåŠ è½½çŠ¶æ€å­—å…¸ä»¥è·å–epochä¿¡æ¯
        state_dict = torch.load(load_model, map_location='cpu')
        
        if is_main_process():
            print(f"\nğŸ“‹ Checkpoint ä¿¡æ¯:")
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«epochä¿¡æ¯
        if isinstance(state_dict, dict) and 'epoch' in state_dict:
            # +1 å› ä¸ºä¿å­˜çš„æ˜¯å®Œæˆçš„epochï¼Œä¸‹ä¸€æ¬¡è®­ç»ƒä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
            resume_from_epoch = state_dict['epoch'] + 1
            if is_main_process():
                print(f"  - å·²å®Œæˆçš„ epoch: {state_dict['epoch']}")
                print(f"  - ä¸‹æ¬¡è®­ç»ƒèµ·å§‹ epoch: {resume_from_epoch}")
        else:
            if is_main_process():
                print(f"  - Epoch ä¿¡æ¯: âš ï¸  æœªæ‰¾åˆ°")
                print(f"  - å°†ä» epoch 0 å¼€å§‹è®¡ç®—å­¦ä¹ ç‡")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«lossä¿¡æ¯
        if isinstance(state_dict, dict) and 'loss' in state_dict:
            resume_best_metric = state_dict['loss']
            if is_main_process():
                print(f"  - å½“å‰ valid_loss: {resume_best_metric:.6f}")
        else:
            if is_main_process():
                print(f"  - æœ€ä½³æŒ‡æ ‡: âš ï¸  æœªæ‰¾åˆ°")
        
        # æ£€æŸ¥å…¶ä»–ä¿¡æ¯
        if is_main_process():
            if isinstance(state_dict, dict):
                if 'img_size' in state_dict:
                    print(f"  - å›¾åƒå°ºå¯¸: {state_dict['img_size']}")
                if 'arch' in state_dict:
                    print(f"  - æ¨¡å‹æ¶æ„: {state_dict['arch']}")
                if 'opt' in state_dict:
                    print(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: âœ… å·²ä¿å­˜")
                else:
                    print(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: âš ï¸  æœªæ‰¾åˆ°")
            
            print(f"{'='*80}\n")
    else:
        if is_main_process():
            print("ğŸ†•  ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹")
    
    # åˆ›å»ºlearnerï¼ˆåœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼Œå› ä¸ºéœ€è¦resume_from_epochä¿¡æ¯ï¼‰
    # æ„å»ºcallbacksåˆ—è¡¨
    callbacks = [
        YOLOv11LRScheduler(epochs=epochs, lr0=lr0, lrf=lrf, warmup_epochs=3, 
                          resume_from_epoch=resume_from_epoch, min_lr=min_lr, 
                          scheduler_type=scheduler_type),
        ResumeEpochCallback(resume_from_epoch=resume_from_epoch),
    ]
    
    # åˆ†å‰²ä»»åŠ¡ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
    if not is_segmentation:
        # å¦‚æœ grad_acc <= 0ï¼Œè‡ªåŠ¨è®¡ç®—
        if grad_acc <= 0:
            n_gpus = torch.cuda.device_count() if distributed else 1
            grad_acc = calculate_grad_acc(batch_size, n_gpus, target_batch=256)
            
            # è®°å½•è‡ªåŠ¨è®¡ç®—çš„ grad_acc åˆ° MLflow
            if mlflow_run is not None and is_main_process():
                try:
                    effective_batch = batch_size * n_gpus * grad_acc
                    mlflow.log_params({
                        'grad_acc_auto': grad_acc,
                        'effective_batch_size': effective_batch
                    })
                except Exception as e:
                    print(f"âš ï¸  è®°å½• grad_acc åˆ° MLflow å¤±è´¥: {e}")
        
        # ğŸ”§ ä¿®å¤ï¼šå¤šGPUè®­ç»ƒæ—¶ç¦ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œé¿å…lossåŒé‡ç¼©æ”¾é—®é¢˜
        # é—®é¢˜ï¼šFastAIçš„GradientAccumulationä¼šå°†lossé™¤ä»¥n_accï¼Œä½†DDPå·²ç»å¹³å‡äº†loss
        # ç»“æœï¼šlossè¢«é”™è¯¯åœ°åŒé‡ç¼©æ”¾ï¼Œå¯¼è‡´æ¢¯åº¦è¿‡å°ï¼Œæ¨¡å‹ä¸æ”¶æ•›
        if distributed and grad_acc > 1:
            if is_main_process():
                print(f"\n{'='*80}")
                print(f"âš ï¸  æ£€æµ‹åˆ°å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ + æ¢¯åº¦ç´¯ç§¯ï¼Œè¿™ä¼šå¯¼è‡´lossè®¡ç®—å†²çªï¼")
                print(f"{'='*80}")
                print(f"é—®é¢˜è¯´æ˜ï¼š")
                print(f"  - FastAIçš„GradientAccumulationä¼šç¼©æ”¾loss: loss /= {grad_acc}")
                print(f"  - ä½†DDPå·²ç»è‡ªåŠ¨åœ¨æ‰€æœ‰GPUé—´å¹³å‡äº†loss")
                print(f"  - åŒé‡ç¼©æ”¾å¯¼è‡´æ¢¯åº¦è¿‡å°ï¼Œæ¨¡å‹å‡ ä¹ä¸å­¦ä¹ ")
                print(f"è§£å†³æ–¹æ¡ˆï¼š")
                print(f"  - åŸgrad_acc={grad_acc} â†’ å¼ºåˆ¶è®¾ä¸º1ï¼ˆç¦ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰")
                print(f"  - æœ‰æ•ˆbatch_size={batch_size} x {n_gpus} GPUs = {batch_size * n_gpus}")
                print(f"  - å¦‚éœ€æ›´å¤§batchï¼Œè¯·å¢å¤§ --batch_size å‚æ•°")
                print(f"{'='*80}\n")
            grad_acc = 1
        
        # åªåœ¨grad_acc > 1æ—¶æ·»åŠ æ¢¯åº¦ç´¯ç§¯å›è°ƒ
        if grad_acc > 1:
            callbacks.insert(0, GradientAccumulation(n_acc=grad_acc))
    
    callbacks.append(EarlyStoppingWithEvalCallback(
        monitor='valid_loss', 
        patience=early_stopping, 
        resume_best_metric=resume_best_metric
    ))
    
    # # åˆ†å¸ƒå¼è®­ç»ƒæ—¶æ·»åŠ éªŒè¯è¯Šæ–­å›è°ƒï¼ˆå¸®åŠ©å‘ç°éªŒè¯lossé—®é¢˜ï¼‰
    # if distributed:
    #     callbacks.append(DistributedValidationDiagnosticCallback(verbose=True))
    #     if is_main_process():
    #         print("ğŸ“Š å·²å¯ç”¨åˆ†å¸ƒå¼éªŒè¯è¯Šæ–­å›è°ƒï¼Œå°†åœ¨æ¯ä¸ªepochåæŠ¥å‘ŠGPUé—´losså·®å¼‚")
    
    # åªåœ¨ä¸»è¿›ç¨‹æ·»åŠ  MLflow å’Œæ¨¡å‹ä¿å­˜å›è°ƒ
    if is_main_process():
        callbacks.extend([
            SaveModelWithEpochCallback(monitor='valid_loss', fname='best', last_fname=model_path,
                                      with_opt=True, resume_from_epoch=resume_from_epoch,
                                      img_size=img_size, arch=arch, 
                                      resume_best_metric=resume_best_metric,
                                      save_dir=model_save_dir, save_last=True,
                                      upload_to_mlflow=(mlflow_run is not None and not skip_mlflow_model_upload)),
            MLflowMetricsCallback(resume_from_epoch=resume_from_epoch) if mlflow_run is not None else None
        ])
        callbacks = [cb for cb in callbacks if cb is not None]
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»º Learner
    if is_segmentation:
        # åˆ†å‰²ä»»åŠ¡
        if is_main_process():
            print(f"âœ… ä½¿ç”¨åˆ†å‰²æ¨¡å‹: {arch}")
        
        # åˆ†å‰²æ¨¡å‹å¿…é¡»æ˜¯è‡ªå®šä¹‰æ¨¡å‹
        if not (CUSTOM_MODELS_AVAILABLE and is_custom_model(arch)):
            raise ValueError(f"åˆ†å‰²æ¨¡å‹ '{arch}' æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ 'unet_seg' ç­‰åˆ†å‰²æ¨¡å‹")
        
        model = get_model(arch, n_classes=1)  # äºŒåˆ†ç±»åˆ†å‰²
        
        # è¾“å‡ºæ¨¡å‹ç»“æ„
        if is_main_process():
            input_size = (1, 3, img_size, img_size)
            print_model_structure(model, model_name=arch, input_size=input_size)
        
        learn = Learner(
            dls,
            model,
            loss_func=CombinedLoss(bce_weight=0.5, dice_weight=0.5),
            opt_func=opt_func,
            metrics=[DiceMetric()],
            wd=wd,
            cbs=callbacks
        )
        
        # æ˜¾ç¤º GPU ä¿¡æ¯
        if is_main_process():
            if torch.cuda.is_available():
                print(f"ğŸ“ è®­ç»ƒå°†ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
            else:
                print(f"âš ï¸  è­¦å‘Š: å°†ä½¿ç”¨ CPU è®­ç»ƒ")
        
    else:
        # åˆ†ç±»ä»»åŠ¡
        if CUSTOM_MODELS_AVAILABLE and is_custom_model(arch):
            # ä½¿ç”¨è‡ªå®šä¹‰åˆ†ç±»æ¨¡å‹
            if is_main_process():
                print(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {arch}")
            
            custom_model = get_model(arch, n_classes=len(dls.vocab))

            # è¾“å‡ºæ¨¡å‹ç»“æ„
            if is_main_process():
                input_size = (1, 3, img_size, img_size)
                print_model_structure(custom_model, model_name=arch, input_size=input_size)
             
            learn = Learner(
                dls,
                custom_model,
                opt_func=opt_func,
                metrics=[
                    accuracy,
                    error_rate,
                    Precision(average='weighted'),
                    Recall(average='weighted'),
                    F1Score(average='weighted'),
                ],
                wd=wd,
                cbs=callbacks
            )
        else:
            # ä½¿ç”¨timmæ¨¡å‹ï¼ˆé€šè¿‡vision_learneråŒ…è£…ï¼‰
            try:
                import timm
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºtimmæ¨¡å‹
                if arch not in timm.list_models():
                    print(f"\nâŒ é”™è¯¯: '{arch}' ä¸æ˜¯æœ‰æ•ˆçš„æ¨¡å‹")
                    print(f"\nğŸ’¡ æç¤º:")
                    print(f"   - ä½¿ç”¨ --list-models æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹")
                    print(f"   - Timmæ¨¡å‹: resnet18, efficientnet_b0, vit_base_patch16_224 ç­‰")
                    print(f"   - è‡ªå®šä¹‰æ¨¡å‹: yolov11s_cls, unet_seg ç­‰")
                    sys.exit(1)
                
                if is_main_process():
                    print(f"âœ… ä½¿ç”¨Timmæ¨¡å‹ï¼ˆvision_learneråŒ…è£…ï¼‰: {arch}")
                
                # æ£€æµ‹æ˜¯å¦ä¸º ConvNeXt æ¨¡å‹ï¼Œè‡ªåŠ¨æç¤º DropPath é…ç½®
                model_kwargs = {}
                if 'convnext' in arch.lower() and drop_path_rate > 0:
                    model_kwargs['drop_path_rate'] = drop_path_rate
                    if is_main_process():
                        print(f"   âœ“ å¯ç”¨ DropPath æ­£åˆ™åŒ–: drop_path_rate={drop_path_rate}")
                        print(f"   æ¨èå€¼: Tiny=0.1, Small=0.2, Base=0.3, Large=0.4")
                elif 'convnext' in arch.lower() and drop_path_rate == 0:
                    if is_main_process():
                        print(f"   âš ï¸  è­¦å‘Š: ConvNeXt æ¨¡å‹æœªå¯ç”¨ DropPath (drop_path_rate=0)")
                        print(f"   è¿™å¯èƒ½å¯¼è‡´è®­ç»ƒæ•ˆæœå˜å·®ï¼Œå»ºè®®è®¾ç½® --drop_path_rate 0.1")
                
                # ä½¿ç”¨vision_learneråˆ›å»ºæ¨¡å‹ï¼ˆFastAIä¼šè‡ªåŠ¨å¤„ç†timmæ¨¡å‹ï¼‰
                # å¼ºåˆ¶ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶ä¼ é€’ drop_path_rate ç­‰å‚æ•°
                learn = vision_learner(
                    dls, 
                    arch=arch,
                    pretrained=False,
                    metrics=[
                        accuracy,
                        error_rate,
                        Precision(average='weighted'),  
                        Recall(average='weighted'),    
                        F1Score(average='weighted'),   
                    ],
                    opt_func=opt_func,
                    cbs=callbacks,
                    **model_kwargs  # ä¼ é€’é¢å¤–å‚æ•°ï¼ˆå¦‚ drop_path_rateï¼‰
                )
                
                # è¾“å‡ºæ¨¡å‹ç»“æ„
                if is_main_process():
                    input_size = (1, 3, img_size, img_size)
                    print_model_structure(learn.model, model_name=arch, input_size=input_size)
                
            except ImportError:
                print(f"\nâŒ é”™è¯¯: æœªå®‰è£…timmåº“")
                print(f"\nå®‰è£…å‘½ä»¤:")
                print(f"   pip install timm")
                sys.exit(1)

    # åŠ è½½æ¨¡å‹æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€ (åªåœ¨ä¸»è¿›ç¨‹æ‰“å°è¯¦ç»†ä¿¡æ¯)
    optimizer_state_to_load = None  # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œç¨ååŠ è½½
    if load_model is not None:
        # è·å–æ¨¡å‹è®¾å¤‡
        device = next(learn.model.parameters()).device
        
        # é‡æ–°åŠ è½½åˆ°æ­£ç¡®çš„è®¾å¤‡
        state_dict = torch.load(load_model, map_location=device)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ FastAI ä¿å­˜çš„æ ¼å¼ï¼ˆåŒ…å« 'model' å’Œ 'opt' é”®ï¼‰
        if isinstance(state_dict, dict) and 'model' in state_dict:
            if is_main_process():
                print("åŠ è½½æ¨¡å‹æƒé‡...")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            # FastAIä¼šè‡ªåŠ¨å¤„ç†DDPæ¨¡å‹çš„'module.'å‰ç¼€é—®é¢˜
            model_state = state_dict['model']
            learn.model.load_state_dict(model_state)
            
            # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œç¨ååœ¨ä¼˜åŒ–å™¨åˆå§‹åŒ–ååŠ è½½
            if 'opt' in state_dict and state_dict['opt'] is not None:
                optimizer_state_to_load = state_dict['opt']
                if is_main_process():
                    print("ğŸ’¾ ä¼˜åŒ–å™¨çŠ¶æ€å·²å‡†å¤‡ï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹åæ¢å¤")
                    
                    # æ˜¾ç¤ºä¿å­˜çš„ä¼˜åŒ–å™¨å‚æ•°
                    try:
                        if 'param_groups' in optimizer_state_to_load and len(optimizer_state_to_load['param_groups']) > 0:
                            saved_pg = optimizer_state_to_load['param_groups'][0]
                            print("ğŸ“‹ checkpointä¸­ä¿å­˜çš„ä¼˜åŒ–å™¨å‚æ•°:")
                            print(f"   - å­¦ä¹ ç‡ (lr): {saved_pg.get('lr', 'N/A')}")
                    except Exception as e:
                        print(f"   æ— æ³•æ˜¾ç¤ºä¿å­˜çš„ä¼˜åŒ–å™¨å‚æ•°: {e}")
            else:
                if is_main_process():
                    print("âš ï¸  checkpointä¸­æ²¡æœ‰ä¼˜åŒ–å™¨çŠ¶æ€")
            
            # æ˜¾ç¤ºå­¦ä¹ ç‡è°ƒåº¦ä¿¡æ¯
            if is_main_process():
                print(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦ä¿¡æ¯:")
                print(f"   - ä»epoch {resume_from_epoch}ç»§ç»­è®­ç»ƒ")
                if resume_from_epoch < 3:
                    print(f"   - å½“å‰ä»åœ¨çƒ­èº«é˜¶æ®µ (warmup_epochs=3)")
                else:
                    print(f"   - å·²è¿‡çƒ­èº«é˜¶æ®µï¼Œä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦")
        else:
            # ç›´æ¥åŠ è½½æƒé‡ï¼ˆçº¯æ¨¡å‹æƒé‡ï¼Œæ— ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
            learn.model.load_state_dict(state_dict)
            if is_main_process():
                print("âš ï¸  åŠ è½½çº¯æ¨¡å‹æƒé‡ï¼ˆæ— ä¼˜åŒ–å™¨çŠ¶æ€å’Œepochä¿¡æ¯ï¼‰")
        
        if is_main_process():
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
    
    # å¦‚æœæœ‰ä¼˜åŒ–å™¨çŠ¶æ€éœ€è¦åŠ è½½ï¼Œæ·»åŠ å¯¹åº”çš„callback
    # æ”¯æŒè¦†ç›–è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†æ–°çš„ wdï¼‰
    if optimizer_state_to_load is not None:
        # å‡†å¤‡è¦è¦†ç›–çš„è¶…å‚æ•°ï¼ˆå¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†ä¸åŒçš„å€¼ï¼‰
        override_hypers = {}
        
        # æ£€æŸ¥ checkpoint ä¸­çš„ wd å’Œå½“å‰å‘½ä»¤è¡ŒæŒ‡å®šçš„ wd æ˜¯å¦ä¸åŒ
        if 'param_groups' in optimizer_state_to_load and len(optimizer_state_to_load['param_groups']) > 0:
            checkpoint_wd = optimizer_state_to_load['param_groups'][0].get('wd', 
                           optimizer_state_to_load['param_groups'][0].get('weight_decay', None))
            
            if checkpoint_wd is not None and abs(checkpoint_wd - wd) > 1e-6:
                if is_main_process():
                    print(f"\nâš ï¸  æ£€æµ‹åˆ°æƒé‡è¡°å‡ä¸ä¸€è‡´:")
                    print(f"   Checkpoint ä¸­: {checkpoint_wd}")
                    print(f"   å‘½ä»¤è¡ŒæŒ‡å®š: {wd}")
                    print(f"   å°†ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„å€¼: {wd}")
                override_hypers['wd'] = wd
        
        learn.add_cb(LoadOptimizerStateCallback(optimizer_state_to_load, override_hypers=override_hypers))

    if not only_val:
        try:
            # é…ç½®DDPå‚æ•°ä»¥æ”¯æŒæœ‰æœªä½¿ç”¨å‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚ConvNeXtç­‰ï¼‰
            # æ³¨æ„ï¼šConvNeXtç­‰æ¨¡å‹çš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯ä¼šäº§ç”Ÿéè¿ç»­æ¢¯åº¦å¸ƒå±€ï¼Œ
            # ç¡®ä¿DDPä½¿ç”¨æ‹·è´æ¨¡å¼è€Œéè§†å›¾æ¨¡å¼
            gradient_as_bucket_view=False 
            if distributed:
                ddp_kwargs = DistributedDataParallelKwargs(
                    find_unused_parameters=True,
                    gradient_as_bucket_view=False  # é¿å…æ­¥å¹…ä¸åŒ¹é…ï¼ˆConvNeXtæ·±åº¦å·ç§¯ï¼‰
                )
                if is_main_process():
                    print("ğŸ”§ å·²é…ç½®DDP: find_unused_parameters=True, gradient_as_bucket_view=False")
            else:
                ddp_kwargs = None
            
            # ä¼ é€’kwargs_handlersåˆ°distrib_ctx
            ctx_kwargs = {'kwargs_handlers': [ddp_kwargs]} if ddp_kwargs else {}
            
            with learn.distrib_ctx(**ctx_kwargs):
                learn.fit(epochs, lr=lr0, wd=wd)
            
        except CancelFitException:
            # æ—©åœè§¦å‘ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼Œä¸æ˜¯é”™è¯¯
            if is_main_process():
                print("\nâœ… è®­ç»ƒå› æ—©åœè€Œç»“æŸ")
        
        except Exception as e:
            print("é”™è¯¯:", file=sys.stderr)
            traceback.print_exc()
            raise
        
        finally:
            # è®­ç»ƒå®Œæˆåçš„æ¸…ç†
            # FastAIçš„to_parallel()ä¼šè‡ªåŠ¨ç®¡ç†DDPç”Ÿå‘½å‘¨æœŸï¼Œæ— éœ€æ‰‹åŠ¨æ¸…ç†
            if is_main_process():
                print("\nâœ… è®­ç»ƒå®Œæˆ")
            
            # éä¸»è¿›ç¨‹åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹é€€å‡ºï¼ˆä¸å‚ä¸è¯„ä¼°ï¼‰
            if distributed and not is_main_process():
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                return
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨è¯„ä¼°å‰å…ˆæ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    # é—®é¢˜ï¼šè¯„ä¼°æ—¶get_preds()åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ä¼šè§¦å‘è¿›ç¨‹åŒæ­¥ï¼Œå¯¼è‡´OOMå’ŒSIGABRT
    # è§£å†³ï¼šä¸»è¿›ç¨‹åœ¨è¯„ä¼°å‰æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒï¼Œæ”¹ä¸ºå•GPUæ¨¡å¼è¯„ä¼°
    if distributed and is_main_process():
        print("\nğŸ”§ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆå‡†å¤‡å•GPUè¯„ä¼°ï¼‰...")
        
        # 1. ç§»é™¤learnerä¸­çš„åˆ†å¸ƒå¼callbacks
        from fastai.distributed import DistributedTrainer, GatherPredsCallback
        callbacks_to_remove = []
        for cb in learn.cbs:
            if isinstance(cb, (DistributedTrainer, GatherPredsCallback)):
                callbacks_to_remove.append(cb)
                print(f"   - ç§»é™¤callback: {cb.__class__.__name__}")
        
        for cb in callbacks_to_remove:
            learn.remove_cb(cb)
        
        # 2. å¦‚æœæ¨¡å‹è¢«DDPåŒ…è£…ï¼Œæå–åŸå§‹æ¨¡å‹
        if hasattr(learn.model, 'module'):
            print(f"   - æå–DDPåŒ…è£…çš„åŸå§‹æ¨¡å‹")
            learn.model = learn.model.module
        
        # 3. æ¸…ç†PyTorchåˆ†å¸ƒå¼ç¯å¢ƒ
        if torch.distributed.is_initialized():
            print(f"   - é”€æ¯åˆ†å¸ƒå¼è¿›ç¨‹ç»„")
            torch.distributed.destroy_process_group()
        
        # 4. æ¸…ç†ç¯å¢ƒå˜é‡
        distributed_env_vars = [
            'RANK', 'LOCAL_RANK', 'WORLD_SIZE', 
            'MASTER_ADDR', 'MASTER_PORT',
            'TORCH_DISTRIBUTED_DEBUG'
        ]
        for var in distributed_env_vars:
            if var in os.environ:
                del os.environ[var]
        
        print(f"   âœ… åˆ†å¸ƒå¼ç¯å¢ƒå·²æ¸…ç†ï¼Œç°åœ¨ä»¥å•GPUæ¨¡å¼ç»§ç»­")
    
    # ä»¥ä¸‹ä»£ç åªæœ‰ä¸»è¿›ç¨‹ä¼šæ‰§è¡Œï¼ˆæˆ–å•GPUè®­ç»ƒï¼‰
    if is_main_process():
        # æ³¨æ„ï¼šbestæŒ‡æ ‡å·²åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶ä¸ŠæŠ¥åˆ°MLflowï¼ˆæ¯æ¬¡æ›´æ–°bestæ—¶ï¼‰
        # æ— éœ€åœ¨è®­ç»ƒç»“æŸåå†æ¬¡ä¸ŠæŠ¥
        
        # ä¸Šä¼ æ¨¡å‹åˆ° MLflow
        if mlflow_run is not None and not skip_mlflow_model_upload:
            try:
                
                # best æ¨¡å‹å·²åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶ä¸Šä¼ ï¼Œè¿™é‡Œåªéœ€è¦è°ƒç”¨è¯„ä¼°
                best_model_path = model_save_dir / 'best.pth'
                if best_model_path.exists():
                    print(f"â„¹ï¸  æœ€ä½³æ¨¡å‹å·²åœ¨è®­ç»ƒä¸­ä¸Šä¼ åˆ° MLflow: best.pth")
                    
                    # åˆ†å‰²ä»»åŠ¡ä¸éœ€è¦æœ€ç»ˆè¯„ä¼°
                    if not is_segmentation:
                        # è°ƒç”¨ç‹¬ç«‹çš„è¯„ä¼°æ¨¡å—ï¼ˆå¤ç”¨è®­ç»ƒæ•°æ®ï¼‰
                        call_evaluation_script(
                            learn=learn,
                            model_path=best_model_path,
                            mlflow_run_id=mlflow_run.info.run_id,
                            project_name=project_name
                        )
                    else:
                        print(f"â„¹ï¸  åˆ†å‰²ä»»åŠ¡è·³è¿‡æœ€ç»ˆè¯„ä¼°")
                    
                    # å¯¼å‡ºONNXæ¨¡å‹
                    if export_onnx:
                        try:
                            print(f"\n{'='*80}")
                            print(f"ğŸ“¦ å¯¼å‡ºONNXæ¨¡å‹")
                            print(f"{'='*80}")
                            
                            from export_onnx import export_to_onnx
                            
                            onnx_path = model_save_dir / 'best.onnx'
                            export_to_onnx(
                                model_path=str(best_model_path),
                                arch=arch,
                                output_path=str(onnx_path),
                                img_size=img_size,
                                device='cpu',  # ONNXå¯¼å‡ºåœ¨CPUä¸Šè¿›è¡Œ
                                data_path=str(data_path),
                                classes=None  # ä»æ•°æ®è·¯å¾„è‡ªåŠ¨è·å–
                            )
                            
                            # ä¸Šä¼ ONNXæ¨¡å‹åˆ°MLflow
                            if mlflow_run is not None and onnx_path.exists():
                                try:
                                    mlflow.log_artifact(str(onnx_path), artifact_path="models")
                                    print(f"âœ… ONNXæ¨¡å‹å·²ä¸Šä¼ åˆ° MLflow")
                                except Exception as e:
                                    print(f"âš ï¸  ä¸Šä¼ ONNXåˆ° MLflow å¤±è´¥: {e}")
                            
                            print(f"{'='*80}\n")
                        except Exception as e:
                            print(f"âš ï¸  å¯¼å‡ºONNXå¤±è´¥: {e}")
                            import traceback as tb
                            tb.print_exc()
                    else:
                        print(f"â„¹ï¸  è·³è¿‡ONNXå¯¼å‡ºï¼ˆ--no-export-onnxï¼‰")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°bestæ¨¡å‹ï¼Œè·³è¿‡è¯„ä¼°: {best_model_path}")
            except Exception as e:
                print(f"âš ï¸  ä¸Šä¼ æ¨¡å‹åˆ° MLflow å¤±è´¥: {e}")
        elif skip_mlflow_model_upload:
            print(f"â„¹ï¸  è°ƒä¼˜æ¨¡å¼ï¼šè·³è¿‡æ¨¡å‹ä¸Šä¼ åˆ° MLflow")
        
        # å®Œæˆ MLflow run
        if mlflow_run is not None:
            mlflow.end_run()
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")




def list_available_models(series_filter=None):
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ï¼ˆTimmã€è‡ªå®šä¹‰æ¨¡å‹ï¼‰
    
    Args:
        series_filter: å¯é€‰ï¼ŒæŒ‡å®šæ¨¡å‹ç³»åˆ—åç§°ï¼ˆå¦‚ 'resnet', 'efficientnet'ç­‰ï¼‰
                      å¦‚æœæŒ‡å®šï¼Œåˆ™æ˜¾ç¤ºè¯¥ç³»åˆ—ä¸‹çš„æ‰€æœ‰æ¨¡å‹ï¼›å¦åˆ™æ˜¾ç¤ºæ‰€æœ‰ç³»åˆ—æ‘˜è¦
    """
    print("\n" + "="*80)
    if series_filter:
        print(f"ğŸ“‹ æ¨¡å‹ç³»åˆ—: {series_filter}")
    else:
        print("ğŸ“‹ å¯ç”¨æ¨¡å‹ç›®å½•")
    print("="*80)
    
    # 1. Timm æ¨¡å‹ï¼ˆåŠ¨æ€è·å–ï¼‰
    timm_series = {}
    try:
        import timm
        timm_models = timm.list_models()
        
        # è‡ªåŠ¨åˆ†ç±»æ‰€æœ‰timmæ¨¡å‹ï¼ˆæŒ‰å‰ç¼€ï¼‰
        from collections import defaultdict
        timm_by_prefix = defaultdict(list)
        for model in sorted(timm_models):
            # æå–å‰ç¼€ï¼ˆç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿æˆ–æ•°å­—å‰çš„éƒ¨åˆ†ï¼‰
            import re
            match = re.match(r'^([a-z]+)', model)
            if match:
                prefix = match.group(1)
                timm_by_prefix[prefix].append(model)
        
        timm_series = dict(timm_by_prefix)
    except ImportError:
        pass
    
    # å¦‚æœæŒ‡å®šäº†ç³»åˆ—è¿‡æ»¤å™¨ï¼Œæ˜¾ç¤ºè¯¥ç³»åˆ—çš„æ‰€æœ‰æ¨¡å‹
    if series_filter:
        series_lower = series_filter.lower()
        found = False
        
        # æœç´¢ Timm æ¨¡å‹
        if series_lower in timm_series:
            models = timm_series[series_lower]
            print(f"\n1ï¸âƒ£  Timm - {series_filter} ({len(models)} ä¸ªæ¨¡å‹):")
            for i, model in enumerate(models, 1):
                print(f"   {i:3d}. {model}")
            found = True
        
        # æœç´¢è‡ªå®šä¹‰æ¨¡å‹
        if CUSTOM_MODELS_AVAILABLE:
            custom_models = list_models()
            matched = [m for m in custom_models if m.lower().startswith(series_lower)]
            if matched:
                print(f"\n2ï¸âƒ£  è‡ªå®šä¹‰æ¨¡å‹ - {series_filter} ({len(matched)} ä¸ªæ¨¡å‹):")
                for i, model in enumerate(matched, 1):
                    model_type = "åˆ†å‰²" if model.endswith('_seg') else "åˆ†ç±»"
                    print(f"   {i:2d}. {model:30s} [{model_type}]")
                found = True
        
        if not found:
            print(f"\nâš ï¸  æœªæ‰¾åˆ°ç³»åˆ— '{series_filter}' çš„æ¨¡å‹")
            print(f"\nğŸ’¡ æŸ¥çœ‹æ‰€æœ‰ç³»åˆ—: python train.py --list-models")
        
    else:
        # æ˜¾ç¤ºæ‰€æœ‰ç³»åˆ—æ‘˜è¦
        if timm_series:
            print(f"\n1ï¸âƒ£  Timm æ¨¡å‹åº“ (å…± {len(timm_series)} ä¸ªç³»åˆ—):")
            # åªæ˜¾ç¤ºæ¨¡å‹æ•°é‡ >= 3 çš„ä¸»è¦ç³»åˆ—
            major_series = {k: v for k, v in timm_series.items() if len(v) >= 3}
            for series_name in sorted(major_series.keys()):
                count = len(major_series[series_name])
                print(f"   â€¢ {series_name:15s} ({count:3d} ä¸ªæ¨¡å‹)")
            
            minor_count = len(timm_series) - len(major_series)
            if minor_count > 0:
                print(f"   ... ä»¥åŠ {minor_count} ä¸ªå…¶ä»–å°ç³»åˆ—")
        else:
            print(f"\n1ï¸âƒ£  Timm æ¨¡å‹åº“: âš ï¸  æœªå®‰è£… (pip install timm)")
        
        if CUSTOM_MODELS_AVAILABLE:
            custom_models = list_models()
            classification_models = [m for m in custom_models if not m.endswith('_seg')]
            segmentation_models = [m for m in custom_models if m.endswith('_seg')]
            
            print(f"\n2ï¸âƒ£  è‡ªå®šä¹‰æ¨¡å‹:")
            if classification_models:
                print(f"   â€¢ åˆ†ç±»æ¨¡å‹ ({len(classification_models)} ä¸ª):")
                for i, model in enumerate(classification_models, 1):
                    print(f"     {i}. {model}")
            if segmentation_models:
                print(f"   â€¢ åˆ†å‰²æ¨¡å‹ ({len(segmentation_models)} ä¸ª):")
                for i, model in enumerate(segmentation_models, 1):
                    print(f"     {i}. {model}")
        else:
            print("\n2ï¸âƒ£  è‡ªå®šä¹‰æ¨¡å‹: âš ï¸  æœªåŠ è½½")
        
        print("\n" + "="*80)
        print("\nğŸ’¡ æŸ¥çœ‹ç‰¹å®šç³»åˆ—çš„æ‰€æœ‰æ¨¡å‹:")
        print("   python train.py --list-models resnet")
        print("   python train.py --list-models efficientnet")
        print("   python train.py --list-models vit")
        print("   python train.py --list-models yolo  # æŸ¥çœ‹è‡ªå®šä¹‰YOLOæ¨¡å‹")
    
    print("\n" + "="*80)
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print("   Timmæ¨¡å‹:   python train.py --arch resnet18 --data_path <æ•°æ®é›†è·¯å¾„>")
    print("   è‡ªå®šä¹‰æ¨¡å‹: python train.py --arch yolov11s_cls --data_path <æ•°æ®é›†è·¯å¾„>")
    print("\nğŸ“ æ•°æ®é›†æ ¼å¼:")
    print("   åˆ†ç±»: data_path/train/class1/, data_path/val/class1/...")
    print("   åˆ†å‰²: data_path/imgs/train/, data_path/masks/train/...")
    print()

def print_structure_command(arch, img_size=224, show_shape=True):
    """
    ä¸“é—¨ç”¨äºæ‰“å°æ¨¡å‹ç»“æ„çš„å‘½ä»¤å‡½æ•°
    
    Args:
        arch: æ¨¡å‹æ¶æ„åç§°
        img_size: è¾“å…¥å›¾åƒå°ºå¯¸
        show_shape: æ˜¯å¦æ˜¾ç¤ºshapeä¿¡æ¯
    """
    print("\n" + "="*80)
    print("ğŸ“Š æ‰“å°æ¨¡å‹ç»“æ„")
    print("="*80)
    print(f"\næ¨¡å‹: {arch}")
    print(f"è¾“å…¥å°ºå¯¸: {img_size}x{img_size}")
    print(f"æ˜¾ç¤ºShape: {'æ˜¯' if show_shape else 'å¦'}")
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå®šä¹‰æ¨¡å‹
        if CUSTOM_MODELS_AVAILABLE and is_custom_model(arch):
            print(f"\nâœ… ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {arch}")
            # ä½¿ç”¨é»˜è®¤ç±»åˆ«æ•°ï¼ˆç”¨äºæ˜¾ç¤ºç»“æ„ï¼‰
            model = get_model(arch, n_classes=10)
        else:
            # ä½¿ç”¨timmæ¨¡å‹
            import timm
            if arch in timm.list_models():
                print(f"\nâœ… ä½¿ç”¨Timmæ¨¡å‹: {arch}")
                model = timm.create_model(arch, pretrained=False, num_classes=10)
            else:
                print(f"\nâŒ æœªæ‰¾åˆ°æ¨¡å‹: {arch}")
                print("\nğŸ’¡ ä½¿ç”¨ --list-models æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹")
                return
        
        # æ‰“å°ç»“æ„
        input_size = (1, 3, img_size, img_size)
        print_model_structure(model, model_name=arch, input_size=input_size, show_shape=show_shape)
        
    except Exception as e:
        print(f"\nâŒ æ‰“å°æ¨¡å‹ç»“æ„å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ’¡ æç¤º:")
    print("   - ä½¿ç”¨ --arch æŒ‡å®šä¸åŒçš„æ¨¡å‹")
    print("   - ä½¿ç”¨ --img_size æŒ‡å®šä¸åŒçš„è¾“å…¥å°ºå¯¸")
    print("   - ä½¿ç”¨ --no-shape å…³é—­shapeæ˜¾ç¤ºï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰")
    print()


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒåŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹')
    
    # æ•°æ®å’Œæ¨¡å‹è·¯å¾„
    parser.add_argument('--data_path', type=str, default='/mnt/ssd/dataset', help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--model_path', type=str, default='last', help='ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰')
    parser.add_argument('--load_model', type=str, default=None, help='åŠ è½½å·²æœ‰çš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆå®Œæ•´è·¯å¾„ï¼‰')
    parser.add_argument('--auto_resume', action='store_true', default=True, 
                       help='è‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„bestæ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-auto-resume', dest='auto_resume', action='store_false', 
                       help='ç¦ç”¨è‡ªåŠ¨æ¢å¤ï¼Œå¼ºåˆ¶ä»å¤´è®­ç»ƒ')
    parser.add_argument('--models_base_dir', type=str, default='runs', 
                       help='ç»Ÿä¸€çš„æ¨¡å‹ä¿å­˜åŸºç¡€ç›®å½•ï¼Œå®é™…ä¿å­˜è·¯å¾„ä¸º: models_base_dir/project_name/task_name/')
    
    # æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
    parser.add_argument('--img_size', type=int, default=224, help='è¾“å…¥å›¾åƒå¤§å°')
    parser.add_argument('--batch_size', type=int, default=256, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--epochs', type=int, default=300, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr0', type=float, default=0.01, help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--lrf', type=float, default=0.1, help='æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹ï¼ˆç›¸å¯¹äºlrï¼‰ï¼Œæ¨è0.1-0.2')
    parser.add_argument('--arch', type=str, default='resnet18', help='æ¨¡å‹æ¶æ„')
    parser.add_argument('--wd', type=float, default=1e-3, help='æƒé‡è¡°å‡')
    parser.add_argument('--early_stopping', type=int, default=100, help='æ—©åœè½®æ•°')
    parser.add_argument('--grad_acc', type=int, default=0, 
                       help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (0=è‡ªåŠ¨è®¡ç®—, >0=æ‰‹åŠ¨æŒ‡å®š, é»˜è®¤0è‡ªåŠ¨)')
    parser.add_argument('--drop_path_rate', type=float, default=0.1,
                       help='DropPathæ­£åˆ™åŒ–æ¦‚ç‡ï¼ˆç”¨äºConvNeXtç­‰æ¨¡å‹ï¼Œ0=ç¦ç”¨ï¼Œæ¨èConvNeXt-Tiny:0.1, Small:0.2, Base:0.3ï¼‰')
    
    # å­¦ä¹ ç‡è°ƒåº¦å‚æ•°
    parser.add_argument('--scheduler_type', type=str, default='cosine', 
                       choices=['cosine', 'cosine_restarts', 'step'],
                       help='å­¦ä¹ ç‡è°ƒåº¦ç±»å‹: cosine=æ ‡å‡†ä½™å¼¦, cosine_restarts=å‘¨æœŸé‡å¯, step=åˆ†æ®µè¡°å‡')
    parser.add_argument('--min_lr', type=float, default=None, help='æœ€å°å­¦ä¹ ç‡ï¼ŒNoneåˆ™ä½¿ç”¨lr*0.01')
    
    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument('--optimizer', type=str, default='Adam',
                       choices=['SGD', 'Adam', 'AdamW', 'RMSprop'],
                       help='ä¼˜åŒ–å™¨ç±»å‹ï¼ˆé»˜è®¤: Adamï¼‰')
    
    # åˆ†å‰²ä»»åŠ¡ä¸“ç”¨å‚æ•°
    parser.add_argument('--scale', type=float, default=1.0, help='å›¾åƒç¼©æ”¾æ¯”ä¾‹ï¼ˆåˆ†å‰²ä»»åŠ¡ï¼Œç”¨äºèŠ‚çœæ˜¾å­˜ï¼‰')
    
    # MLflowå‚æ•°
    parser.add_argument('--task_name', type=str, default='Image Classification', help='MLflowè¿è¡Œåç§°')
    parser.add_argument('--project_name', type=str, default='ai-classifier', help='MLflowå®éªŒåç§°')
    
    # æ•°æ®é›†å¤§å°é™åˆ¶ï¼ˆç”¨äºå¿«é€ŸéªŒè¯ï¼‰
    parser.add_argument('--train_size', type=int, default=None, help='æ¯ä¸ªç±»åˆ«ä¿ç•™çš„è®­ç»ƒæ ·æœ¬æ•°ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨')
    parser.add_argument('--val_size', type=int, default=None, help='æ¯ä¸ªç±»åˆ«ä¿ç•™çš„éªŒè¯æ ·æœ¬æ•°ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨')
    
    # GPUè®¾å¤‡é€‰æ‹©
    parser.add_argument('--device', type=str, default=None, help='æŒ‡å®šGPUè®¾å¤‡ï¼Œä¾‹å¦‚ "0", "1" æˆ– "cuda:0", None=ä½¿ç”¨é»˜è®¤')
    
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument('--distributed', action='store_true', help='å¯ç”¨å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ')

    parser.add_argument('--only_val', action='store_true', help='ä¸è®­ç»ƒï¼Œåªä½¿ç”¨ best æ¨¡å‹è¯„ä¼°')
    
    # ONNXå¯¼å‡º
    parser.add_argument('--export_onnx', action='store_true', default=True,
                       help='è®­ç»ƒå®Œæˆåè‡ªåŠ¨å¯¼å‡ºONNXæ¨¡å‹ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-export-onnx', dest='export_onnx', action='store_false',
                       help='ç¦ç”¨è‡ªåŠ¨å¯¼å‡ºONNX')
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    parser.add_argument('--list-models', type=str, nargs='?', const='', default=None,
                       metavar='SERIES',
                       help='åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ã€‚ä¸å¸¦å‚æ•°æ˜¾ç¤ºæ‰€æœ‰ç³»åˆ—ï¼Œå¸¦å‚æ•°æ˜¾ç¤ºæŒ‡å®šç³»åˆ—çš„æ‰€æœ‰æ¨¡å‹ï¼ˆå¦‚: --list-models resnetï¼‰')
    
    # ä»…æ˜¾ç¤ºæ¨¡å‹ç»“æ„
    parser.add_argument('--show-model', action='store_true',
                       help='ä»…æ˜¾ç¤ºæ¨¡å‹ç»“æ„ï¼Œä¸è¿›è¡Œè®­ç»ƒï¼ˆé…åˆ--archä½¿ç”¨ï¼‰')
    
    # æ‰“å°æ¨¡å‹ç»“æ„ï¼ˆæ–°å‘½ä»¤ï¼‰
    parser.add_argument('--print-structure', action='store_true',
                       help='æ‰“å°æ¨¡å‹è¯¦ç»†ç»“æ„ï¼ˆåŒ…æ‹¬æ¯å±‚çš„shapeä¿¡æ¯ï¼Œé…åˆ--archä½¿ç”¨ï¼‰')
    parser.add_argument('--no-shape', action='store_true',
                       help='æ‰“å°ç»“æ„æ—¶ä¸æ˜¾ç¤ºshapeä¿¡æ¯ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰')
    
    args = parser.parse_args()

    # å¦‚æœç”¨æˆ·è¯·æ±‚åˆ—å‡ºæ¨¡å‹ï¼Œæ˜¾ç¤ºåé€€å‡º
    if args.list_models is not None:
        # args.list_models ä¸ºç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸å¸¦å‚æ•°ï¼Œæ˜¾ç¤ºæ‰€æœ‰ç³»åˆ—
        # args.list_models ä¸ºå…·ä½“å€¼è¡¨ç¤ºæŒ‡å®šç³»åˆ—
        series_filter = args.list_models if args.list_models else None
        list_available_models(series_filter)
        return

    # å¦‚æœç”¨æˆ·è¯·æ±‚æ‰“å°ç»“æ„
    if args.print_structure:
        print_structure_command(
            arch=args.arch,
            img_size=args.img_size,
            show_shape=not args.no_shape
        )
        return

    if is_main_process(): 
        # æ˜¾ç¤ºæ¨¡å‹ä¿å­˜è·¯å¾„ä¿¡æ¯
        print(f"\n{'='*80}")
        print(f"æ¨¡å‹ä¿å­˜é…ç½®:")
        print(f"  åŸºç¡€ç›®å½•: {args.models_base_dir}")
        print(f"  é¡¹ç›®åç§°: {args.project_name}")
        print(f"  ä»»åŠ¡åç§°: {args.task_name}")
        print(f"  å®é™…è·¯å¾„: {Path(args.models_base_dir).absolute() / args.project_name / args.task_name}")
        print(f"{'='*80}\n")
    
    # ç»Ÿä¸€çš„å‚æ•°å­—å…¸ï¼Œé¿å…é‡å¤ä»£ç 
    train_params = {
        'data_path': args.data_path,
        'model_path': args.model_path,
        'img_size': args.img_size,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'lr0': args.lr0,
        'lrf': args.lrf,
        'arch': args.arch,
        'wd': args.wd,
        'early_stopping': args.early_stopping,
        'grad_acc': args.grad_acc,
        'load_model': args.load_model,
        'auto_resume': args.auto_resume,
        'task_name': args.task_name,
        'project_name': args.project_name,
        'train_size': args.train_size,
        'val_size': args.val_size,
        'device': args.device,
        'scheduler_type': args.scheduler_type,
        'min_lr': args.min_lr,
        'distributed': args.distributed,
        'models_base_dir': args.models_base_dir,
        'only_val': args.only_val,
        'scale': args.scale,
        'export_onnx': args.export_onnx,
        'optimizer': args.optimizer,
        'drop_path_rate': args.drop_path_rate,
    }
    
    # FastAI åˆ†å¸ƒå¼è®­ç»ƒï¼šåœ¨ä¸»å‡½æ•°ä¸­åˆå§‹åŒ–ç¯å¢ƒï¼Œç„¶åæ­£å¸¸è°ƒç”¨è®­ç»ƒ
    if args.distributed:
        setup_distrib()
        try:
            train_model(**train_params)
        finally:
            teardown_distrib()
    else:
        train_model(**train_params)

if __name__ == '__main__':
    main()
