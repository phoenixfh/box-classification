"""
FastAIè¶…å‚æ•°è°ƒä¼˜ä¸»è„šæœ¬

æ”¯æŒç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ã€è´å¶æ–¯ä¼˜åŒ–ï¼ˆOptunaï¼‰
æ”¯æŒåˆ†å¸ƒå¼å¤šGPUè®­ç»ƒ
"""

import argparse
import yaml
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional
import mlflow
from fastai.distributed import setup_distrib, teardown_distrib
import torch
import torch.distributed as dist

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼ˆç¡®ä¿å¯ä»¥å¯¼å…¥ utilsï¼‰
sys.path.insert(0, str(Path(__file__).parent.parent))

from train import train_model
from utils.tuning import create_search_strategy, OptunaSearchStrategy
from utils.mlflow_tuning import (
    TuningExperimentManager, 
    select_best_run,
    analyze_parameter_importance,
    TuningCheckpoint,
    log_tuning_summary,
    compare_runs
)
from utils import is_main_process


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    if 'search_space' not in config:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»åŒ…å« 'search_space' å­—æ®µ")
    
    if 'base_args' not in config:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»åŒ…å« 'base_args' å­—æ®µ")
    
    return config


def merge_params(base_args: Dict[str, Any], trial_params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆå¹¶åŸºç¡€å‚æ•°å’Œtrialå‚æ•°"""
    merged = base_args.copy()
    
    # è¿‡æ»¤æ‰å†…éƒ¨å­—æ®µ
    for key, value in trial_params.items():
        if not key.startswith('_'):
            merged[key] = value
    
    # ç§»é™¤ä¸åº”ä¼ é€’ç»™ train_model çš„å‚æ•°
    exclude_keys = {'_tuning_metric', 'mlflow_uri'}
    for key in exclude_keys:
        merged.pop(key, None)
    
    return merged


def run_single_trial(trial_idx: int, trial_params: Dict[str, Any], 
                     base_args: Dict[str, Any], exp_manager: TuningExperimentManager
                     ) -> Optional[float]:
    """
    è¿è¡Œå•ä¸ªtrial
    
    Args:
        trial_idx: trialç´¢å¼•
        trial_params: trialå‚æ•°
        base_args: åŸºç¡€å‚æ•°
        exp_manager: å®éªŒç®¡ç†å™¨
    
    Returns:
        éªŒè¯æŒ‡æ ‡å€¼ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    if is_main_process():
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ Trial {trial_idx + 1}")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºå‚æ•°
        print("å‚æ•°:")
        for key, value in trial_params.items():
            if not key.startswith('_'):
                print(f"  {key}: {value}")
    
    # å¯åŠ¨MLflow trial runï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    trial_run = None
    if is_main_process():
        trial_run = exp_manager.start_trial_run(trial_idx, trial_params)
    
    try:
        # åˆå¹¶å‚æ•°
        train_args = merge_params(base_args, trial_params)
        
        # ä¸ºæ¯ä¸ª trial è®¾ç½®å”¯ä¸€çš„æ¨¡å‹ä¿å­˜è·¯å¾„
        project_name = train_args.get('project_name', 'hyperparameter-tuning')
        task_name = train_args.get('task_name', 'tuning')
        trial_task_name = f"{task_name}_trial_{trial_idx:03d}"
        train_args['task_name'] = trial_task_name  # æ¯ä¸ª trial æœ‰ç‹¬ç«‹çš„ç›®å½•
        
        # ç¡®ä¿only_val=Falseï¼ˆæˆ‘ä»¬è¦è®­ç»ƒï¼‰
        train_args['only_val'] = False
        
        # é»˜è®¤ä¸å¯¼å‡ºONNXï¼ˆè°ƒä¼˜æ—¶é¿å…é¢å¤–å¼€é”€ï¼‰
        if 'export_onnx' not in train_args:
            train_args['export_onnx'] = False
        
        # è°ƒä¼˜æ¨¡å¼ï¼šç¦ç”¨æ¨¡å‹ä¸Šä¼ åˆ° MLflowï¼ˆé¿å… S3 å‡­è¯é—®é¢˜ï¼‰
        train_args['skip_mlflow_model_upload'] = True
        
        # ç¦ç”¨ auto_resumeï¼ˆæ¯ä¸ª trial éƒ½æ˜¯å…¨æ–°è®­ç»ƒï¼‰
        train_args['auto_resume'] = False
        
        # å°† trial run ID ä¼ é€’ç»™è®­ç»ƒï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process() and trial_run is not None:
            train_args['mlflow_parent_run_id'] = trial_run.info.run_id
        
        # ä¸´æ—¶æ¢å¤ MLflow URI ç”¨äºè®­ç»ƒ
        import os
        mlflow_uri = base_args.get('mlflow_uri', 'http://192.168.16.130:5000/')
        mlflow_backup = os.environ.get('MLFLOW_TRACKING_URI', '')
        os.environ['MLFLOW_TRACKING_URI'] = mlflow_uri
        
        try:
            # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¸åœ¨è¿™é‡Œåˆå§‹åŒ–ï¼ˆå·²åœ¨tuneä¸»å‡½æ•°åˆå§‹åŒ–ï¼‰
            # åªåœ¨å•ç‹¬è°ƒç”¨ train_model æ—¶æ‰éœ€è¦ setup_distrib
            train_model(**train_args)
        finally:
            # æ¢å¤ä¸ºç©ºï¼ˆç¦ç”¨ S3ï¼‰
            os.environ['MLFLOW_TRACKING_URI'] = mlflow_backup
        
        # è·å–æŒ‡æ ‡å€¼ï¼ˆä»MLflowï¼Œä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            run = mlflow.get_run(trial_run.info.run_id)
            metric_name = base_args.get('_tuning_metric', 'valid_loss')
            
            # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰å¯ç”¨çš„æŒ‡æ ‡
            available_metrics = list(run.data.metrics.keys())
            if is_main_process():
                print(f"ğŸ“Š å¯ç”¨æŒ‡æ ‡: {available_metrics}")
            
            metric_value = run.data.metrics.get(metric_name)
            
            if metric_value is None:
                print(f"âš ï¸ æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_name}'ï¼Œtrialå¤±è´¥")
                print(f"   å¯ç”¨æŒ‡æ ‡: {available_metrics}")
                return None
            
            print(f"\nâœ… Trialå®Œæˆ: {metric_name} = {metric_value:.6f}")
            return metric_value
        else:
            # éä¸»è¿›ç¨‹è¿”å›None
            return None
        
    except Exception as e:
        if is_main_process():
            print(f"\nâŒ Trialå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        return None
    finally:
        if is_main_process() and trial_run is not None:
            exp_manager.end_trial_run()


def run_tuning(config: Dict[str, Any], resume_run_id: Optional[str] = None,
               dry_run: bool = False, distributed: bool = False):
    """
    è¿è¡Œè°ƒä¼˜ä¸»å¾ªç¯
    
    Args:
        config: è°ƒä¼˜é…ç½®
        resume_run_id: æ¢å¤çš„run IDï¼ˆå¯é€‰ï¼‰
        dry_run: ä»…æ˜¾ç¤ºå‚æ•°ç»„åˆï¼Œä¸å®é™…è®­ç»ƒ
        distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    """
    import os
    
    # è·å–å¹¶è®¾ç½® MLflow URI (ä»…ä¸»è¿›ç¨‹)
    if is_main_process():
        mlflow_uri = config.get('base_args', {}).get('mlflow_uri', 'http://192.168.16.130:5000/')
        mlflow.set_tracking_uri(mlflow_uri)
    
    # ç¦ç”¨å†…ç½® MLflow å›è°ƒé¿å… S3 å‡­è¯é—®é¢˜
    mlflow_uri_backup = os.environ.get('MLFLOW_TRACKING_URI', '')
    if not dry_run:
        os.environ['MLFLOW_TRACKING_URI'] = ''
    
    try:
        _run_tuning_impl(config, resume_run_id, dry_run, 
                        config.get('base_args', {}).get('mlflow_uri', 'http://192.168.16.130:5000/'), 
                        distributed)
    finally:
        # æ¢å¤ MLflow URI
        if mlflow_uri_backup:
            os.environ['MLFLOW_TRACKING_URI'] = mlflow_uri_backup


def _run_tuning_impl(config: Dict[str, Any], resume_run_id: Optional[str] = None,
                     dry_run: bool = False, mlflow_uri: str = '', distributed: bool = False):
    """è¿è¡Œè°ƒä¼˜ä¸»å¾ªç¯çš„å®é™…å®ç°
    
    Args:
        config: è°ƒä¼˜é…ç½®
        resume_run_id: æ¢å¤çš„run ID
        dry_run: ä»…æ˜¾ç¤ºå‚æ•°ç»„åˆ
        mlflow_uri: MLflow tracking URI
        distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    """
    
    # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼ˆåœ¨ä»»ä½•åˆ†å¸ƒå¼æ“ä½œä¹‹å‰ï¼‰
    if distributed and not dist.is_initialized():
        # è·å–å½“å‰è¿›ç¨‹çš„ GPU
        import os
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        setup_distrib(gpu=local_rank)
        # åˆå§‹åŒ–åå†æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
        is_main = is_main_process()
        if is_main:
            print(f"ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ– (GPUs: {torch.cuda.device_count()})")
    else:
        is_main = True  # éåˆ†å¸ƒå¼æ¨¡å¼ï¼Œå½“å‰è¿›ç¨‹å°±æ˜¯ä¸»è¿›ç¨‹
    
    # æå–é…ç½®
    base_args = config['base_args']
    strategy_name = config.get('strategy', 'grid')
    metric = config.get('metric', 'valid_loss')
    mode = config.get('mode', 'minimize')
    
    # å°†metricä¿¡æ¯ä¼ é€’ç»™train
    base_args['_tuning_metric'] = metric
    
    # å°†distributedå‚æ•°ä¼ é€’ç»™train_model
    base_args['distributed'] = distributed
    
    # ä»…ä¸»è¿›ç¨‹åˆ›å»ºæœç´¢ç­–ç•¥å’Œæ‰“å°ä¿¡æ¯
    if is_main_process():
        print(f"\nğŸ” æœç´¢ç­–ç•¥: {strategy_name}")
        if distributed:
            print(f"ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒ: {torch.cuda.device_count()} GPUs")
    
    strategy = create_search_strategy(config)
    total_trials = strategy.get_total_trials()
    
    if is_main_process():
        print(f"ğŸ“Š æ€»è¯•éªŒæ•°: {total_trials}")
    
    # Dry runæ¨¡å¼ (ä»…ä¸»è¿›ç¨‹)
    if dry_run:
        if is_main_process():
            print(f"\n{'='*80}")
            print("ğŸ” Dry Run: å‚æ•°ç»„åˆé¢„è§ˆ")
            print(f"{'='*80}\n")
            
            for idx, trial_params in enumerate(strategy.generate_trials()):
                print(f"Trial {idx + 1}:")
                for key, value in trial_params.items():
                    if not key.startswith('_'):
                        print(f"  {key}: {value}")
                print()
                
                if idx >= 9:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                    print(f"... è¿˜æœ‰ {total_trials - 10} ä¸ªç»„åˆ\n")
                    break
            
            print(f"æ€»è®¡: {total_trials} ä¸ªå‚æ•°ç»„åˆ")
        return
    
    # é¡¹ç›®åç§°
    project_name = base_args.get('project_name', 'hyperparameter-tuning')
    task_name = base_args.get('task_name', f"{strategy_name}-tuning")
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨ (ä»…ä¸»è¿›ç¨‹)
    exp_manager = None
    parent_run_id = None
    if is_main_process():
        exp_manager = TuningExperimentManager(project_name, task_name, config)
        
        # Checkpointç®¡ç†
        checkpoint_path = Path('runs') / project_name / task_name / 'tuning_checkpoint.json'
        checkpoint = TuningCheckpoint(checkpoint_path)
        
        # æ¢å¤æˆ–æ–°å»º
        completed_trials = set()
        if resume_run_id or checkpoint.exists():
            if resume_run_id:
                parent_run_id = resume_run_id
                print(f"ğŸ”„ ä»æŒ‡å®šrunæ¢å¤: {parent_run_id}")
            else:
                state = checkpoint.load()
                parent_run_id = state.get('parent_run_id')
                completed_trials = set(state.get('completed_trials', []))
                print(f"ğŸ”„ ä»checkpointæ¢å¤: {len(completed_trials)} ä¸ªå·²å®Œæˆ")
            
            # TODO: é‡æ–°å…³è”parent run
            # ç›®å‰ç®€åŒ–å¤„ç†ï¼Œé‡æ–°åˆ›å»º
            parent_run = exp_manager.start_parent_run()
            parent_run_id = parent_run.info.run_id
        else:
            parent_run = exp_manager.start_parent_run()
            parent_run_id = parent_run.info.run_id
    else:
        # éä¸»è¿›ç¨‹
        completed_trials = set()
        checkpoint = None
    
    # ä¸»å¾ªç¯
    start_time = time.time()
    trial_results = []
    
    # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹éœ€è¦æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œç›¸åŒçš„trial
    # ä¸»è¿›ç¨‹ç”Ÿæˆtrialå‚æ•°ï¼Œç„¶åå¹¿æ’­ç»™å…¶ä»–è¿›ç¨‹
    if distributed:
        # æ”¶é›†æ‰€æœ‰trialå‚æ•° (ä»…ä¸»è¿›ç¨‹)
        all_trials = []
        if is_main_process():
            for trial_idx, trial_params in enumerate(strategy.generate_trials()):
                if trial_idx not in completed_trials:
                    all_trials.append((trial_idx, trial_params))
            print(f"ğŸ“‹ å‡†å¤‡æ‰§è¡Œ {len(all_trials)} ä¸ªè¯•éªŒ")
        
        # å¹¿æ’­trialæ•°é‡
        trial_count = len(all_trials) if is_main_process() else 0
        trial_count = torch.tensor(trial_count, dtype=torch.long)
        if torch.cuda.is_available():
            trial_count = trial_count.cuda()
        dist.broadcast(trial_count, src=0)
        trial_count = trial_count.item()
        
        # éä¸»è¿›ç¨‹åˆå§‹åŒ–ç©ºåˆ—è¡¨
        if not is_main_process():
            all_trials = [(None, None)] * trial_count
        
        # é€ä¸ªå¹¿æ’­trialå‚æ•°
        for i in range(trial_count):
            if is_main_process():
                trial_idx, trial_params = all_trials[i]
                # åºåˆ—åŒ–å‚æ•°
                import pickle
                params_bytes = pickle.dumps((trial_idx, trial_params))
                params_size = len(params_bytes)
            else:
                params_size = 0
            
            # å¹¿æ’­å‚æ•°å¤§å°
            size_tensor = torch.tensor(params_size, dtype=torch.long)
            if torch.cuda.is_available():
                size_tensor = size_tensor.cuda()
            dist.broadcast(size_tensor, src=0)
            params_size = size_tensor.item()
            
            # å¹¿æ’­å‚æ•°å†…å®¹
            if is_main_process():
                params_tensor = torch.ByteTensor(list(params_bytes))
            else:
                params_tensor = torch.ByteTensor(params_size)
            
            if torch.cuda.is_available():
                params_tensor = params_tensor.cuda()
            dist.broadcast(params_tensor, src=0)
            
            # ååºåˆ—åŒ–
            if not is_main_process():
                import pickle
                params_bytes = bytes(params_tensor.cpu().numpy())
                trial_idx, trial_params = pickle.loads(params_bytes)
                all_trials[i] = (trial_idx, trial_params)
        
        # æ‰§è¡Œæ‰€æœ‰trial
        try:
            for trial_idx, trial_params in all_trials:
                # è¿è¡Œtrial (æ‰€æœ‰è¿›ç¨‹å‚ä¸)
                metric_value = run_single_trial( trial_idx, trial_params, base_args, exp_manager)
                
                # è®°å½•ç»“æœ (ä»…ä¸»è¿›ç¨‹)
                if is_main_process() and metric_value is not None:
                    # ç¡®ä¿ trial_params å¯ä»¥ JSON åºåˆ—åŒ–
                    serializable_params = {}
                    for k, v in trial_params.items():
                        if isinstance(v, (int, float, str, bool, list, dict, type(None))):
                            serializable_params[k] = v
                        else:
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            serializable_params[k] = str(v)
                    
                    trial_results.append({
                        'trial_idx': trial_idx,
                        'params': serializable_params,
                        'metric_value': metric_value
                    })
                    
                    # Optunaåé¦ˆ
                    if isinstance(strategy, OptunaSearchStrategy):
                        strategy.report_result(trial_params, metric_value)
                    
                    # ä¿å­˜checkpoint
                    completed_trials.add(trial_idx)
                    checkpoint.save({
                        'parent_run_id': parent_run_id,
                        'completed_trials': list(completed_trials),
                        'trial_results': trial_results
                    })
        except KeyboardInterrupt:
            if is_main_process():
                print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è°ƒä¼˜")
                print(f"å·²å®Œæˆ {len(completed_trials)} ä¸ªè¯•éªŒ")
                if parent_run_id:
                    print(f"æ¢å¤å‘½ä»¤: python fastai/tune.py --resume {parent_run_id}")
        except Exception as e:
            if is_main_process():
                print(f"\nâŒ è°ƒä¼˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    else:
        # éåˆ†å¸ƒå¼æ¨¡å¼
        try:
            for trial_idx, trial_params in enumerate(strategy.generate_trials()):
                # è·³è¿‡å·²å®Œæˆçš„ (ä»…ä¸»è¿›ç¨‹æ£€æŸ¥)
                if is_main_process() and trial_idx in completed_trials:
                    print(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„ Trial {trial_idx + 1}")
                    continue
                
                # è¿è¡Œtrial (æ‰€æœ‰è¿›ç¨‹å‚ä¸)
                metric_value = run_single_trial(
                    trial_idx, trial_params, base_args, exp_manager
                )
                
                # è®°å½•ç»“æœ (ä»…ä¸»è¿›ç¨‹)
                if is_main_process() and metric_value is not None:
                    # ç¡®ä¿ trial_params å¯ä»¥ JSON åºåˆ—åŒ–
                    serializable_params = {}
                    for k, v in trial_params.items():
                        if isinstance(v, (int, float, str, bool, list, dict, type(None))):
                            serializable_params[k] = v
                        else:
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            serializable_params[k] = str(v)
                    
                    trial_results.append({
                        'trial_idx': trial_idx,
                        'params': serializable_params,
                        'metric_value': metric_value
                    })
                    
                    # Optunaåé¦ˆ
                    if isinstance(strategy, OptunaSearchStrategy):
                        strategy.report_result(trial_params, metric_value)
                    
                    # ä¿å­˜checkpoint
                    completed_trials.add(trial_idx)
                    checkpoint.save({
                        'parent_run_id': parent_run_id,
                        'completed_trials': list(completed_trials),
                        'trial_results': trial_results
                    })
        except KeyboardInterrupt:
            if is_main_process():
                print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è°ƒä¼˜")
                print(f"å·²å®Œæˆ {len(completed_trials)} ä¸ªè¯•éªŒ")
                if parent_run_id:
                    print(f"æ¢å¤å‘½ä»¤: python fastai/tune.py --resume {parent_run_id}")
        except Exception as e:
            if is_main_process():
                print(f"\nâŒ è°ƒä¼˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    
    # è°ƒä¼˜å®Œæˆ (ä»…ä¸»è¿›ç¨‹å¤„ç†)
    if is_main_process():
        duration = time.time() - start_time
        print(f"\n{'='*80}")
        print(f"âœ… è°ƒä¼˜å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æ€»è€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
        print(f"å®Œæˆè¯•éªŒ: {len(trial_results)}/{total_trials}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_result = select_best_run(parent_run_id, metric, mode)
        
        # å‚æ•°é‡è¦æ€§åˆ†æ
        if len(trial_results) >= 10:
            output_dir = Path('runs') / project_name / task_name
            analyze_parameter_importance(parent_run_id, metric, output_dir)
            compare_runs(parent_run_id, metric, output_dir)
        
        # å…ˆç»“æŸ parent runï¼Œå†è®°å½•æ€»ç»“ï¼ˆé¿å… run å†²çªï¼‰
        if exp_manager is not None:
            exp_manager.end_parent_run()
        
        # è®°å½•æ€»ç»“
        log_tuning_summary(parent_run_id, len(trial_results), best_result, duration)
        
        # æ¸…ç†checkpoint
        checkpoint.clear()
    
    # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if distributed:
        dist.barrier()
        if is_main_process():
            print("ğŸ”§ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ...")
        teardown_distrib()


def main():
    parser = argparse.ArgumentParser(description='FastAIè¶…å‚æ•°è°ƒä¼˜')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, required=True,
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    
    # æ¢å¤
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤ä¹‹å‰çš„è°ƒä¼˜runï¼ˆæä¾›parent run IDï¼‰')
    
    # Dry run
    parser.add_argument('--dry-run', action='store_true',
                       help='ä»…æ˜¾ç¤ºå‚æ•°ç»„åˆï¼Œä¸å®é™…è®­ç»ƒ')
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument('--distributed', action='store_true',
                       help='å¯ç”¨å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ')
    
    # å‚æ•°è¦†ç›–
    parser.add_argument('--override', type=str, nargs='+',
                       help='è¦†ç›–é…ç½®å‚æ•°ï¼Œæ ¼å¼: key=value')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®ï¼ˆä»…ä¸»è¿›ç¨‹æ‰“å°ï¼‰
    if is_main_process():
        print(f"ğŸ“ åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    # å‚æ•°è¦†ç›–
    if args.override:
        if is_main_process():
            print("\nâš™ï¸ å‚æ•°è¦†ç›–:")
        for override in args.override:
            if '=' not in override:
                if is_main_process():
                    print(f"âš ï¸ å¿½ç•¥æ— æ•ˆè¦†ç›–: {override}")
                continue
            
            key, value = override.split('=', 1)
            
            # å°è¯•è§£æå€¼
            try:
                # å°è¯•evalï¼ˆæ”¯æŒæ•°å­—ã€åˆ—è¡¨ç­‰ï¼‰
                value = eval(value)
            except:
                # ä¿æŒå­—ç¬¦ä¸²
                pass
            
            # è¦†ç›–åˆ°base_args
            config['base_args'][key] = value
            if is_main_process():
                print(f"  {key} = {value}")
    
    # æ˜¾ç¤ºé…ç½®æ‘˜è¦ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if is_main_process():
        print(f"\nğŸ“‹ é…ç½®æ‘˜è¦:")
        print(f"  ç­–ç•¥: {config.get('strategy', 'grid')}")
        print(f"  è¯•éªŒæ•°: {config.get('n_trials', 'all')}")
        print(f"  ä¼˜åŒ–æŒ‡æ ‡: {config.get('metric', 'valid_loss')} ({config.get('mode', 'minimize')})")
        print(f"  æœç´¢å‚æ•°: {', '.join(config['search_space'].keys())}")
    if args.distributed and is_main_process():
        import torch
        print(f"  åˆ†å¸ƒå¼è®­ç»ƒ: {torch.cuda.device_count()} GPUs")
    
    # è¿è¡Œè°ƒä¼˜
    run_tuning(config, resume_run_id=args.resume, dry_run=args.dry_run, distributed=args.distributed)


if __name__ == '__main__':
    main()
