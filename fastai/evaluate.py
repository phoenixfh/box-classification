#!/usr/bin/env python
"""
ç‹¬ç«‹çš„æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒå¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œç”Ÿæˆæ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Š

ç”¨æ³•:
    # åŸºæœ¬ç”¨æ³•
    python fastai/evaluate.py --model best.pth --data /path/to/data
    
    # å®Œæ•´å‚æ•°ï¼ˆå…³è”åˆ°è®­ç»ƒ Runï¼‰
    python fastai/evaluate.py \
        --model runs/ai-classifier/resnet18/best.pth \
        --data /mnt/ssd/dataset \
        --img_size 224 \
        --batch_size 256 \
        --arch resnet18 \
        --output_dir ./evaluation_results \
        --mlflow_run_id <run_id>
"""

from fastai.vision.all import *
from pathlib import Path
import argparse
import sys
import os
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import mlflow

# å¯¼å…¥å…¬å…±å·¥å…·å‡½æ•°
sys.path.insert(0, str(Path(__file__).parent))
from utils import upload_figure_to_mlflow, upload_metrics_to_mlflow, upload_artifact_to_mlflow

def load_model_from_checkpoint(checkpoint_path, data_path, img_size, batch_size, arch):
    """
    ä»checkpointåŠ è½½æ¨¡å‹
    
    Args:
        checkpoint_path: æ¨¡å‹checkpointè·¯å¾„
        data_path: æ•°æ®é›†è·¯å¾„
        img_size: å›¾åƒå°ºå¯¸
        batch_size: batchå¤§å°
        arch: æ¨¡å‹æ¶æ„
        
    Returns:
        learn: åŠ è½½å¥½æƒé‡çš„Learnerå¯¹è±¡
    """
    # å…³é”®ä¿®å¤ï¼šæ¸…ç†åˆ†å¸ƒå¼ç›¸å…³çš„ç¯å¢ƒå˜é‡
    # è¯„ä¼°ä½œä¸ºç‹¬ç«‹è¿›ç¨‹è¿è¡Œï¼Œä¸åº”ç»§æ‰¿è®­ç»ƒè¿›ç¨‹çš„åˆ†å¸ƒå¼è®¾ç½®
    import os
    distributed_env_vars = [
        'RANK', 'LOCAL_RANK', 'WORLD_SIZE', 
        'MASTER_ADDR', 'MASTER_PORT',
        'TORCH_DISTRIBUTED_DEBUG'
    ]
    
    cleaned_vars = []
    for var in distributed_env_vars:
        if var in os.environ:
            del os.environ[var]
            cleaned_vars.append(var)
    
    if cleaned_vars:
        print(f"ğŸ”§ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {', '.join(cleaned_vars)}")
    
    print(f"\n{'='*80}")
    print("åŠ è½½æ¨¡å‹")
    print(f"{'='*80}")
    
    checkpoint_path = Path(checkpoint_path)
    data_path = Path(data_path)
    
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    if not data_path.exists():
        raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {data_path}")
    
    # 1. åŠ è½½checkpoint
    print(f"ğŸ“¦ åŠ è½½checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # è·å–checkpointä¸­çš„ä¿¡æ¯
    saved_img_size = checkpoint.get('img_size', img_size)
    saved_arch = checkpoint.get('arch', arch)
    
    print(f"   Checkpointä¿¡æ¯:")
    print(f"   - æ¶æ„: {saved_arch}")
    print(f"   - å›¾åƒå°ºå¯¸: {saved_img_size}")
    if 'epoch' in checkpoint:
        print(f"   - è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
    if 'best_metric' in checkpoint:
        print(f"   - æœ€ä½³æŒ‡æ ‡: {checkpoint['best_metric']:.6f}")
    
    # ä½¿ç”¨checkpointä¸­çš„é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
    if saved_img_size != img_size:
        print(f"   âš ï¸  ä½¿ç”¨checkpointä¸­çš„å›¾åƒå°ºå¯¸: {saved_img_size}")
        img_size = saved_img_size
    
    if saved_arch != arch:
        print(f"   âš ï¸  ä½¿ç”¨checkpointä¸­çš„æ¨¡å‹æ¶æ„: {saved_arch}")
        arch = saved_arch
    
    # 2. é‡å»ºDataLoadersï¼ˆéåˆ†å¸ƒå¼ï¼‰
    print(f"\nğŸ“Š æ„å»ºæ•°æ®é›†...")
    print(f"   æ•°æ®è·¯å¾„: {data_path}")
    print(f"   å›¾åƒå°ºå¯¸: {img_size}")
    print(f"   Batchå¤§å°: {batch_size}")
    
    dls = ImageDataLoaders.from_folder(
        data_path,
        valid='val',
        bs=batch_size,
        item_tfms=Resize(img_size),
        batch_tfms=None,  # è¯„ä¼°ä¸éœ€è¦æ•°æ®å¢å¼º
        num_workers=min(8, os.cpu_count()),
        shuffle=False
    )
    
    print(f"   è®­ç»ƒé›†: {len(dls.train.dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(dls.valid.dataset)} æ ·æœ¬")
    print(f"   ç±»åˆ«æ•°: {len(dls.vocab)}")
    print(f"   ç±»åˆ«: {dls.vocab[:10]}{'...' if len(dls.vocab) > 10 else ''}")
    
    # 3. åˆ›å»ºLearner
    print(f"\nğŸ”§ åˆ›å»ºLearner...")
    print(f"   æ¶æ„: {arch}")
    
    learn = vision_learner(
        dls,
        arch=arch,
        metrics=[
            accuracy,
            error_rate,
            Precision(average='weighted'),
            Recall(average='weighted'),
            F1Score(average='weighted')
        ],
        pretrained=False  # ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡ï¼Œæˆ‘ä»¬ä¼šåŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    )
    
    # å…³é”®ä¿®å¤ï¼šç§»é™¤æ‰€æœ‰åˆ†å¸ƒå¼ç›¸å…³çš„callbacks
    # è¯„ä¼°è„šæœ¬ä½œä¸ºç‹¬ç«‹è¿›ç¨‹è¿è¡Œï¼Œä¸éœ€è¦åˆ†å¸ƒå¼åŒæ­¥
    print(f"\nğŸ”§ ç§»é™¤åˆ†å¸ƒå¼callbacksï¼ˆç‹¬ç«‹è¿›ç¨‹æ¨¡å¼ï¼‰...")
    from fastai.distributed import DistributedTrainer, GatherPredsCallback
    
    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„åˆ†å¸ƒå¼callbacks
    callbacks_to_remove = []
    for cb in learn.cbs:
        if isinstance(cb, (DistributedTrainer, GatherPredsCallback)):
            callbacks_to_remove.append(cb)
            print(f"   - ç§»é™¤: {cb.__class__.__name__}")
    
    for cb in callbacks_to_remove:
        learn.remove_cb(cb)
    
    if not callbacks_to_remove:
        print(f"   âœ… æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼callbacks")
    
    # 4. åŠ è½½æƒé‡
    print(f"\nâš™ï¸  åŠ è½½æ¨¡å‹æƒé‡...")
    model_state = checkpoint['model']
    
    # å¤„ç†å¯èƒ½çš„DDPå‰ç¼€
    if any(k.startswith('module.') for k in model_state.keys()):
        print(f"   æ£€æµ‹åˆ°DDPå‰ç¼€ï¼Œæ­£åœ¨ç§»é™¤...")
        model_state = {k.replace('module.', ''): v for k, v in model_state.items()}
    
    learn.model.load_state_dict(model_state)
    print(f"   âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    
    print(f"{'='*80}\n")
    
    return learn

def evaluate_with_learner(
    learn,
    model_path,
    output_dir='./evaluation_results',
    mlflow_run_id=None
):
    """
    ä½¿ç”¨å·²æœ‰çš„Learnerå¯¹è±¡è¿›è¡Œè¯„ä¼°ï¼ˆå¤ç”¨è®­ç»ƒæ•°æ®ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
    
    Args:
        learn: FastAI Learnerå¯¹è±¡ï¼ˆå·²è®­ç»ƒå¥½çš„ï¼‰
        model_path: æ¨¡å‹checkpointè·¯å¾„ï¼ˆç”¨äºåŠ è½½bestæƒé‡ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        mlflow_run_id: MLflow Run IDï¼ˆå¯é€‰ï¼Œç”¨äºå…³è”è®­ç»ƒRunï¼‰
    """
    
    # å…³é”®ä¿®å¤ï¼šæ¸…ç†åˆ†å¸ƒå¼ç›¸å…³çš„ç¯å¢ƒå˜é‡
    import os
    distributed_env_vars = [
        'RANK', 'LOCAL_RANK', 'WORLD_SIZE', 
        'MASTER_ADDR', 'MASTER_PORT',
        'TORCH_DISTRIBUTED_DEBUG'
    ]
    
    cleaned_vars = []
    for var in distributed_env_vars:
        if var in os.environ:
            del os.environ[var]
            cleaned_vars.append(var)
    
    if cleaned_vars:
        print(f"ğŸ”§ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {', '.join(cleaned_vars)}")
    
    print(f"\n{'='*80}")
    print("å‡†å¤‡è¯„ä¼°ï¼ˆå¤ç”¨è®­ç»ƒæ•°æ®ï¼‰")
    print(f"{'='*80}")
    
    # ç§»é™¤åˆ†å¸ƒå¼callbacks
    print(f"\nğŸ”§ ç§»é™¤åˆ†å¸ƒå¼callbacksï¼ˆç‹¬ç«‹è¯„ä¼°æ¨¡å¼ï¼‰...")
    from fastai.distributed import DistributedTrainer, GatherPredsCallback
    
    callbacks_to_remove = []
    for cb in learn.cbs:
        if isinstance(cb, (DistributedTrainer, GatherPredsCallback)):
            callbacks_to_remove.append(cb)
            print(f"   - ç§»é™¤: {cb.__class__.__name__}")
    
    for cb in callbacks_to_remove:
        learn.remove_cb(cb)
    
    if not callbacks_to_remove:
        print(f"   âœ… æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼callbacks")
    
    # åŠ è½½bestæ¨¡å‹æƒé‡
    model_path = Path(model_path)
    if model_path.exists():
        print(f"\nğŸ“¦ åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡: {model_path}")
        checkpoint = torch.load(model_path, map_location='cpu')
        model_state = checkpoint['model']
        
        # å¤„ç†å¯èƒ½çš„DDPå‰ç¼€
        if any(k.startswith('module.') for k in model_state.keys()):
            print(f"   æ£€æµ‹åˆ°DDPå‰ç¼€ï¼Œæ­£åœ¨ç§»é™¤...")
            model_state = {k.replace('module.', ''): v for k, v in model_state.items()}
        
        learn.model.load_state_dict(model_state)
        print(f"   âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print(f"   âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æƒé‡: {model_path}")
    
    # æ‰§è¡Œè¯„ä¼°
    print(f"\n{'='*80}")
    print("å¼€å§‹è¯„ä¼°")
    print(f"{'='*80}")
    print("æ­£åœ¨å¯¹éªŒè¯é›†è¿›è¡Œé¢„æµ‹...")
    print(f"   æ•°æ®é›†: {len(learn.dls.valid.dataset)} ä¸ªæ ·æœ¬")
    print(f"   ç±»åˆ«æ•°: {len(learn.dls.vocab)}")
    
    preds, targs = learn.get_preds(dl=learn.dls.valid)
    
    print(f"âœ… é¢„æµ‹å®Œæˆ ({len(targs)} ä¸ªæ ·æœ¬)")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_reports(
        learn=learn,
        preds=preds,
        targs=targs,
        output_dir=output_dir,
        mlflow_run_id=mlflow_run_id
    )
    
    print(f"\n{'='*80}")
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {Path(output_dir).absolute()}")
    print(f"{'='*80}\n")

def evaluate_model(
    model_path,
    data_path,
    img_size=224,
    batch_size=256,
    arch='resnet18',
    output_dir='./evaluation_results',
    mlflow_run_id=None
):
    """
    æ‰§è¡Œæ¨¡å‹è¯„ä¼°
    
    Args:
        model_path: æ¨¡å‹checkpointè·¯å¾„
        data_path: æ•°æ®é›†è·¯å¾„
        img_size: å›¾åƒå°ºå¯¸
        batch_size: batchå¤§å°
        arch: æ¨¡å‹æ¶æ„
        output_dir: è¾“å‡ºç›®å½•
        mlflow_run_id: MLflow Run IDï¼ˆå¯é€‰ï¼Œç”¨äºå…³è”è®­ç»ƒRunï¼‰
    """
    
    # 1. åŠ è½½æ¨¡å‹
    learn = load_model_from_checkpoint(
        checkpoint_path=model_path,
        data_path=data_path,
        img_size=img_size,
        batch_size=batch_size,
        arch=arch
    )
    
    # 2. æ‰§è¡Œè¯„ä¼°
    print(f"{'='*80}")
    print("å¼€å§‹è¯„ä¼°")
    print(f"{'='*80}")
    print("æ­£åœ¨å¯¹éªŒè¯é›†è¿›è¡Œé¢„æµ‹...")
    
    preds, targs = learn.get_preds(dl=learn.dls.valid)
    
    print(f"âœ… é¢„æµ‹å®Œæˆ ({len(targs)} ä¸ªæ ·æœ¬)")
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    generate_reports(
        learn=learn,
        preds=preds,
        targs=targs,
        output_dir=output_dir,
        mlflow_run_id=mlflow_run_id
    )
    
    print(f"\n{'='*80}")
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {Path(output_dir).absolute()}")
    print(f"{'='*80}\n")

def generate_reports(learn, preds, targs, output_dir, mlflow_run_id=None):
    """
    ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    
    Args:
        learn: Learnerå¯¹è±¡
        preds: é¢„æµ‹ç»“æœ
        targs: çœŸå®æ ‡ç­¾
        output_dir: è¾“å‡ºç›®å½•
        mlflow_run_id: MLflow Run IDï¼ˆå¯é€‰ï¼Œç”¨äºä¸Šä¼ ç»“æœï¼‰
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    classes = learn.dls.vocab
    
    # è½¬æ¢æ ‡ç­¾
    targ_indices = targs.cpu().numpy()
    pred_indices = preds.argmax(dim=1).cpu().numpy()
    classes_array = np.array(classes)
    true_labels = classes_array[targ_indices].tolist()
    pred_labels = classes_array[pred_indices].tolist()
    
    # 1. ç±»åˆ«é¢„æµ‹ç»Ÿè®¡
    print_class_statistics(classes, true_labels, pred_labels)
    
    # 2. æ··æ·†çŸ©é˜µ
    save_confusion_matrix(
        targ_indices, pred_indices, classes, 
        output_dir, mlflow_run_id
    )
    
    # 3. åˆ†ç±»æŠ¥å‘Š
    save_classification_report(
        true_labels, pred_labels, classes,
        output_dir, mlflow_run_id
    )

def print_class_statistics(classes, true_labels, pred_labels):
    """æ‰“å°ç±»åˆ«é¢„æµ‹ç»Ÿè®¡"""
    print("\n" + "="*80)
    print("ç±»åˆ«é¢„æµ‹ç»Ÿè®¡")
    print("="*80)
    print(f"{'ç±»åˆ«':<20} {'çœŸå®æ ·æœ¬æ•°':<15} {'è¢«é¢„æµ‹æ¬¡æ•°':<15} {'çŠ¶æ€':<20}")
    print("-"*80)
    
    true_label_counts = pd.Series(true_labels).value_counts()
    pred_label_counts = pd.Series(pred_labels).value_counts()
    
    classes_with_no_predictions = []
    classes_with_no_samples = []
    
    for class_name in classes:
        true_count = true_label_counts.get(class_name, 0)
        pred_count = pred_label_counts.get(class_name, 0)
        
        if pred_count == 0 and true_count > 0:
            status = "âš ï¸  æœªè¢«é¢„æµ‹åˆ°"
            classes_with_no_predictions.append(class_name)
        elif true_count == 0 and pred_count > 0:
            status = "âš ï¸  è¯¯é¢„æµ‹ï¼ˆæ— çœŸå®æ ·æœ¬ï¼‰"
            classes_with_no_samples.append(class_name)
        elif true_count == 0 and pred_count == 0:
            status = "â“ æ— æ•°æ®"
        else:
            status = "âœ“ æ­£å¸¸"
        
        print(f"{class_name:<20} {true_count:<15} {pred_count:<15} {status:<20}")
    
    print("-"*80)
    
    if classes_with_no_predictions:
        print(f"\nâš ï¸  è­¦å‘Šï¼šä»¥ä¸‹ç±»åˆ«åœ¨éªŒè¯é›†ä¸­æœ‰æ ·æœ¬ä½†æœªè¢«æ¨¡å‹é¢„æµ‹åˆ°ï¼š")
        for cls in classes_with_no_predictions:
            print(f"   - {cls} (çœŸå®æ ·æœ¬æ•°: {true_label_counts[cls]})")
        print(f"   è¿™äº›ç±»åˆ«çš„ Precision å°†è¢«è®¾ä¸º 0.0")
    
    if classes_with_no_samples:
        print(f"\nâš ï¸  è­¦å‘Šï¼šä»¥ä¸‹ç±»åˆ«åœ¨éªŒè¯é›†ä¸­æ— çœŸå®æ ·æœ¬ä½†è¢«æ¨¡å‹è¯¯é¢„æµ‹ï¼š")
        for cls in classes_with_no_samples:
            print(f"   - {cls} (è¢«é¢„æµ‹æ¬¡æ•°: {pred_label_counts[cls]})")
        print(f"   è¿™äº›ç±»åˆ«çš„ Recall å°†è¢«è®¾ä¸º 0.0")
    
    print("="*80 + "\n")

def save_confusion_matrix(targ_indices, pred_indices, classes, output_dir, mlflow_run_id=None):
    """ä¿å­˜æ··æ·†çŸ©é˜µ"""
    print("ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    
    try:
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(targ_indices, pred_indices)
        
        # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                    xticklabels=classes, yticklabels=classes, cbar=True)
        plt.title('Confusion Matrix (Normalized)')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        normalized_path = output_dir / 'confusion_matrix_normalized.png'
        plt.savefig(normalized_path, dpi=150, bbox_inches='tight')
        print(f"   âœ… ä¿å­˜å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ: {normalized_path}")
        
        # ä¸Šä¼ åˆ° MLflow
        if mlflow_run_id:
            upload_figure_to_mlflow(plt.gcf(), 'confusion_matrix_normalized', mlflow_run_id)
        
        plt.close()
        
        # åŸå§‹è®¡æ•°æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=classes, yticklabels=classes, cbar=True)
        plt.title('Confusion Matrix (Count)')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        count_path = output_dir / 'confusion_matrix_count.png'
        plt.savefig(count_path, dpi=150, bbox_inches='tight')
        print(f"   âœ… ä¿å­˜è®¡æ•°æ··æ·†çŸ©é˜µ: {count_path}")
        
        # ä¸Šä¼ åˆ° MLflow
        if mlflow_run_id:
            upload_figure_to_mlflow(plt.gcf(), 'confusion_matrix_count', mlflow_run_id)
        
        plt.close()
        
        # æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹
        print("\næœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹:")
        confused_pairs = []
        for i in range(len(cm)):
            for j in range(len(cm)):
                if i != j and cm[i, j] > 0:
                    confused_pairs.append((classes[i], classes[j], cm[i, j]))
        
        confused_pairs.sort(key=lambda x: x[2], reverse=True)
        if confused_pairs:
            for actual, predicted, count in confused_pairs:
                print(f"   çœŸå®: {actual:<15} â†’ é¢„æµ‹ä¸º: {predicted:<15} (é”™è¯¯ {int(count)} æ¬¡)")
        else:
            print("   æ— æ˜æ˜¾æ··æ·†")
        
    except Exception as e:
        print(f"âš ï¸  ç»˜åˆ¶æ··æ·†çŸ©é˜µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def save_classification_report(true_labels, pred_labels, classes, output_dir, mlflow_run_id=None):
    """ä¿å­˜åˆ†ç±»æŠ¥å‘Š"""
    print("\nç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š...")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = classification_report(
        true_labels,
        pred_labels,
        labels=classes,
        output_dict=True,
        zero_division=0
    )
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*80)
    print("åˆ†ç±»æŠ¥å‘Š (Classification Report)")
    print("="*80)
    print(classification_report(
        true_labels,
        pred_labels,
        labels=classes,
        zero_division=0
    ))
    
    # ä¿å­˜ä¸ºDataFrame
    report_df = pd.DataFrame({
        'class': classes,
        'precision': [report[c]['precision'] if c in report else 0 for c in classes],
        'recall': [report[c]['recall'] if c in report else 0 for c in classes],
        'f1-score': [report[c]['f1-score'] if c in report else 0 for c in classes],
        'support': [report[c]['support'] if c in report else 0 for c in classes]
    })
    
    # æ·»åŠ æ€»ä½“æŒ‡æ ‡
    overall_metrics = pd.DataFrame({
        'class': ['accuracy', 'macro avg', 'weighted avg'],
        'precision': [
            report.get('accuracy', 0),
            report['macro avg']['precision'],
            report['weighted avg']['precision']
        ],
        'recall': [
            report.get('accuracy', 0),
            report['macro avg']['recall'],
            report['weighted avg']['recall']
        ],
        'f1-score': [
            report.get('accuracy', 0),
            report['macro avg']['f1-score'],
            report['weighted avg']['f1-score']
        ],
        'support': [
            sum(report_df['support']),
            sum(report_df['support']),
            sum(report_df['support'])
        ]
    })
    
    full_report_df = pd.concat([report_df, overall_metrics], ignore_index=True)
    
    # ä¿å­˜åˆ°CSV
    csv_path = output_dir / 'classification_report.csv'
    full_report_df.to_csv(csv_path, index=False)
    print(f"   âœ… ä¿å­˜åˆ†ç±»æŠ¥å‘Š: {csv_path}")
    
    # ä¸Šä¼ åˆ° MLflow
    if mlflow_run_id:
        upload_metrics_to_mlflow(report, classes, mlflow_run_id)
        upload_artifact_to_mlflow(csv_path, mlflow_run_id)
    
    print("="*80)

def main():
    parser = argparse.ArgumentParser(
        description='è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•ï¼ˆä»…ä¿å­˜æœ¬åœ°æ–‡ä»¶ï¼‰
  python fastai/evaluate.py --model best.pth --data /mnt/ssd/dataset
  
  # å…³è”åˆ° MLflow è®­ç»ƒ Run
  python fastai/evaluate.py \\
      --model runs/ai-classifier/resnet18/best.pth \\
      --data /mnt/ssd/dataset \\
      --img_size 224 \\
      --batch_size 256 \\
      --arch resnet18 \\
      --output_dir ./evaluation_results \\
      --mlflow_run_id <run_id>
        """
    )
    
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„ (å¿…éœ€)')
    parser.add_argument('--data', type=str, required=True,
                       help='æ•°æ®é›†è·¯å¾„ (å¿…éœ€)')
    parser.add_argument('--img_size', type=int, default=224,
                       help='è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤: 224)')
    parser.add_argument('--batch_size', type=int, default=256,
                       help='Batchå¤§å° (é»˜è®¤: 256)')
    parser.add_argument('--arch', type=str, default='resnet18',
                       help='æ¨¡å‹æ¶æ„ (é»˜è®¤: resnet18)')
    parser.add_argument('--output_dir', type=str, default='./evaluation_results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ./evaluation_results)')
    parser.add_argument('--mlflow_run_id', type=str, default=None,
                       help='MLflow Run ID (å¯é€‰ï¼Œç”¨äºå…³è”è®­ç»ƒRun)')
    
    args = parser.parse_args()
    
    try:
        evaluate_model(
            model_path=args.model,
            data_path=args.data,
            img_size=args.img_size,
            batch_size=args.batch_size,
            arch=args.arch,
            output_dir=args.output_dir,
            mlflow_run_id=args.mlflow_run_id
        )
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
